<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
  <title>Machine Learning on Musa R. Nyakerabachi</title>
  <link>/tags/machine-learning/</link>
  <description>Recent content in Machine Learning on Musa R. Nyakerabachi</description>
  <generator>Hugo -- gohugo.io</generator>
<language>en-us</language>
<copyright>Musa R. Nyakerabachi&amp;copy; 2020. All rights reserved.</copyright>
<lastBuildDate>Mon, 27 Jul 2020 00:00:00 +0000</lastBuildDate>

<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />


<item>
  <title>Titanic project.</title>
  <link>/2020/07/27/titanic-project./</link>
  <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
  
<guid>/2020/07/27/titanic-project./</guid>
  <description>&lt;h1 id=&#34;titanic-machine-learning-from-disaster-project&#34;&gt;Titanic: Machine Learning from Disaster Project&lt;/h1&gt;
&lt;h2 id=&#34;1-business-understanding&#34;&gt;1. BUSINESS UNDERSTANDING.&lt;/h2&gt;
&lt;p&gt;Source:&lt;a href=&#34;https://www.kaggle.com/c/titanic/&#34;&gt;Machine Learning Titanic Competition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data: &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Titanic Data Set from Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;context&#34;&gt;Context:&lt;/h3&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://www.kaggle.com/c/titanic/discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;problem-statement-&#34;&gt;Problem statement :&lt;/h3&gt;
&lt;p&gt;Using machine learning to create a model that predicts which passengers survived the Titanic shipwreck.&lt;/p&gt;
&lt;h2 id=&#34;21-data-description-&#34;&gt;2.1 Data Description :&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Column Name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Key&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;survival&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Survival&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0&lt;/strong&gt; = No, &lt;strong&gt;1&lt;/strong&gt; = Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;pclass&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Ticket class&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;1&lt;/strong&gt; = 1st, &lt;strong&gt;2&lt;/strong&gt; = 2nd, &lt;strong&gt;3&lt;/strong&gt; = 3rd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;sex&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;C&lt;/strong&gt; = Cherbourg, &lt;strong&gt;Q&lt;/strong&gt; = Queenstown, &lt;strong&gt;S&lt;/strong&gt; = Southampton&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Age&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Age in years&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;sibsp&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of siblings / spouses aboard the Titanic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;parch&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of parents / children aboard the Titanic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;ticket&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Ticket number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;fare&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Passenger fare&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;cabin&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Cabin number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;embarked&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Port of Embarkation&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;211-importing-libraries&#34;&gt;2.1.1 Importing Libraries.&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll load the important libraries needed for this machine learning project task.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#loading libraries&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pearsonr
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LogisticRegression
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AdaBoostClassifier, BaggingClassifier, RandomForestClassifier
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SVC
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.tree &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; DecisionTreeClassifier
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (accuracy_score, auc, classification_report, 
                             confusion_matrix, fbeta_score,precision_score,recall_score,f1_score)
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; MinMaxScaler, StandardScaler
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GridSearchCV, train_test_split,KFold
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; warnings &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; filterwarnings
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; modeling_util &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; meu
filterwarnings(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ignore&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&amp;rsquo;ll get thed data set using pandas library by storing it as df and also get the data description using &lt;strong&gt;describe()&lt;/strong&gt; and &lt;strong&gt;info()&lt;/strong&gt; methods.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train.csv&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Getting the first four rows of the data set using &lt;strong&gt;.head()&lt;/strong&gt; method.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table1.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;212-statistical-summary&#34;&gt;2.1.2 Statistical Summary.&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;describe(include&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;number)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T &lt;span style=&#34;color:#75715e&#34;&gt;# transposing the data set description&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table2.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above the maximum Number of parents / children aboard the Titanic(&lt;strong&gt;Parch&lt;/strong&gt;) were &lt;strong&gt;6&lt;/strong&gt; while minimum being &lt;strong&gt;0&lt;/strong&gt;, while maximum &lt;strong&gt;Age&lt;/strong&gt; being &lt;strong&gt;80&lt;/strong&gt; and minimum age being less than a year for passenges who boarded titanic and averagely most people who were in the titanic were about &lt;strong&gt;30&lt;/strong&gt; years of age, while the maximum number of siblings / spouses aboard the Titanic(&lt;strong&gt;SibSp&lt;/strong&gt;) was &lt;strong&gt;8&lt;/strong&gt; while minimum being &lt;strong&gt;0&lt;/strong&gt;, roughly the maximum amount of fare was &lt;strong&gt;512&lt;/strong&gt; while minimum being &lt;strong&gt;0&lt;/strong&gt;. Why is minimum &lt;strong&gt;Fare&lt;/strong&gt; is &lt;strong&gt;0&lt;/strong&gt; maybe children within a given threshhold are not being charged.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info() &lt;span style=&#34;color:#75715e&#34;&gt;# information about data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above there is some rows with missing &lt;strong&gt;Age&lt;/strong&gt;, &lt;strong&gt;Cabin&lt;/strong&gt; and &lt;strong&gt;Embarked&lt;/strong&gt; i&amp;rsquo;ll make some data imputation on them.&lt;/p&gt;
&lt;p&gt;Let me group the three classes and see the average &lt;strong&gt;Age&lt;/strong&gt; of people in each class using &lt;strong&gt;.groupby()&lt;/strong&gt; method of the dataframe type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;AGE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Age&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
],axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    
    
AGE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table3.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the boxplot above the second class(&lt;strong&gt;Pclass&lt;/strong&gt;) age has got outliers as shown above maybe &lt;strong&gt;standarndadization&lt;/strong&gt; if not &lt;strong&gt;normalization&lt;/strong&gt; will handle this.From above it is vividly depicted that old people boarded first class while middle aged boarded second class while children boarded third class reason being known little explation to the reason being attached since it&amp;rsquo;s common sense.Let me use this data to make imputation on Age column for missing values with first class people age being roughly 39 from my above &lt;strong&gt;groupby()&lt;/strong&gt; method of means in 3 classes, and 30 for second class while the third class being 25 years old.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;age_na&lt;/span&gt;(fill):
    Age &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fill[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
    Pclass &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fill[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isnull(Age):

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; Pclass &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;39&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; Pclass &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Age
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;]]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apply(age_na,axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          891 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The missing values of &lt;strong&gt;Age&lt;/strong&gt; column have been filled up the remaining column with missing values are &lt;strong&gt;Embarked&lt;/strong&gt; and &lt;strong&gt;Cabin&lt;/strong&gt; i&amp;rsquo;ll fill the &lt;strong&gt;Embarked&lt;/strong&gt; missing values with largest number of station &lt;strong&gt;Embarked&lt;/strong&gt; and also since &lt;strong&gt;Embarked&lt;/strong&gt; has &lt;strong&gt;3&lt;/strong&gt; distinct value i&amp;rsquo;ll convert it to categorical data type while &lt;strong&gt;Cabin&lt;/strong&gt; has alot of missing values hence i&amp;rsquo;ll drop it during machine learning session.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;dfn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()&lt;span style=&#34;color:#75715e&#34;&gt;#finding total number of people embarkedin each station&lt;/span&gt;

    
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#75715e&#34;&gt;#finding total number of people embarkedin each station&lt;/span&gt;
dfn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Count&amp;#39;&lt;/span&gt;]
dfn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table4.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above the &lt;strong&gt;Southampton&lt;/strong&gt; station had a lot of people than rest so ill use it in filling the 2 missing values in the &lt;strong&gt;Embarked&lt;/strong&gt; column.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fillna(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;S&amp;#34;&lt;/span&gt;,inplace &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True)&lt;span style=&#34;color:#75715e&#34;&gt;# filling Embarked missing values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          891 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     891 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me convert Embarked to categorical data type using &lt;strong&gt;.astype()&lt;/strong&gt; method.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;category&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#converting Embarked column to categorical data type&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Confirming if it has been converted successfly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype &lt;span style=&#34;color:#75715e&#34;&gt;# checking type of embarked column&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;CategoricalDtype(categories=[&#39;C&#39;, &#39;Q&#39;, &#39;S&#39;], ordered=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me divide the people in the ship into two i.e child or adult. With year less than 18 I consider being child while anything above 18 as adult. I&amp;rsquo;ll use &lt;strong&gt;Age&lt;/strong&gt; in dividing them and I&amp;rsquo;ll create another column called &lt;strong&gt;Category&lt;/strong&gt; to hold &lt;strong&gt;child&lt;/strong&gt; and &lt;strong&gt;adult&lt;/strong&gt; variables.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Asumptions are based on my area of jurisdiction constitution stipulation as to who an adult and child should be according to number of years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cut(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;18&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;], labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;child&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adult&amp;#39;&lt;/span&gt;]) &lt;span style=&#34;color:#75715e&#34;&gt;#creating category column&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table5.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Bingo!!! the &lt;strong&gt;Category&lt;/strong&gt; column have been created with pandas &lt;strong&gt;.cut()&lt;/strong&gt; method as shown in  the above table.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll also add a &lt;strong&gt;Family&lt;/strong&gt; column where I&amp;rsquo;ll take into consideration &lt;strong&gt;SibSp&lt;/strong&gt; and &lt;strong&gt;Parch&lt;/strong&gt; columns where I&amp;rsquo;ll write an expresion where the two columns should be more than one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; My assumption is for a family to be called a family,the number of siblings / spouses aboard the Titanic and number of parents / children aboard the Titanic	should be greater than zero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SibSp&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; ([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Parch&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#creating family column&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table6.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Bingo!!! the &lt;strong&gt;Family&lt;/strong&gt; column have been created with pandas &lt;strong&gt;.cut()&lt;/strong&gt; method as shown in  the above table. 
I&amp;rsquo;ll inspect the data frame to see if I have handled missing values and columns I&amp;rsquo;ve created their data type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 14 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   PassengerId  891 non-null    int64   
 1   Survived     891 non-null    int64   
 2   Pclass       891 non-null    int64   
 3   Name         891 non-null    object  
 4   Sex          891 non-null    object  
 5   Age          891 non-null    float64 
 6   SibSp        891 non-null    int64   
 7   Parch        891 non-null    int64   
 8   Ticket       891 non-null    object  
 9   Fare         891 non-null    float64 
 10  Cabin        204 non-null    object  
 11  Embarked     891 non-null    category
 12  Category     891 non-null    category
 13  Family       891 non-null    bool    
dtypes: bool(1), category(2), float64(2), int64(5), object(4)
memory usage: 79.5+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me inspect the uniqueness in each columns.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nunique()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;PassengerId    891
Survived         2
Pclass           3
Name           891
Sex              2
Age             88
SibSp            7
Parch            7
Ticket         681
Fare           248
Cabin          147
Embarked         3
Category         2
Family           2
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me convert &lt;strong&gt;Family&lt;/strong&gt; to categorical since it has two variables either &lt;strong&gt;True&lt;/strong&gt; or &lt;strong&gt;False&lt;/strong&gt; for &lt;strong&gt;Family&lt;/strong&gt; and no &lt;strong&gt;Family&lt;/strong&gt; respectively.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;category&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#75715e&#34;&gt;# converting family from bool to categorical&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 14 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   PassengerId  891 non-null    int64   
 1   Survived     891 non-null    int64   
 2   Pclass       891 non-null    int64   
 3   Name         891 non-null    object  
 4   Sex          891 non-null    object  
 5   Age          891 non-null    float64 
 6   SibSp        891 non-null    int64   
 7   Parch        891 non-null    int64   
 8   Ticket       891 non-null    object  
 9   Fare         891 non-null    float64 
 10  Cabin        204 non-null    object  
 11  Embarked     891 non-null    category
 12  Category     891 non-null    category
 13  Family       891 non-null    category
dtypes: category(3), float64(2), int64(5), object(4)
memory usage: 79.6+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me inspect the fare that was being paid by those who had family and those who didn&amp;rsquo;t have in each station. Using below snippet of code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
df3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table7.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is evident from above that those who had family paid more than those who were alone in each station.
Let me include an &lt;strong&gt;adult&lt;/strong&gt; and a &lt;strong&gt;child&lt;/strong&gt; from &lt;strong&gt;Category&lt;/strong&gt; column and &lt;strong&gt;Sex&lt;/strong&gt; columm of &lt;strong&gt;Female&lt;/strong&gt; and &lt;strong&gt;Male&lt;/strong&gt; in each station and see who paid more in average.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Category&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sex&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
df3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table8.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that females who are adults paid more than child who is female and a child of male gender from Cherbourg and Queenstown paid more than their adult counterpart.
Let me see how many were family member and between family and those who were single who died more.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
df3&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Count&amp;#34;&lt;/span&gt;]
df3

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table9.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that in first class &lt;strong&gt;20&lt;/strong&gt; family members died and &lt;strong&gt;59&lt;/strong&gt; survived while &lt;strong&gt;60&lt;/strong&gt; who had no family died and &lt;strong&gt;77&lt;/strong&gt; survived.In the second class &lt;strong&gt;27&lt;/strong&gt; family members died while &lt;strong&gt;37&lt;/strong&gt; survived while their counterpart &lt;strong&gt;70&lt;/strong&gt; died while &lt;strong&gt;50&lt;/strong&gt; survived lastly in the third class &lt;strong&gt;104&lt;/strong&gt; family members died and &lt;strong&gt;36&lt;/strong&gt; survived while those who had no family &lt;strong&gt;268&lt;/strong&gt; died and &lt;strong&gt;83&lt;/strong&gt; survived.
Let me inspect the &lt;strong&gt;Fare&lt;/strong&gt; that those who were family members and singles paid.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df4 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

df4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table10.PNG&#34; alt=&#34;&#34;&gt;
From above it is evident that family members(&lt;strong&gt;True&lt;/strong&gt;) paid more than those who were single(&lt;strong&gt;False&lt;/strong&gt;).&lt;/p&gt;
&lt;h1 id=&#34;213-data-visualization&#34;&gt;2.1.3 Data Visualization.&lt;/h1&gt;
&lt;p&gt;Let me draw a &lt;strong&gt;boxplot&lt;/strong&gt; to show how people are distributed across the three classes in the ship using &lt;strong&gt;seaborn&lt;/strong&gt; and &lt;strong&gt;matplotlib&lt;/strong&gt; libraries .&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# figure size&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;whitegrid&amp;#34;&lt;/span&gt;, color_codes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True) &lt;span style=&#34;color:#75715e&#34;&gt;#  background appearance &lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;boxplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;,y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;#plotting boxplot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_55_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above boxplot old boarded first class while childrens boarded third class.&lt;/p&gt;
&lt;p&gt;Let me draw a countplot using seaborn library to see how adults and children distributed themselves among three classes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;# background style&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;#plotting countplot&lt;/span&gt;

plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_57_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is evident from above class that adults were more in all classes as compared to childrens.&lt;/p&gt;
&lt;p&gt;Next let me draw a graph to see survival rate of both sex.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;);
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sex&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_59_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above graph &lt;strong&gt;males&lt;/strong&gt; died more than &lt;strong&gt;females&lt;/strong&gt; whle &lt;strong&gt;female&lt;/strong&gt; survived more than &lt;strong&gt;male&lt;/strong&gt;.
Let me also show the survival rate in each class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;)
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_61_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;People in the first class survived more than the rest while people in third class died more than the rest.&lt;/p&gt;
&lt;p&gt;Next let me see between children and adults who survived more.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;);
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Category&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_63_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In both scenarios more adults died and survived than childrens.
Next let me draw a histogram to see how fare is distributed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fare&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(bins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;darkred&amp;#39;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_65_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that Fare is not normal distributed and it needs to be standardized.&lt;/p&gt;
&lt;p&gt;Next let me see how the Number of siblings / spouses who aboarded the Titanic are distributed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SibSp&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Set1&amp;#34;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_67_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fro above it is evident those who were single with no sibling and spouses were more than the rest in all classes in the Titanic.&lt;/p&gt;
&lt;p&gt;Next let me see how the number of parents / children who aboard the Titanic are distributed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Parch&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Set1&amp;#34;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_69_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that those who had 0 number of parents/children in the Titanic were more than the rest.&lt;/p&gt;
&lt;p&gt;Next let me see if there is a correlation between different variables.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;# background style&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;corr(), annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_71_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above the correlation between different column is minimal.&lt;/p&gt;
&lt;h1 id=&#34;3-machine-learning&#34;&gt;3. MACHINE LEARNING.&lt;/h1&gt;
&lt;h2 id=&#34;31-feature-engineering&#34;&gt;3.1 Feature Engineering.&lt;/h2&gt;
&lt;p&gt;Before dividing the data into features and labels I&amp;rsquo;ll drop &lt;strong&gt;Cabin&lt;/strong&gt; column since its less important and it has a lot of missing variables, I&amp;rsquo;ll set &lt;strong&gt;inplace&lt;/strong&gt; to &lt;strong&gt;True&lt;/strong&gt; so that it si compltetely removed from the dataframe.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#dropping the Cabin column&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;drop(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cabin&amp;#39;&lt;/span&gt;,axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,inplace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let me see if the &lt;strong&gt;Cabin&lt;/strong&gt; column has been removed by viewing the first few rows using &lt;strong&gt;.head()&lt;/strong&gt; method.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#printing first four values.&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table11.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#info about column and data&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 13 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   PassengerId  891 non-null    int64   
 1   Survived     891 non-null    int64   
 2   Pclass       891 non-null    int64   
 3   Name         891 non-null    object  
 4   Sex          891 non-null    object  
 5   Age          891 non-null    float64 
 6   SibSp        891 non-null    int64   
 7   Parch        891 non-null    int64   
 8   Ticket       891 non-null    object  
 9   Fare         891 non-null    float64 
 10  Embarked     891 non-null    category
 11  Category     891 non-null    category
 12  Family       891 non-null    category
dtypes: category(3), float64(2), int64(5), object(3)
memory usage: 72.6+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above I can see I don&amp;rsquo;t have any missing values then my data is ready for further engineering and modelling&lt;/p&gt;
&lt;h2 id=&#34;311-converting-categorical-features&#34;&gt;3.1.1 Converting Categorical Features.&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won&amp;rsquo;t be able to directly take in those features as inputs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Converting  the categorical variables to dummy variables&lt;/span&gt;
sex &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_dummies(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;],drop_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
embark &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_dummies(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;],drop_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;drop([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Family&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Ticket&amp;#39;&lt;/span&gt;],axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,inplace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&amp;rsquo;ll return the &lt;strong&gt;Sex&lt;/strong&gt; and &lt;strong&gt;Embarked&lt;/strong&gt; column that I had dropped durring encoding stage using a method called &lt;strong&gt;concat()&lt;/strong&gt; from pandas library&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt; &lt;span style=&#34;color:#75715e&#34;&gt;#concatinating sex and embark column to data&lt;/span&gt;
df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([df,sex,embark],axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#printing first four rows&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table12.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Infering my  few rows of data I can see I have my data in numerical form for my model to work best.&lt;/p&gt;
&lt;p&gt;Before I build the model I&amp;rsquo;ll have to split  data into features and labels i.e X and y.&lt;/p&gt;
&lt;p&gt;After splitting  data into X and y i&amp;rsquo;ll further split into training set and test set whereby i&amp;rsquo;ll use training set to train the model and also use it in evaluation stage in the process of developing the model.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll use the trained model to predict the model using the unseen test set whereby it will help in assessing how the model is performing and also its robustness.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Preparing the X  and y variables.&lt;/span&gt;
X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;PassengerId&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SibSp&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Parch&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fare&amp;#39;&lt;/span&gt;,
       &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;S&amp;#39;&lt;/span&gt;]]
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From &lt;strong&gt;sklearn&lt;/strong&gt; package I&amp;rsquo;ll use a module called &lt;strong&gt;model_selection&lt;/strong&gt; which has a function called &lt;strong&gt;train_test_split()&lt;/strong&gt; which divides the data into training and test set.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;train_test_split()&lt;/strong&gt; function I have to pass features and labels and also split my data using &lt;strong&gt;test_size&lt;/strong&gt; argument whereby I&amp;rsquo;ll set it to 30% meaning  30% of my data will be used as test set while rest as traing set.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll set my &lt;strong&gt;random_state&lt;/strong&gt; to 101 for reproducibility purposes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#loading library and function&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Splitting the data into train and test sets.&lt;/span&gt;
X_train, X_test, y_train, y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(X, y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.30&lt;/span&gt;, 
                                                    random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;101&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;32-building-and-evaluating-model&#34;&gt;3.2 Building and Evaluating Model.&lt;/h2&gt;
&lt;p&gt;Since our task at hand has labelled data and also predicted data is known so it fall under supervised typer of machine learning.&lt;/p&gt;
&lt;p&gt;Under supervised machine learning it is divided into two classification and regression type. Classification answers the question of the form yes/no  and regession answers the questions how much?.&lt;/p&gt;
&lt;p&gt;Since my task at hand is to predict whether a persion died or survived so it falls under classication problem and under classification i&amp;rsquo;ll use below machine learning algorithms in training my model which are available in sklearn package read more about them in this link &lt;a href=&#34;http://scikit-learn.org/stable/supervised_learning.html&#34;&gt;&lt;code&gt;scikit-learn&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian Naive Bayes (GaussianNB).&lt;/li&gt;
&lt;li&gt;Decision Trees.&lt;/li&gt;
&lt;li&gt;Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting).&lt;/li&gt;
&lt;li&gt;K-Nearest Neighbors (KNeighbors).&lt;/li&gt;
&lt;li&gt;Support Vector Machines (SVM).&lt;/li&gt;
&lt;li&gt;Logistic Regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;321-implementation&#34;&gt;3.2.1 Implementation.&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ll implement my model using the following procedure in each algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;ll start by importing the module&lt;/li&gt;
&lt;li&gt;Next I&amp;rsquo;ll initialize the module using the classifier.&lt;/li&gt;
&lt;li&gt;After that train the model by fitting it to my data using &lt;strong&gt;.fit()&lt;/strong&gt; method.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ll use trained model to make prediction using &lt;strong&gt;predict()&lt;/strong&gt; function.The &lt;strong&gt;predict()&lt;/strong&gt; function will return an array of prediction for each data instance in test set.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;330-building-a-logistic-regression-model&#34;&gt;3.3.0 Building a Logistic Regression model.&lt;/h1&gt;
&lt;p&gt;Logistic regression is a type of regression where its class variable is not continous but categorical, as our cas to survival rate where it is categorical with two classes.Since I&amp;rsquo;m dealing with binary classification problem I&amp;rsquo;ll use binomial logistic regression, which is of this form 
$$ Y =  \frac{e^x}{1+e^x}  $$&lt;br&gt;
$$ where\ X = B_0 + B{_1}x $$&lt;/p&gt;
&lt;p&gt;I had imported the Logistic classifier in the beggining of this project what I&amp;rsquo;ll do is to initialize the classifier then fit in the model using &lt;strong&gt;.fit method&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the models from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing the LogisticRegression Classifier&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LogisticRegression
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier as below.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# instatiating the classifier.&lt;/span&gt;
logmodel &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LogisticRegression()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting your LogisticRegression class to the training data&lt;/span&gt;
logmodel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using LogisticRegression on the test data&lt;/span&gt;
predictions_log &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; logmodel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model - this will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;331-metrics-for-evaluating-model-performance&#34;&gt;3.3.1 Metrics for Evaluating Model Performance.&lt;/h3&gt;
&lt;p&gt;Let me define metrics that ill use in evaluating my model accurcy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;True Positive (TP)&lt;/strong&gt;: Correctly classified as the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;True Negative (TN)&lt;/strong&gt;: Correctly classified as not the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;False Positive (FP)&lt;/strong&gt;: Incorrectly classified as the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;False Negative (FN)&lt;/strong&gt;: Incorrectly classified as not the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; : The proportion of correct predictions out of all the predictions given by:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ Accurcy  =  \frac{TP + TN}{TP + FP + TN  + FN} $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; (also known as the positive predictive value) is defined as the proportion of positive examples that are truly positive, given by the formula:
$$ Precision = \frac{TP}{TP + FP}  $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt;: This is defined as the number of true positives over the total number of positives, given by:
$$ Recall = \frac{Tp}{TP + FN}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;F1 score&lt;/strong&gt;: It combines precision and recall using &lt;strong&gt;harmonic mean&lt;/strong&gt; also known as &lt;strong&gt;F-measure&lt;/strong&gt; or &lt;strong&gt;F-score&lt;/strong&gt;, iven by:
$$ F-MEASURE = \frac{2 \cdot precision \cdot recall}{recall + precision} = \frac{2 \cdot TP }{2 \cdot TP + FP + FN}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confusion matrix&lt;/strong&gt;: This is a &lt;strong&gt;n x n&lt;/strong&gt;  matrix that is used to compare predicted values to actual values.As shown below.
&lt;img src=&#34;/post/2020-07-27-mine_files/Confusion_Matrix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;model_metrics&lt;/span&gt;(y_true, preds, model_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    INPUT:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    y_true - the y values that are actually true in the dataset (numpy array or pandas series)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    preds - the predictions for those values from some model (numpy array or pandas series)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    model_name - (str - optional) a name associated with the model if you would like to add it to the print
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    statements 
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    OUTPUT:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    None - prints the accuracy, precision, recall, and F1 score
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; None:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Accuracy score for : &amp;#39;&lt;/span&gt;, format(accuracy_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Precision score for : &amp;#39;&lt;/span&gt;, format(precision_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Recall score for : &amp;#39;&lt;/span&gt;, format(recall_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F1 score for: &amp;#39;&lt;/span&gt;, format(f1_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Accuracy score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt; , format(accuracy_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Precision score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt;, format(precision_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Recall score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt;, format(recall_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F1 score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt;, format(f1_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing Logistic scores&lt;/span&gt;
model_metrics(y_test, predictions_log, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;LOGISTIC&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for LOGISTIC : 0.7686567164179104
Precision score for LOGISTIC : 0.782608695652174
Recall score for LOGISTIC : 0.631578947368421
F1 score for LOGISTIC : 0.6990291262135923
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;Logistic regession&lt;/strong&gt; has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;77%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;78%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;63%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;70%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with &lt;strong&gt;confusion matrix&lt;/strong&gt; and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([logmodel]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plotting the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_109_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;87%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;63%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;37%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;13%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;34-building-a-knn-model&#34;&gt;3.4 Building a KNN model.&lt;/h1&gt;
&lt;p&gt;This method is called lazy learning because parameters for modelling purposes is not defined.For new data point prediction, the train data is searched for an instance resembling the new instance to be predicted.During modelling purposes the training themselves conjures the knowledge.In determining the proper class during modelling phace KNN looks for the closest points-the nearest neighbors.In order to determine the neighbors to be modelled by algorithm the k comes into play, for example if k=6, six nearest neighbors will be examined.IThe weakness of the algorithm is of equal weight attached traning all the six points even if some of them are meaningless.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the model from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Import the KNeighbors Classifier&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.neighbors &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; KNeighborsClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate a KNeighborsClassifier with:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# n_neighbors=1 and everything else as default values&lt;/span&gt;
mod_knn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; KNeighborsClassifier(n_neighbors&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting your KNeighborsClassifier to the training data&lt;/span&gt;
mod_knn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,
                     weights=&#39;uniform&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using KNeighborsClassifier on the test data&lt;/span&gt;
preds_knn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_knn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model .This will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing KNeighbors scores&lt;/span&gt;
model_metrics(y_test, preds_knn, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;KNN&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for KNN : 0.5932835820895522
Precision score for KNN : 0.5242718446601942
Recall score for KNN : 0.47368421052631576
F1 score for KNN : 0.4976958525345622
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;KNeighbors&lt;/strong&gt; has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;59%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;52%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;47%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;50%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([mod_knn]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_123_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;68%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;47%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;52%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;31%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;35-building-a-decision-tree-model&#34;&gt;3.5 Building a DECISION TREE model.&lt;/h1&gt;
&lt;p&gt;A decision tree works by generating a separating hyperplane or a threshold for the features in data. It does this by considering every feature and finding the correlation between the spread of the values in that feature and the label that you are trying to predict.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the model from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing the  DecisionTree Classifier&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.tree &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; DecisionTreeClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiating a DecisionTreeClassifier&lt;/span&gt;
mod_tree &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DecisionTreeClassifier()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fiting your DecisionTreeClassifier to the training data&lt;/span&gt;
mod_tree&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;,
                       random_state=None, splitter=&#39;best&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting  using DecisionTreeClassifie on the test data&lt;/span&gt;
predictions_tree &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_tree&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model . This will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing the Decision Tree classifier.&lt;/span&gt;
model_metrics(y_test, predictions_tree, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Decision Tree&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for Decision Tree : 0.7388059701492538
Precision score for Decision Tree : 0.7340425531914894
Recall score for Decision Tree : 0.6052631578947368
F1 score for Decision Tree : 0.6634615384615384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;Decision Tree&lt;/strong&gt; has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;74%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;783%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;61%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;66%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([mod_tree]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_137_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;83%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;61%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;40%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;16%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;36-building-support-vector-model&#34;&gt;3.6 Building Support Vector Model.&lt;/h1&gt;
&lt;p&gt;Support vector is like a points of plotted in multidimensional expessing values and their examples.The support vector machine(SVM) creates a boundary called hyperplane, which divides the space creating homogeneous partition on either side, hence combining Nearest Neighbor and Linear Regression methods.SVM uses the idea of what is known as Maximum Marginal Hyperplane which create greater separation between two classes.Kernel Trick is also applied in seperating the two classes.&lt;/p&gt;
&lt;p&gt;Some of the kernel trick applied include:
Kernell function takes this form:
&lt;img src=&#34;/post/2020-07-27-mine_files/kernell.PNG&#34; alt=&#34;&#34;&gt;
where Greek letter phi, that is, (x), is a mapping of the data into another space. Therefore,  the general kernel function applies some transformation to the feature vectors xi and 
xj and combines them using the dot product, which takes two vectors and returns a  single number.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;linear kernel which does not transform data at all, given by the formula:
&lt;img src=&#34;/post/2020-07-27-mine_files/ln.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Polynomial kernel of degree d which adds a simple nonlinear transformation of data.
&lt;img src=&#34;/post/2020-07-27-mine_files/poly.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sigmoid Kernel: Is almost like sigmoid activation function used in neural network.Given by the formula.
&lt;img src=&#34;/post/2020-07-27-mine_files/sigmoid.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian RBF Kernel which is anologous to RBF neural network.It does well in many types of data.
&lt;img src=&#34;/post/2020-07-27-mine_files/gaussian.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First I&amp;rsquo;ll load the package necessary for the modelling from &lt;strong&gt;sklearn&lt;/strong&gt; where it has a method called &lt;strong&gt;svm&lt;/strong&gt; which has &lt;strong&gt;SVC&lt;/strong&gt; function embedded for training support vector machine modells as below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the model from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SVC &lt;span style=&#34;color:#75715e&#34;&gt;#loading library necessary for modelling&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model_svc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SVC() &lt;span style=&#34;color:#75715e&#34;&gt;#instatiate the model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model_svc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train) &lt;span style=&#34;color:#75715e&#34;&gt;#fitting trained model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;,
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;predictions_svm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model_svc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test) &lt;span style=&#34;color:#75715e&#34;&gt;#predicting the feature labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model . This will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing Support Vector Machine scores.&lt;/span&gt;
model_metrics(y_test, predictions_svm, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Support Vector Machine&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for Support Vector Machine : 0.6044776119402985
Precision score for Support Vector Machine : 0.75
Recall score for Support Vector Machine : 0.10526315789473684
F1 score for Support Vector Machine : 0.1846153846153846
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;Support Vector Machine&lt;/strong&gt;  has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;60%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;75%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;11%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;18%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([model_svc]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_151_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;97%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;11%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;90%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;3%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use ensemble type of classification in modelling the classification problem at hand and see if there will be some improvement on perfomance of the model&lt;/p&gt;
&lt;h1 id=&#34;36-ensemble-learning-model&#34;&gt;3.6 Ensemble Learning Model.&lt;/h1&gt;
&lt;p&gt;Ensemble learning, as the name denotes, is a method that combines several machine learning models to generate a superior model, thereby decreasing variability/variance and bias, and boosting performance, as shown below.
&lt;img src=&#34;/post/2020-07-27-mine_files/Ensemble.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ensemble models combine many weaker models that differ in variance and bias, thereby creating a better model, outperforming the individual weaker models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/ensemble_chart.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In  modelling  the  ensemble  learning  type  of  classification  I&amp;rsquo;ll  follow the following 5 methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Importing&lt;/strong&gt; the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instantiating&lt;/strong&gt; the model with the hyperparameters of interest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fitting&lt;/strong&gt; the model to the training data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predicting&lt;/strong&gt; on the test data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scoring&lt;/strong&gt; the model by comparing the predictions to the actual values.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;ll follow the above steps in modelling each of the ensemble methods: &lt;strong&gt;BaggingClassifier&lt;/strong&gt;, &lt;strong&gt;RandomForestClassifier&lt;/strong&gt;, and &lt;strong&gt;AdaBoostClassifier&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For more info about the above ensemble methods can be found below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier&#34;&gt;BaggingClassifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier&#34;&gt;RandomForestClassifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier&#34;&gt;AdaBoostClassifier&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another really useful guide for ensemble methods can be found &lt;a href=&#34;http://scikit-learn.org/stable/modules/ensemble.html&#34;&gt;in the documentation here&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the models from &lt;code&gt;sklearn&lt;/code&gt; and use them to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing the Bagging, RandomForest, and AdaBoost Classifier.&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; RandomForestClassifier,AdaBoostClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; each with the hyperparameters specified in each comment.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiating a RandomForestClassifier with:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 200 weak learners (n_estimators) and everything else as default values&lt;/span&gt;
mod_rf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RandomForestClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiating an a AdaBoostClassifier with:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# With 300 weak learners (n_estimators) and a learning_rate of 0.2&lt;/span&gt;
mod_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AdaBoostClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;600&lt;/span&gt;, learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated each of the models, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; them using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.  This may take a bit of time, I&amp;rsquo;m fitting 1100 weak learners after all!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fit your RandomForestClassifier to the training data&lt;/span&gt;
mod_rf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Fit your AdaBoostClassifier to the training data&lt;/span&gt;
mod_ada&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;, base_estimator=None, learning_rate=0.2,
                   n_estimators=600, random_state=None)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit each of the models, I will use each to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using RandomForestClassifier on the test set data&lt;/span&gt;
preds_rf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_rf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using AdaBoostClassifier on the test set data&lt;/span&gt;
preds_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_ada&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for each of the models - this will give  the &lt;code&gt;score&lt;/code&gt; for how well each of the models is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print Random Forest scores&lt;/span&gt;
model_metrics(y_test, preds_rf, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;random forest&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Print AdaBoost scores&lt;/span&gt;
model_metrics(y_test, preds_rf, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adaboost&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for random forest : 0.832089552238806
Precision score for random forest : 0.8556701030927835
Recall score for random forest : 0.7280701754385965
F1 score for random forest : 0.7867298578199052



Accuracy score for adaboost : 0.832089552238806
Precision score for adaboost : 0.8556701030927835
Recall score for adaboost : 0.7280701754385965
F1 score for adaboost : 0.7867298578199052
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the random forest has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;83%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;86%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;73%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;79%&lt;/strong&gt; while  adaboost has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;83%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;86%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;73%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;79%&lt;/strong&gt; which are the same.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([mod_rf,mod_ada]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;, fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_167_0.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_167_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;Random Forest&lt;/strong&gt; &lt;strong&gt;91%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;73%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;27%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;9%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;AdaBoost&lt;/strong&gt; &lt;strong&gt;86%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;66%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;34%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;14%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally I&amp;rsquo;ll compare all the algorithms that I have used in training the model and choose which is the best and use it tuning the model and also in making general overal deduction of the model perfomance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Algorithm Name&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Accuracy score&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Precision score&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Recall score&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;strong&gt;F1 score&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.76865671641791&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7826086956521740&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.631578947368421&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.699029126213592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;K-Nearest Neighbors&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.59328358208965&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.5242718446601942&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.473684210526316&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4976958525345622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Decision Tree&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.73880597014926&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7340425531914894&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.605263157894737&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6634615384615384&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Support Vector Machine&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.75373134328358&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7500000000000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.631578947368421&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6857142857142857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AdaBoost&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.83208955223880&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.8556701030927835&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.728070175438597&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7867298578199052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.83208955223880&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.8556701030927835&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.728070175438597&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7867298578199052&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From above table it&amp;rsquo;s evident that adaboost and random forest are the best algorithm for training the overal perfomance of the model and also it&amp;rsquo;s tuning process.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use adaboost in tuning the model to see it&amp;rsquo;s improvement I&amp;rsquo;ll use gridsearchCv method of model tuning.&lt;/p&gt;
&lt;h1 id=&#34;4-optimizing-the-model&#34;&gt;4. Optimizing The Model.&lt;/h1&gt;
&lt;h1 id=&#34;41-gridsearchcv&#34;&gt;4.1 GridSearchCV.&lt;/h1&gt;
&lt;p&gt;GridsearchCV is a method of tuning wherein the model can be built by evaluating the combination of parameters mentioned in a grid.&lt;/p&gt;
&lt;p&gt;I can conduct a grid search much more easily in practice by leveraging &lt;strong&gt;model_selection.GridSearchCV&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing &amp;#39;GridSearchCV&amp;#39;, &amp;#39;make_scorer&amp;#39; &lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GridSearchCV
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; make_scorer

&lt;span style=&#34;color:#75715e&#34;&gt;# Starting the timer&lt;/span&gt;
start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time()

&lt;span style=&#34;color:#75715e&#34;&gt;# Initializing the classifier&lt;/span&gt;
mod_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AdaBoostClassifier(base_estimator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DecisionTreeClassifier())

&lt;span style=&#34;color:#75715e&#34;&gt;# Creating the parameters list for tuning&lt;/span&gt;
parameters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;n_estimators&amp;#39;&lt;/span&gt;:[&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;], 
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;:[&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;],
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;base_estimator__min_samples_split&amp;#39;&lt;/span&gt; : np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;base_estimator__max_depth&amp;#39;&lt;/span&gt; : np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
             }

&lt;span style=&#34;color:#75715e&#34;&gt;# Making an fbeta_score scoring object&lt;/span&gt;
scorer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_scorer(fbeta_score,beta&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Perform grid search on the classifier using &amp;#39;scorer&amp;#39; as the scoring method&lt;/span&gt;
grid_search &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GridSearchCV(mod_ada, parameters, scorer)

&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting the grid search object to the training data and finding the optimal parameters&lt;/span&gt;
grid_fit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; grid_search&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Getting the estimator&lt;/span&gt;
best_mod &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; grid_fit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;best_estimator_

&lt;span style=&#34;color:#75715e&#34;&gt;# Making predictions using the unoptimized and model&lt;/span&gt;
predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
best_predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; best_mod&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;span style=&#34;color:#75715e&#34;&gt;# Report the before-and-afterscores&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Unoptimized model&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F-score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Optimized Model&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Final accuracy score on the testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, best_predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Final F-score on the testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, best_predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(best_mod)

&lt;span style=&#34;color:#75715e&#34;&gt;# Printing runtime&lt;/span&gt;
end &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time()
runtime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; end &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; start
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Runtime:&amp;#34;&lt;/span&gt;, runtime)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Unoptimized model
------
Accuracy score on testing data: 0.7425
F-score on testing data: 0.7072

Optimized Model
------
Final accuracy score on the testing data: 0.7873
Final F-score on the testing data: 0.7792
AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;,
                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,
                                                         class_weight=None,
                                                         criterion=&#39;gini&#39;,
                                                         max_depth=2,
                                                         max_features=None,
                                                         max_leaf_nodes=None,
                                                         min_impurity_decrease=0.0,
                                                         min_impurity_split=None,
                                                         min_samples_leaf=1,
                                                         min_samples_split=2,
                                                         min_weight_fraction_leaf=0.0,
                                                         presort=&#39;deprecated&#39;,
                                                         random_state=None,
                                                         splitter=&#39;best&#39;),
                   learning_rate=0.1, n_estimators=20, random_state=None)
Runtime: 64.8770227432251
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;5-feature-importance&#34;&gt;5. Feature Importance.&lt;/h1&gt;
&lt;p&gt;In any supervised machine learning model it&amp;rsquo;s best to identify the features that affect how the model perfom best.Like in the titanic case I identify which features most affect the survival rate of passengers in the titanic.&lt;/p&gt;
&lt;p&gt;From all the features in the titanic dataset I&amp;rsquo;ll use only the top 5 of the features that affect the how the model perfom as shown below&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;feature_plot&lt;/span&gt;(importances, X_train, y_train):
    
    &lt;span style=&#34;color:#75715e&#34;&gt;# Displaying the five most important features&lt;/span&gt;
    indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(importances)[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values[indices[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]]
    values &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; importances[indices][:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]

    &lt;span style=&#34;color:#75715e&#34;&gt;# Creating the plot&lt;/span&gt;
    fig &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;))
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Normalized Weights for First Five Most Predictive Features&amp;#34;&lt;/span&gt;, fontsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bar(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;), values, width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.6&lt;/span&gt;, align&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;center&amp;#34;&lt;/span&gt;, color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#00A000&amp;#39;&lt;/span&gt;, \
          label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Feature Weight&amp;#34;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bar(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cumsum(values), width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, align &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;center&amp;#34;&lt;/span&gt;, color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#00A0A0&amp;#39;&lt;/span&gt;, \
          label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Cumulative Feature Weight&amp;#34;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;), columns)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlim((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.5&lt;/span&gt;))
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Weight&amp;#34;&lt;/span&gt;, fontsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Feature&amp;#34;&lt;/span&gt;, fontsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)
    
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(loc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;upper center&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tight_layout()
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing a supervised learning model that has &amp;#39;feature_importances_&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AdaBoostClassifier

&lt;span style=&#34;color:#75715e&#34;&gt;# Training the supervised model on the training set &lt;/span&gt;
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AdaBoostClassifier()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Extracting the feature importances&lt;/span&gt;
importances &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;feature_importances_

&lt;span style=&#34;color:#75715e&#34;&gt;# Plotting features.&lt;/span&gt;
feature_plot(importances, X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_177_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;51-feature-selection&#34;&gt;5.1. Feature Selection.&lt;/h1&gt;
&lt;p&gt;An interesting thing to think about here is that how does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower, at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model we found earlier, and train it on the same training set with only the top five important features.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importting functionality for cloning a model&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.base &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; clone

&lt;span style=&#34;color:#75715e&#34;&gt;# Reducing the feature space&lt;/span&gt;
X_train_reduced &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_train[X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values[(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(importances)[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]]]
X_test_reduced &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_test[X_test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values[(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(importances)[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]]]

&lt;span style=&#34;color:#75715e&#34;&gt;# Training on the &amp;#34;best&amp;#34; model found from grid search earlier&lt;/span&gt;
mod_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (clone(best_mod))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train_reduced, y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Making new predictions&lt;/span&gt;
reduced_predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_ada&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test_reduced)

&lt;span style=&#34;color:#75715e&#34;&gt;# Reporting scores from the final model using both versions of data&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Final Model trained on full data&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, best_predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F-score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, best_predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Final Model trained on reduced data&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, reduced_predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F-score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, reduced_predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Final Model trained on full data
------
Accuracy on testing data: 0.7873
F-score on testing data: 0.7792

Final Model trained on reduced data
------
Accuracy on testing data: 0.6754
F-score on testing data: 0.6150
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it can be seen that training on full data gives the best score as compared to training on reduced data since there is reduction in score from 78% to 68%.Therefore it&amp;rsquo;s best to make inference on features that make great impact to the model but not training using the only those features but on whole features.&lt;/p&gt;
&lt;p&gt;Thank you for going through my first project in my blog stay in touch for more complex projects,&lt;/p&gt;
</description>
  </item>
  
</channel>
  </rss>