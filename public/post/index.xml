<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
  <title>Posts on Musa R. Nyakerabachi</title>
  <link>/post/</link>
  <description>Recent content in Posts on Musa R. Nyakerabachi</description>
  <generator>Hugo -- gohugo.io</generator>
<language>en-us</language>
<copyright>Musa R. Nyakerabachi&amp;copy; 2020. All rights reserved.</copyright>
<lastBuildDate>Wed, 19 Jan 2022 00:00:00 +0000</lastBuildDate>

<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />


<item>
  <title>Investigating Covid 19 Virus trends.</title>
  <link>/2022/01/19/investigating-covid-19-virus-trends./</link>
  <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
  
<guid>/2022/01/19/investigating-covid-19-virus-trends./</guid>
  <description>


&lt;div id=&#34;objectives.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objectives.&lt;/h2&gt;
&lt;p&gt;This project is about building my skills of the data analysis workflow by evaluating the COVID_19 situation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-exploration&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data exploration&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#loading data set
options(warn = -1)
#install.packages(&amp;quot;tidyverse&amp;quot;)
#install.packages(&amp;quot;readr&amp;quot;)
library(readr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.2     v dplyr   1.0.2
## v tibble  3.0.4     v stringr 1.4.0
## v tidyr   1.1.1     v forcats 0.5.0
## v purrr   0.3.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df&amp;lt;-read.csv(&amp;quot;covid19.csv&amp;quot;)

head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Date Continent_Name Two_Letter_Country_Code Country_Region
## 1 2020-01-20           Asia                      KR    South Korea
## 2 2020-01-22  North America                      US  United States
## 3 2020-01-22  North America                      US  United States
## 4 2020-01-23  North America                      US  United States
## 5 2020-01-23  North America                      US  United States
## 6 2020-01-24           Asia                      KR    South Korea
##   Province_State positive hospitalized recovered death total_tested active
## 1     All States        1            0         0     0            4      0
## 2     All States        1            0         0     0            1      0
## 3     Washington        1            0         0     0            1      0
## 4     All States        1            0         0     0            1      0
## 5     Washington        1            0         0     0            1      0
## 6     All States        2            0         0     0           27      0
##   hospitalizedCurr daily_tested daily_positive
## 1                0            0              0
## 2                0            0              0
## 3                0            0              0
## 4                0            0              0
## 5                0            0              0
## 6                0            5              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#number of rows and columns
dim(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10903    14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#viewing column names and storinh them in a variable
col_names&amp;lt;- colnames(df)

#install.packages(&amp;quot;skimr&amp;quot;) for skimming through the data set

#skimming through the data
library(skimr)
skim(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10903&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: character&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;empty&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;whitespace&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Date&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Continent_Name&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Two_Letter_Country_Code&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;109&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Country_Region&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;109&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Province_State&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;positive&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17768.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93143.46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1026&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7440.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1783570&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;hospitalized&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;766.40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5626.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89590&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;recovered&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2409.22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11254.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;500.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;171883&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;death&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;947.59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5507.24&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;136.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98536&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;total_tested&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195475.72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;819022.18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2147.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30358&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;125285.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16936891&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;active&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4472.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22277.48&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;660.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;280931&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;hospitalizedCurr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;454.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2253.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33004&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;daily_tested&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6841.40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27198.46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-330230&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1048&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4688.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;492276&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▁▇▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;daily_positive&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;497.88&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2354.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-4735&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;264.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;63047&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;col_names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Date&amp;quot;                    &amp;quot;Continent_Name&amp;quot;         
##  [3] &amp;quot;Two_Letter_Country_Code&amp;quot; &amp;quot;Country_Region&amp;quot;         
##  [5] &amp;quot;Province_State&amp;quot;          &amp;quot;positive&amp;quot;               
##  [7] &amp;quot;hospitalized&amp;quot;            &amp;quot;recovered&amp;quot;              
##  [9] &amp;quot;death&amp;quot;                   &amp;quot;total_tested&amp;quot;           
## [11] &amp;quot;active&amp;quot;                  &amp;quot;hospitalizedCurr&amp;quot;       
## [13] &amp;quot;daily_tested&amp;quot;            &amp;quot;daily_positive&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#viewing  and summarising data
glimpse(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 10,903
## Columns: 14
## $ Date                    &amp;lt;chr&amp;gt; &amp;quot;2020-01-20&amp;quot;, &amp;quot;2020-01-22&amp;quot;, &amp;quot;2020-01-22&amp;quot;, &amp;quot;...
## $ Continent_Name          &amp;lt;chr&amp;gt; &amp;quot;Asia&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;...
## $ Two_Letter_Country_Code &amp;lt;chr&amp;gt; &amp;quot;KR&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;KR&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;...
## $ Country_Region          &amp;lt;chr&amp;gt; &amp;quot;South Korea&amp;quot;, &amp;quot;United States&amp;quot;, &amp;quot;United Sta...
## $ Province_State          &amp;lt;chr&amp;gt; &amp;quot;All States&amp;quot;, &amp;quot;All States&amp;quot;, &amp;quot;Washington&amp;quot;, &amp;quot;...
## $ positive                &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 2, 1, 1, 4, 0, 3, 0, 0, 0, 0...
## $ hospitalized            &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ recovered               &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ death                   &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ total_tested            &amp;lt;int&amp;gt; 4, 1, 1, 1, 1, 27, 1, 1, 0, 0, 0, 0, 0, 0, ...
## $ active                  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ hospitalizedCurr        &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ daily_tested            &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ daily_positive          &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above analysis I have used &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;skimr&lt;/code&gt; package to give all overview of the data set. The data set has 14 colums and 10903 rows. It has 5 columns with character and 9 columns with numeric. From above, the data has no missing values.The &lt;code&gt;glimpse&lt;/code&gt; function also show an overview of the all columns but &lt;code&gt;skim&lt;/code&gt; function from &lt;code&gt;skimr&lt;/code&gt; package give more details than what &lt;code&gt;glimpse&lt;/code&gt; function which is in the umbrella of a group of packages hold by &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#filter the ``all states`` province state and remove the Province_state column
covid_allstates &amp;lt;- df %&amp;gt;% filter(Province_State == &amp;quot;All States&amp;quot;) %&amp;gt;% select(-Province_State)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above snippet of code we have removed one column and it consist only All state row.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;isolating-the-columns-we-need&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Isolating the Columns We Need&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Creating a dataset for the daily columns from &lt;code&gt;covid_df_all_states&lt;/code&gt; dataframe&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s recall the description of the dataset’s columns.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;Date&lt;/code&gt;: Date&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Continent_Name&lt;/code&gt;: Continent names&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Two_Letter_Country_Code&lt;/code&gt;: Country codes&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Country_Region&lt;/code&gt;: Country names&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Province_State&lt;/code&gt;: States/province names; value is &lt;code&gt;All States&lt;/code&gt; when state/provincial level data is not available&lt;/li&gt;
&lt;li&gt;&lt;code&gt;positive&lt;/code&gt;: Cumulative number of positive cases reported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;active&lt;/code&gt;: Number of actively cases on that &lt;strong&gt;day&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hospitalized&lt;/code&gt;: Cumulative number of hospitalized cases reported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hospitalizedCurr&lt;/code&gt;: Number of actively hospitalized cases on that &lt;strong&gt;day&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;recovered&lt;/code&gt;: Cumulative number of recovered cases reported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;death&lt;/code&gt;: Cumulative number of deaths reported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;total_tested&lt;/code&gt;: Cumulative number of tests conducted.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;daily_tested&lt;/code&gt;: Number of tests conducted on the &lt;strong&gt;day&lt;/strong&gt;; if daily data is unavailable, daily tested is averaged across number of days in between.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;daily_positive&lt;/code&gt;: Number of positive cases reported on the &lt;strong&gt;day&lt;/strong&gt;; if daily data is unavailable, daily positive is averaged across number of days in.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Selecting the columns with cumulative numbers
# Selecting the columns with cumulative numbers
covid_df_all_states_daily &amp;lt;- df %&amp;gt;% 
  select(Date, Country_Region, active, hospitalizedCurr, daily_tested, daily_positive)
head(covid_df_all_states_daily)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Date Country_Region active hospitalizedCurr daily_tested daily_positive
## 1 2020-01-20    South Korea      0                0            0              0
## 2 2020-01-22  United States      0                0            0              0
## 3 2020-01-22  United States      0                0            0              0
## 4 2020-01-23  United States      0                0            0              0
## 5 2020-01-23  United States      0                0            0              0
## 6 2020-01-24    South Korea      0                0            5              0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extracting-the-top-ten-countries-in-the-number-of-tested-cases&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Extracting the Top Ten countries in the number of tested cases&lt;/h1&gt;
&lt;div id=&#34;summarizing-the-data-based-on-the-country_region-column.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summarizing the data based on the &lt;code&gt;Country_Region&lt;/code&gt; column.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_df_all_states_daily_sum &amp;lt;- covid_df_all_states_daily %&amp;gt;% 
  group_by(Country_Region) %&amp;gt;% 
  summarise(tested = sum(daily_tested), 
            positive = sum(daily_positive),
            active = sum(active),
            hospitalized = sum(hospitalizedCurr)) %&amp;gt;% 
  arrange(desc(tested)) #this is equivalent to `arrange(-tested)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_df_all_states_daily_sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 109 x 5
##    Country_Region   tested positive   active hospitalized
##    &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;
##  1 United States  34218115  3660635 27687993      2899151
##  2 Russia         10542266   406368  6924890            0
##  3 Italy           4091291   251710  6202214      1699003
##  4 India           3692851    60959        0            0
##  5 Canada          3314703   181769  1477358            0
##  6 Australia       2658762    14400   269172        14020
##  7 Turkey          2031192   163941  2980960            0
##  8 United Kingdom  1473672   166909        0            0
##  9 Peru             976790    59497        0            0
## 10 Poland           928256    23987   538203            0
## # ... with 99 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Date, Country_Region, active, hospitalizedCurr, daily_tested, daily_positive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;taking-the-top-10&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Taking the top 10&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_top_10 &amp;lt;- head(covid_df_all_states_daily_sum, 10)
covid_top_10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 5
##    Country_Region   tested positive   active hospitalized
##    &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;
##  1 United States  34218115  3660635 27687993      2899151
##  2 Russia         10542266   406368  6924890            0
##  3 Italy           4091291   251710  6202214      1699003
##  4 India           3692851    60959        0            0
##  5 Canada          3314703   181769  1477358            0
##  6 Australia       2658762    14400   269172        14020
##  7 Turkey          2031192   163941  2980960            0
##  8 United Kingdom  1473672   166909        0            0
##  9 Peru             976790    59497        0            0
## 10 Poland           928256    23987   538203            0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-the-highest-positive-against-tested-cases&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Identifying the Highest Positive Against Tested Cases&lt;/h1&gt;
&lt;div id=&#34;getting-vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting vectors&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;countries &amp;lt;- covid_top_10$Country_Region
tested_cases &amp;lt;- covid_top_10$tested
positive_cases &amp;lt;- covid_top_10$positive
active_cases &amp;lt;- covid_top_10$active
hospitalized_cases &amp;lt;- covid_top_10$hospitalized&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;naming-vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naming vectors&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(positive_cases) &amp;lt;- countries
names(tested_cases) &amp;lt;- countries
names(active_cases) &amp;lt;- countries
names(hospitalized_cases) &amp;lt;- countries&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identifying&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;positive_cases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  United States         Russia          Italy          India         Canada 
##        3660635         406368         251710          60959         181769 
##      Australia         Turkey United Kingdom           Peru         Poland 
##          14400         163941         166909          59497          23987&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(positive_cases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4990175&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(positive_cases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 499017.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;positive_cases/sum(positive_cases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  United States         Russia          Italy          India         Canada 
##    0.733568462    0.081433617    0.050441117    0.012215804    0.036425376 
##      Australia         Turkey United Kingdom           Peru         Poland 
##    0.002885670    0.032852756    0.033447524    0.011922828    0.004806845&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;positive_cases/tested_cases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  United States         Russia          Italy          India         Canada 
##    0.106979446    0.038546552    0.061523368    0.016507300    0.054837191 
##      Australia         Turkey United Kingdom           Peru         Poland 
##    0.005416055    0.080711720    0.113260617    0.060910738    0.025840932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;positive_tested_top_3 &amp;lt;- c(&amp;quot;United Kingdom&amp;quot; = 0.11, &amp;quot;United States&amp;quot; = 0.10, &amp;quot;Turkey&amp;quot; = 0.08)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;keeping-relevant-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Keeping relevant information&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating vectors
united_kingdom &amp;lt;- c(0.11, 1473672, 166909, 0, 0)
united_states &amp;lt;- c(0.10, 17282363, 1877179, 0, 0)
turkey &amp;lt;- c(0.08, 2031192, 163941, 2980960, 0)
# Creating the matrix covid_mat
covid_mat &amp;lt;- rbind(united_kingdom, united_states, turkey)
# Naming columns
colnames(covid_mat) &amp;lt;- c(&amp;quot;Ratio&amp;quot;, &amp;quot;tested&amp;quot;, &amp;quot;positive&amp;quot;, &amp;quot;active&amp;quot;, &amp;quot;hospitalized&amp;quot;)
#d Displaying the matrix
covid_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Ratio   tested positive  active hospitalized
## united_kingdom  0.11  1473672   166909       0            0
## united_states   0.10 17282363  1877179       0            0
## turkey          0.08  2031192   163941 2980960            0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-all-together&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting all together&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;question &amp;lt;- &amp;quot;Which countries have had the highest number of positive cases against the number of tests?&amp;quot;
answer &amp;lt;- c(&amp;quot;Positive tested cases&amp;quot; = positive_tested_top_3)
datasets &amp;lt;- list(
  original = df,
  allstates = covid_allstates,
  daily = covid_df_all_states_daily,
  top_10 = covid_top_10
)
matrices &amp;lt;- list(covid_mat)
vectors &amp;lt;- list(col_names, countries)
data_structure_list &amp;lt;- list(&amp;quot;dataframe&amp;quot; = datasets, &amp;quot;matrix&amp;quot; = matrices, &amp;quot;vector&amp;quot; = vectors)
covid_analysis_list &amp;lt;- list(question, answer, data_structure_list)
covid_analysis_list[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Positive tested cases.United Kingdom  Positive tested cases.United States 
##                                 0.11                                 0.10 
##         Positive tested cases.Turkey 
##                                 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Airbnb Edinburgh Exploratory Data Analysis.</title>
  <link>/2020/12/18/airbnb-edinburgh-exploratory-data-analysis./</link>
  <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
  
<guid>/2020/12/18/airbnb-edinburgh-exploratory-data-analysis./</guid>
  <description>


&lt;hr /&gt;
&lt;p&gt;In this blog post i’ll be analysing Airbnb dataset from Edinburg city distributed by &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34;&gt;Inside Airbnb&lt;/a&gt; and that is hosted in &lt;a href=&#34;https://github.com/rstudio-education/dsbox&#34;&gt;dsbox&lt;/a&gt;
package. The datset covers 13 areas as we shall see below.let’s start by describing the data.&lt;/p&gt;
&lt;div id=&#34;data-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Description :&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Column Name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;id&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ID number of the listing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;price&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Price, in GBP, for one night stay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;neighbourhood&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of people listing accommodates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;bathrooms&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of bathrooms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;bedrooms&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of bedrooms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;beds&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of beds (which can be different than the number of bedrooms)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;review_scores_rating&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Average rating of property&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;number_of_reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of reviews&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;listing_url&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Listing URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;accommodates&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of people listing accommodates&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;First I’ll begin by loading all the packages that I’ll use in this my EDA post. If you dont have them install you’ll use &lt;code&gt;install.packages(&#34;package name&#34;)&lt;/code&gt; operation to have them installed or you uncomment the installation operation in the package section below.Sometimes installing a package that is not in cran i.e install from github may not throw an error during building process, in that case you will need to add an optional argument to ensure the installation in accepted i.e you attach &lt;code&gt;INSATLL-opts = c(&#34;--no-multiarch)&lt;/code&gt; to your package installation from github.&lt;/p&gt;
&lt;p&gt;Having said that let’s dive in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install required packages
###uncoment to install packages if you don&amp;#39;t have them installed.
#devtools::install_github(&amp;quot;rstudio-education/dsbox&amp;quot;)
#install.packages(&amp;quot;gmodels&amp;quot;)
#install.packages(&amp;quot;skimr&amp;quot;)
#install.packages(&amp;quot;tidyverse&amp;quot;)
#loading packages
library(dsbox)
library(gmodels)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.2     v purrr   0.3.4
## v tibble  3.0.4     v dplyr   1.0.2
## v tidyr   1.1.1     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(skimr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I’ll load the edinburg dataset into my working directory and skim through it to see how the dataset is structured using &lt;code&gt;skimr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load data

 data(edibnb)
skim(edibnb)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;edibnb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;13245&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: character&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;empty&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;whitespace&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;neighbourhood&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2294&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;listing_url&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13245&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;id&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20077241.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9844763.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15420&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13279107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20171841&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27397925&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36066014&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▅▆▇▇▇&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;price&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;199&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.98&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;97.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;110&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;999&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;accommodates&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▂▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bathrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.55&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▂▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bedrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.93&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;beds&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;review_scores_rating&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2177&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.84&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;97&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▁▁▁▇&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;number_of_reviews&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;63.91&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;773&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From above the data set has 13245 number of rows and it also has 10 columns. The daset has two character columns and 8 numeric columns evident from above &lt;code&gt;skim&lt;/code&gt; function. The dataset has also missing values in some columns with only &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;listing_url&lt;/code&gt;, &lt;code&gt;accommodates&lt;/code&gt;, &lt;code&gt;number_of_reviews&lt;/code&gt; columns not having missing values. The &lt;code&gt;neighbourhood&lt;/code&gt; column has 13 unique entries i.e 13 different locations with most people paying averagely 97 dollars also the standard deviation, quantiles and distribution of numeric columns have been captured.&lt;/p&gt;
&lt;p&gt;Next let’s see the 13 distinct entries in the &lt;code&gt;neighbourhood&lt;/code&gt; column using &lt;code&gt;distinct&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt; package bundled in &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#distinct elements
distinct(edibnb, edibnb$neighbourhood)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 1
##    `edibnb$neighbourhood`
##    &amp;lt;chr&amp;gt;                 
##  1 New Town              
##  2 Southside             
##  3 &amp;lt;NA&amp;gt;                  
##  4 Leith                 
##  5 Old Town              
##  6 West End              
##  7 Haymarket             
##  8 Morningside           
##  9 Newington             
## 10 Marchmont             
## 11 Cannonmills           
## 12 Tollcross             
## 13 Bruntsfield           
## 14 Stockbridge&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above an &lt;code&gt;NA&lt;/code&gt; value has appeared which will alarm my data handling criteria with precaution attached.&lt;/p&gt;
&lt;p&gt;Next let me see how the 13 areas are distributed using &lt;code&gt;crossTable&lt;/code&gt; function from &lt;code&gt;gmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CrossTable(edibnb$neighbourhood)#neighbourhood areas distribution&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  10951 
## 
##  
##             | Bruntsfield | Cannonmills |   Haymarket |       Leith |   Marchmont | 
##             |-------------|-------------|-------------|-------------|-------------|
##             |         290 |         485 |        1026 |        3134 |         730 | 
##             |       0.026 |       0.044 |       0.094 |       0.286 |       0.067 | 
##             |-------------|-------------|-------------|-------------|-------------|
## 
## 
##             | Morningside |    New Town |   Newington |    Old Town |   Southside | 
##             |-------------|-------------|-------------|-------------|-------------|
##             |         346 |        1058 |         439 |        1283 |         481 | 
##             |       0.032 |       0.097 |       0.040 |       0.117 |       0.044 | 
##             |-------------|-------------|-------------|-------------|-------------|
## 
## 
##             | Stockbridge |   Tollcross |    West End | 
##             |-------------|-------------|-------------|
##             |         587 |         270 |         822 | 
##             |       0.054 |       0.025 |       0.075 | 
##             |-------------|-------------|-------------|
## 
## 
## 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The `&lt;code&gt;crossTable&lt;/code&gt; function has only captures 10951 observation discarding the &lt;code&gt;NA&lt;/code&gt; values. &lt;code&gt;Leith&lt;/code&gt; had highest number of accommodation while &lt;code&gt;Bruntsfield&lt;/code&gt; having least number of people.&lt;/p&gt;
&lt;p&gt;Let me inspect the average price people in 13 areas paid and infer the expensive area to accommodate people and also the cheapest.To handle &lt;code&gt;NA&lt;/code&gt; value I’ll use &lt;code&gt;na.rm = T&lt;/code&gt; while calculating the mean price.I’ll use &lt;code&gt;arrange&lt;/code&gt; function from &lt;code&gt;dplyr&lt;/code&gt; package and arrange them from expensive to cheapest using &lt;code&gt;desc&lt;/code&gt; argument in the &lt;code&gt;arrange&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edibnb %&amp;gt;% group_by(neighbourhood) %&amp;gt;% 
  summarise(mean = mean(price, na.rm = T), n = n()) %&amp;gt;% 
  arrange(desc(mean))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 3
##    neighbourhood  mean     n
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1 New Town      136.   1058
##  2 Old Town      128.   1283
##  3 West End      116.    822
##  4 Marchmont     111.    730
##  5 Southside     107.    481
##  6 Cannonmills   104.    485
##  7 Stockbridge   104.    587
##  8 Bruntsfield    99.4   290
##  9 Newington      98.4   439
## 10 Tollcross      90.8   270
## 11 Haymarket      84.8  1026
## 12 Leith          83.8  3134
## 13 Morningside    82.9   346
## 14 &amp;lt;NA&amp;gt;           73.5  2294&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;New Town&lt;/code&gt; is considered expensive area to look for accommodation while &lt;code&gt;Morningside&lt;/code&gt; seems to be more friendly to people loking for cheap houses.Also from above the &lt;code&gt;n()&lt;/code&gt; argumet gives the total observation of each locality same as in &lt;code&gt;crossTable&lt;/code&gt; function above.&lt;/p&gt;
&lt;p&gt;Next let me see how the beds are distributed in different houses and give the traveller better information in considering the amount of money he has to spend.I’ll first use &lt;code&gt;crossTable&lt;/code&gt; function from &lt;code&gt;gmodels&lt;/code&gt; package to look at how the beds are distributed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CrossTable(edibnb$beds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  13230 
## 
##  
##           |         0 |         1 |         2 |         3 |         4 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |        42 |      6164 |      3710 |      1801 |       832 | 
##           |     0.003 |     0.466 |     0.280 |     0.136 |     0.063 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |         5 |         6 |         7 |         8 |         9 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |       329 |       159 |        47 |        52 |        20 | 
##           |     0.025 |     0.012 |     0.004 |     0.004 |     0.002 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |        10 |        11 |        12 |        13 |        14 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |        15 |         7 |        11 |         2 |         7 | 
##           |     0.001 |     0.001 |     0.001 |     0.000 |     0.001 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |        15 |        16 |        17 |        20 |        21 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |         5 |        17 |         2 |         3 |         1 | 
##           |     0.000 |     0.001 |     0.000 |     0.000 |     0.000 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |        22 |        24 |        25 |        30 | 
##           |-----------|-----------|-----------|-----------|
##           |         1 |         1 |         1 |         1 | 
##           |     0.000 |     0.000 |     0.000 |     0.000 | 
##           |-----------|-----------|-----------|-----------|
## 
## 
## 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above code snippet there is house with no bed, while most houses had one bed and the is a house with 30 beds with only one person accommodation.&lt;/p&gt;
&lt;p&gt;Next let’s see the house which was expensive taking bed factor taken into consideration.I’ll use the same &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edibnb %&amp;gt;% group_by(beds) %&amp;gt;% 
  summarise(mean = mean(price, na.rm = T), n = n()) %&amp;gt;% 
  arrange(desc(mean))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 25 x 3
##     beds  mean     n
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1    20  880      3
##  2    14  386.     7
##  3    12  384.    11
##  4    10  379.    15
##  5    16  371     17
##  6    15  304.     5
##  7     8  256.    52
##  8    13  240      2
##  9     9  234     20
## 10     7  230.    47
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above a 20 bed house was considered expensive, a person paying roughly 880 dollars and only 3 people listed for the house.Having seen the most expensive house considering number of beds next I’ll look at the house that was cheap considering number of beds a house had, in this case I’ll drop &lt;code&gt;desc&lt;/code&gt; in &lt;code&gt;arrange&lt;/code&gt; function to arrange the number of beds in ascending order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edibnb %&amp;gt;% group_by(beds) %&amp;gt;% 
  summarise(mean = mean(price, na.rm = T), n = n()) %&amp;gt;% 
  arrange(mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 25 x 3
##     beds  mean     n
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1     1  65.0  6164
##  2     2  97.5  3710
##  3     0 110.     42
##  4    NA 121.     15
##  5     3 128.   1801
##  6    24 135       1
##  7     4 167.    832
##  8    17 200       2
##  9     6 202.    159
## 10    11 212.      7
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above a 1 bed house was considered cheap only costing roughly 65 dollars and it is the one most of the people opted for in Edinburg town.&lt;/p&gt;
&lt;p&gt;Next I’ll look at the number of people a house can accommodate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CrossTable(edibnb$accommodates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  13245 
## 
##  
##           |         1 |         2 |         3 |         4 |         5 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |       754 |      5390 |       754 |      3451 |       640 | 
##           |     0.057 |     0.407 |     0.057 |     0.261 |     0.048 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |         6 |         7 |         8 |         9 |        10 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |      1386 |       181 |       373 |        59 |       133 | 
##           |     0.105 |     0.014 |     0.028 |     0.004 |     0.010 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |        11 |        12 |        13 |        14 |        15 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |        20 |        39 |         7 |        14 |         7 | 
##           |     0.002 |     0.003 |     0.001 |     0.001 |     0.001 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |        16 |        19 | 
##           |-----------|-----------|
##           |        36 |         1 | 
##           |     0.003 |     0.000 | 
##           |-----------|-----------|
## 
## 
## 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s like most houses in Edinburg can only accomodate 2 people while there is also a house that could accommodate 19 people.&lt;/p&gt;
&lt;p&gt;Let me look at the price each house accommodation costed using below snippet of code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edibnb %&amp;gt;% group_by(accommodates) %&amp;gt;% 
  summarise(mean = mean(price, na.rm = T), n = n()) %&amp;gt;% 
  arrange(mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 3
##    accommodates  mean     n
##           &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1            1  52.7   754
##  2            2  64.0  5390
##  3            3  82.2   754
##  4            4 107.   3451
##  5            5 121.    640
##  6            6 145.   1386
##  7            7 180.    181
##  8            8 213.    373
##  9            9 236.     59
## 10           15 253       7
## 11           10 254.    133
## 12           11 297.     20
## 13           12 344.     39
## 14           13 346.      7
## 15           14 353.     14
## 16           16 383.     36
## 17           19 NaN       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above the house that accomodates 1 person is cheap while the one accomodates 19 people considered expensive.&lt;/p&gt;
&lt;p&gt;Next I’ll look at the number of bedrooms most of the houses had.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CrossTable(edibnb$bedrooms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  13241 
## 
##  
##           |         0 |         1 |         2 |         3 |         4 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |       266 |      7552 |      3675 |      1196 |       379 | 
##           |     0.020 |     0.570 |     0.278 |     0.090 |     0.029 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |         5 |         6 |         7 |         8 |         9 | 
##           |-----------|-----------|-----------|-----------|-----------|
##           |       127 |        29 |         8 |         4 |         2 | 
##           |     0.010 |     0.002 |     0.001 |     0.000 |     0.000 | 
##           |-----------|-----------|-----------|-----------|-----------|
## 
## 
##           |        10 |        13 | 
##           |-----------|-----------|
##           |         1 |         2 | 
##           |     0.000 |     0.000 | 
##           |-----------|-----------|
## 
## 
## 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most of the houses are 1 bedroom house whose their number is 7552 , while least being a 10 bedroom house wich is only 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edibnb %&amp;gt;% group_by(bedrooms) %&amp;gt;% 
  summarise(mean = mean(price, na.rm = T), n = n()) %&amp;gt;% 
  arrange(mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 3
##    bedrooms  mean     n
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1        1  68.5  7552
##  2        0  78.7   266
##  3       13  85       2
##  4       NA  99.5     4
##  5        2 115.   3675
##  6        3 163.   1196
##  7        4 228.    379
##  8       10 300       1
##  9        5 300.    127
## 10        7 333.      8
## 11        6 379.     29
## 12        8 423       4
## 13        9 474       2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s like most of houses are 1 bedroom house which cost only 69 dollars while an expensive one costed 474 dollars a 9 bedroom house.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-visualization.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Visualization.&lt;/h2&gt;
&lt;p&gt;Next I’ll visualize the dataset using &lt;code&gt;ggplot&lt;/code&gt; function from &lt;code&gt;tidyverse&lt;/code&gt;.I’ll visualise the &lt;code&gt;nieghbourhood&lt;/code&gt; column against &lt;code&gt;beds&lt;/code&gt; using a bar chat and see the relationship graphically.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = neighbourhood, y = beds, fill = neighbourhood)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 15 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above grapgh &lt;code&gt;Leith&lt;/code&gt; has the highest number of beds and &lt;code&gt;Tollcross&lt;/code&gt; having least.Next I’ll visualise visualise &lt;code&gt;neighbourhood&lt;/code&gt; against &lt;code&gt;bathrooms&lt;/code&gt; using a bar chat and deducerelationship from below snippet of code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = neighbourhood, y = bathrooms, fill = neighbourhood)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 12 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above graph &lt;code&gt;Leith&lt;/code&gt; has highest number of beds with &lt;code&gt;Tollcross&lt;/code&gt; having least, next I’ll compare &lt;code&gt;neighbourhood&lt;/code&gt; against &lt;code&gt;price&lt;/code&gt; to see where houses are expensive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = neighbourhood, y = price, fill = neighbourhood)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 199 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above houses in &lt;code&gt;Leith&lt;/code&gt; are expensive with least being&lt;code&gt;Tollcross&lt;/code&gt;.Next I’ll compare the &lt;code&gt;neighbourhood&lt;/code&gt; against price wrapping &lt;code&gt;beds&lt;/code&gt; to see the relationship.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = neighbourhood, y = price, fill = neighbourhood)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  facet_wrap( ~ beds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 199 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s evident that people prefer to spend money in renting houses with beds arnging 1 to 4 approximately while the rest having least people renting for a day.
Next I’ll visualize those houses in the &lt;code&gt;neighbourhood&lt;/code&gt; against &lt;code&gt;price&lt;/code&gt; taking into consideration on &lt;code&gt;accommodation&lt;/code&gt; factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = neighbourhood, y = price, fill = neighbourhood)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  facet_wrap( ~ accommodates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 199 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s like most houses accommodates roughly 2-6 people with other accomomdating very few people.Lastly I’ll see how the price is distributed generally among the whole of Edinburg town.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = price)) + 
  geom_histogram(binwidth = 50) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 199 rows containing non-finite values (stat_bin).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s like the price is skewed to the like I’ll have to intoduce logarithmic function to ensure there is no skewness as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = edibnb, aes(x = price)) + 
  geom_histogram() + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Transformation introduced infinite values in continuous x-axis&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 201 rows containing non-finite values (stat_bin).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-Edinburgh-Exploratory-Data-Analysis_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now the price have been transformed and averagely the house in edinburg costs 100 dollars.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References.&lt;/h2&gt;
&lt;p&gt;1.Chapter 10: Interactive tutorials &lt;a href=&#34;https://datasciencebox.org/interactive-tutorials.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2.Data Science in the tidyverse by Amelia McNamara &lt;a href=&#34;https://github.com/cwickham/data-science-in-tidyverse&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Machine Learning from Disaster</title>
  <link>/2020/07/28/machine-learning-from-disaster/</link>
  <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
  
<guid>/2020/07/28/machine-learning-from-disaster/</guid>
  <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/datatables-css/datatables-crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/datatables-binding/datatables.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;machine-learning-from-disaster.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Machine Learning from Disaster.&lt;/h1&gt;
&lt;div id=&#34;part-1exploratory-data-analysis.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1:Exploratory Data Analysis.&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;business-understanding.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. BUSINESS UNDERSTANDING.&lt;/h2&gt;
&lt;p&gt;Source:&lt;a href=&#34;https://www.kaggle.com/c/titanic/&#34;&gt;Machine Learning Titanic Competition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data: &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Titanic Data Set from Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;context&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Context:&lt;/h3&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://www.kaggle.com/c/titanic/discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-statement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem statement :&lt;/h3&gt;
&lt;p&gt;Using machine learning to create a model that predicts which passengers survived the Titanic shipwreck.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 Data Description :&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Column Name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Key&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;survival&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Survival&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0&lt;/strong&gt; = No, &lt;strong&gt;1&lt;/strong&gt; = Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;pclass&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Ticket class&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;1&lt;/strong&gt; = 1st, &lt;strong&gt;2&lt;/strong&gt; = 2nd, &lt;strong&gt;3&lt;/strong&gt; = 3rd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;sex&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;C&lt;/strong&gt; = Cherbourg, &lt;strong&gt;Q&lt;/strong&gt; = Queenstown, &lt;strong&gt;S&lt;/strong&gt; = Southampton&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Age&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Age in years&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;sibsp&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of siblings / spouses aboard the Titanic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;parch&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of parents / children aboard the Titanic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;ticket&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Ticket number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;fare&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Passenger fare&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;cabin&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Cabin number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;embarked&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Port of Embarkation&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;First I’ll load the data file using user input option &lt;code&gt;file.choose()&lt;/code&gt;.The data is given in two set both training and test set.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_train &amp;lt;- read.csv (file.choose(), stringsAsFactors = F,na.strings=c(&amp;quot;&amp;quot;,&amp;quot;NA&amp;quot;,&amp;quot; &amp;quot;))
df_test &amp;lt;- read.csv (file.choose(), stringsAsFactors = F,na.strings=c(&amp;quot;&amp;quot;,&amp;quot;NA&amp;quot;,&amp;quot; &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll load the training set and test set data using &lt;code&gt;DT&lt;/code&gt; package as shown below.I’ll view only the first eight columns for easy page view but if you need whole columns for the two data sets remove &lt;code&gt;[,1:8]&lt;/code&gt; from the code snippet below&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

DT::datatable(df_train[,1:8])&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;,&#34;11&#34;,&#34;12&#34;,&#34;13&#34;,&#34;14&#34;,&#34;15&#34;,&#34;16&#34;,&#34;17&#34;,&#34;18&#34;,&#34;19&#34;,&#34;20&#34;,&#34;21&#34;,&#34;22&#34;,&#34;23&#34;,&#34;24&#34;,&#34;25&#34;,&#34;26&#34;,&#34;27&#34;,&#34;28&#34;,&#34;29&#34;,&#34;30&#34;,&#34;31&#34;,&#34;32&#34;,&#34;33&#34;,&#34;34&#34;,&#34;35&#34;,&#34;36&#34;,&#34;37&#34;,&#34;38&#34;,&#34;39&#34;,&#34;40&#34;,&#34;41&#34;,&#34;42&#34;,&#34;43&#34;,&#34;44&#34;,&#34;45&#34;,&#34;46&#34;,&#34;47&#34;,&#34;48&#34;,&#34;49&#34;,&#34;50&#34;,&#34;51&#34;,&#34;52&#34;,&#34;53&#34;,&#34;54&#34;,&#34;55&#34;,&#34;56&#34;,&#34;57&#34;,&#34;58&#34;,&#34;59&#34;,&#34;60&#34;,&#34;61&#34;,&#34;62&#34;,&#34;63&#34;,&#34;64&#34;,&#34;65&#34;,&#34;66&#34;,&#34;67&#34;,&#34;68&#34;,&#34;69&#34;,&#34;70&#34;,&#34;71&#34;,&#34;72&#34;,&#34;73&#34;,&#34;74&#34;,&#34;75&#34;,&#34;76&#34;,&#34;77&#34;,&#34;78&#34;,&#34;79&#34;,&#34;80&#34;,&#34;81&#34;,&#34;82&#34;,&#34;83&#34;,&#34;84&#34;,&#34;85&#34;,&#34;86&#34;,&#34;87&#34;,&#34;88&#34;,&#34;89&#34;,&#34;90&#34;,&#34;91&#34;,&#34;92&#34;,&#34;93&#34;,&#34;94&#34;,&#34;95&#34;,&#34;96&#34;,&#34;97&#34;,&#34;98&#34;,&#34;99&#34;,&#34;100&#34;,&#34;101&#34;,&#34;102&#34;,&#34;103&#34;,&#34;104&#34;,&#34;105&#34;,&#34;106&#34;,&#34;107&#34;,&#34;108&#34;,&#34;109&#34;,&#34;110&#34;,&#34;111&#34;,&#34;112&#34;,&#34;113&#34;,&#34;114&#34;,&#34;115&#34;,&#34;116&#34;,&#34;117&#34;,&#34;118&#34;,&#34;119&#34;,&#34;120&#34;,&#34;121&#34;,&#34;122&#34;,&#34;123&#34;,&#34;124&#34;,&#34;125&#34;,&#34;126&#34;,&#34;127&#34;,&#34;128&#34;,&#34;129&#34;,&#34;130&#34;,&#34;131&#34;,&#34;132&#34;,&#34;133&#34;,&#34;134&#34;,&#34;135&#34;,&#34;136&#34;,&#34;137&#34;,&#34;138&#34;,&#34;139&#34;,&#34;140&#34;,&#34;141&#34;,&#34;142&#34;,&#34;143&#34;,&#34;144&#34;,&#34;145&#34;,&#34;146&#34;,&#34;147&#34;,&#34;148&#34;,&#34;149&#34;,&#34;150&#34;,&#34;151&#34;,&#34;152&#34;,&#34;153&#34;,&#34;154&#34;,&#34;155&#34;,&#34;156&#34;,&#34;157&#34;,&#34;158&#34;,&#34;159&#34;,&#34;160&#34;,&#34;161&#34;,&#34;162&#34;,&#34;163&#34;,&#34;164&#34;,&#34;165&#34;,&#34;166&#34;,&#34;167&#34;,&#34;168&#34;,&#34;169&#34;,&#34;170&#34;,&#34;171&#34;,&#34;172&#34;,&#34;173&#34;,&#34;174&#34;,&#34;175&#34;,&#34;176&#34;,&#34;177&#34;,&#34;178&#34;,&#34;179&#34;,&#34;180&#34;,&#34;181&#34;,&#34;182&#34;,&#34;183&#34;,&#34;184&#34;,&#34;185&#34;,&#34;186&#34;,&#34;187&#34;,&#34;188&#34;,&#34;189&#34;,&#34;190&#34;,&#34;191&#34;,&#34;192&#34;,&#34;193&#34;,&#34;194&#34;,&#34;195&#34;,&#34;196&#34;,&#34;197&#34;,&#34;198&#34;,&#34;199&#34;,&#34;200&#34;,&#34;201&#34;,&#34;202&#34;,&#34;203&#34;,&#34;204&#34;,&#34;205&#34;,&#34;206&#34;,&#34;207&#34;,&#34;208&#34;,&#34;209&#34;,&#34;210&#34;,&#34;211&#34;,&#34;212&#34;,&#34;213&#34;,&#34;214&#34;,&#34;215&#34;,&#34;216&#34;,&#34;217&#34;,&#34;218&#34;,&#34;219&#34;,&#34;220&#34;,&#34;221&#34;,&#34;222&#34;,&#34;223&#34;,&#34;224&#34;,&#34;225&#34;,&#34;226&#34;,&#34;227&#34;,&#34;228&#34;,&#34;229&#34;,&#34;230&#34;,&#34;231&#34;,&#34;232&#34;,&#34;233&#34;,&#34;234&#34;,&#34;235&#34;,&#34;236&#34;,&#34;237&#34;,&#34;238&#34;,&#34;239&#34;,&#34;240&#34;,&#34;241&#34;,&#34;242&#34;,&#34;243&#34;,&#34;244&#34;,&#34;245&#34;,&#34;246&#34;,&#34;247&#34;,&#34;248&#34;,&#34;249&#34;,&#34;250&#34;,&#34;251&#34;,&#34;252&#34;,&#34;253&#34;,&#34;254&#34;,&#34;255&#34;,&#34;256&#34;,&#34;257&#34;,&#34;258&#34;,&#34;259&#34;,&#34;260&#34;,&#34;261&#34;,&#34;262&#34;,&#34;263&#34;,&#34;264&#34;,&#34;265&#34;,&#34;266&#34;,&#34;267&#34;,&#34;268&#34;,&#34;269&#34;,&#34;270&#34;,&#34;271&#34;,&#34;272&#34;,&#34;273&#34;,&#34;274&#34;,&#34;275&#34;,&#34;276&#34;,&#34;277&#34;,&#34;278&#34;,&#34;279&#34;,&#34;280&#34;,&#34;281&#34;,&#34;282&#34;,&#34;283&#34;,&#34;284&#34;,&#34;285&#34;,&#34;286&#34;,&#34;287&#34;,&#34;288&#34;,&#34;289&#34;,&#34;290&#34;,&#34;291&#34;,&#34;292&#34;,&#34;293&#34;,&#34;294&#34;,&#34;295&#34;,&#34;296&#34;,&#34;297&#34;,&#34;298&#34;,&#34;299&#34;,&#34;300&#34;,&#34;301&#34;,&#34;302&#34;,&#34;303&#34;,&#34;304&#34;,&#34;305&#34;,&#34;306&#34;,&#34;307&#34;,&#34;308&#34;,&#34;309&#34;,&#34;310&#34;,&#34;311&#34;,&#34;312&#34;,&#34;313&#34;,&#34;314&#34;,&#34;315&#34;,&#34;316&#34;,&#34;317&#34;,&#34;318&#34;,&#34;319&#34;,&#34;320&#34;,&#34;321&#34;,&#34;322&#34;,&#34;323&#34;,&#34;324&#34;,&#34;325&#34;,&#34;326&#34;,&#34;327&#34;,&#34;328&#34;,&#34;329&#34;,&#34;330&#34;,&#34;331&#34;,&#34;332&#34;,&#34;333&#34;,&#34;334&#34;,&#34;335&#34;,&#34;336&#34;,&#34;337&#34;,&#34;338&#34;,&#34;339&#34;,&#34;340&#34;,&#34;341&#34;,&#34;342&#34;,&#34;343&#34;,&#34;344&#34;,&#34;345&#34;,&#34;346&#34;,&#34;347&#34;,&#34;348&#34;,&#34;349&#34;,&#34;350&#34;,&#34;351&#34;,&#34;352&#34;,&#34;353&#34;,&#34;354&#34;,&#34;355&#34;,&#34;356&#34;,&#34;357&#34;,&#34;358&#34;,&#34;359&#34;,&#34;360&#34;,&#34;361&#34;,&#34;362&#34;,&#34;363&#34;,&#34;364&#34;,&#34;365&#34;,&#34;366&#34;,&#34;367&#34;,&#34;368&#34;,&#34;369&#34;,&#34;370&#34;,&#34;371&#34;,&#34;372&#34;,&#34;373&#34;,&#34;374&#34;,&#34;375&#34;,&#34;376&#34;,&#34;377&#34;,&#34;378&#34;,&#34;379&#34;,&#34;380&#34;,&#34;381&#34;,&#34;382&#34;,&#34;383&#34;,&#34;384&#34;,&#34;385&#34;,&#34;386&#34;,&#34;387&#34;,&#34;388&#34;,&#34;389&#34;,&#34;390&#34;,&#34;391&#34;,&#34;392&#34;,&#34;393&#34;,&#34;394&#34;,&#34;395&#34;,&#34;396&#34;,&#34;397&#34;,&#34;398&#34;,&#34;399&#34;,&#34;400&#34;,&#34;401&#34;,&#34;402&#34;,&#34;403&#34;,&#34;404&#34;,&#34;405&#34;,&#34;406&#34;,&#34;407&#34;,&#34;408&#34;,&#34;409&#34;,&#34;410&#34;,&#34;411&#34;,&#34;412&#34;,&#34;413&#34;,&#34;414&#34;,&#34;415&#34;,&#34;416&#34;,&#34;417&#34;,&#34;418&#34;,&#34;419&#34;,&#34;420&#34;,&#34;421&#34;,&#34;422&#34;,&#34;423&#34;,&#34;424&#34;,&#34;425&#34;,&#34;426&#34;,&#34;427&#34;,&#34;428&#34;,&#34;429&#34;,&#34;430&#34;,&#34;431&#34;,&#34;432&#34;,&#34;433&#34;,&#34;434&#34;,&#34;435&#34;,&#34;436&#34;,&#34;437&#34;,&#34;438&#34;,&#34;439&#34;,&#34;440&#34;,&#34;441&#34;,&#34;442&#34;,&#34;443&#34;,&#34;444&#34;,&#34;445&#34;,&#34;446&#34;,&#34;447&#34;,&#34;448&#34;,&#34;449&#34;,&#34;450&#34;,&#34;451&#34;,&#34;452&#34;,&#34;453&#34;,&#34;454&#34;,&#34;455&#34;,&#34;456&#34;,&#34;457&#34;,&#34;458&#34;,&#34;459&#34;,&#34;460&#34;,&#34;461&#34;,&#34;462&#34;,&#34;463&#34;,&#34;464&#34;,&#34;465&#34;,&#34;466&#34;,&#34;467&#34;,&#34;468&#34;,&#34;469&#34;,&#34;470&#34;,&#34;471&#34;,&#34;472&#34;,&#34;473&#34;,&#34;474&#34;,&#34;475&#34;,&#34;476&#34;,&#34;477&#34;,&#34;478&#34;,&#34;479&#34;,&#34;480&#34;,&#34;481&#34;,&#34;482&#34;,&#34;483&#34;,&#34;484&#34;,&#34;485&#34;,&#34;486&#34;,&#34;487&#34;,&#34;488&#34;,&#34;489&#34;,&#34;490&#34;,&#34;491&#34;,&#34;492&#34;,&#34;493&#34;,&#34;494&#34;,&#34;495&#34;,&#34;496&#34;,&#34;497&#34;,&#34;498&#34;,&#34;499&#34;,&#34;500&#34;,&#34;501&#34;,&#34;502&#34;,&#34;503&#34;,&#34;504&#34;,&#34;505&#34;,&#34;506&#34;,&#34;507&#34;,&#34;508&#34;,&#34;509&#34;,&#34;510&#34;,&#34;511&#34;,&#34;512&#34;,&#34;513&#34;,&#34;514&#34;,&#34;515&#34;,&#34;516&#34;,&#34;517&#34;,&#34;518&#34;,&#34;519&#34;,&#34;520&#34;,&#34;521&#34;,&#34;522&#34;,&#34;523&#34;,&#34;524&#34;,&#34;525&#34;,&#34;526&#34;,&#34;527&#34;,&#34;528&#34;,&#34;529&#34;,&#34;530&#34;,&#34;531&#34;,&#34;532&#34;,&#34;533&#34;,&#34;534&#34;,&#34;535&#34;,&#34;536&#34;,&#34;537&#34;,&#34;538&#34;,&#34;539&#34;,&#34;540&#34;,&#34;541&#34;,&#34;542&#34;,&#34;543&#34;,&#34;544&#34;,&#34;545&#34;,&#34;546&#34;,&#34;547&#34;,&#34;548&#34;,&#34;549&#34;,&#34;550&#34;,&#34;551&#34;,&#34;552&#34;,&#34;553&#34;,&#34;554&#34;,&#34;555&#34;,&#34;556&#34;,&#34;557&#34;,&#34;558&#34;,&#34;559&#34;,&#34;560&#34;,&#34;561&#34;,&#34;562&#34;,&#34;563&#34;,&#34;564&#34;,&#34;565&#34;,&#34;566&#34;,&#34;567&#34;,&#34;568&#34;,&#34;569&#34;,&#34;570&#34;,&#34;571&#34;,&#34;572&#34;,&#34;573&#34;,&#34;574&#34;,&#34;575&#34;,&#34;576&#34;,&#34;577&#34;,&#34;578&#34;,&#34;579&#34;,&#34;580&#34;,&#34;581&#34;,&#34;582&#34;,&#34;583&#34;,&#34;584&#34;,&#34;585&#34;,&#34;586&#34;,&#34;587&#34;,&#34;588&#34;,&#34;589&#34;,&#34;590&#34;,&#34;591&#34;,&#34;592&#34;,&#34;593&#34;,&#34;594&#34;,&#34;595&#34;,&#34;596&#34;,&#34;597&#34;,&#34;598&#34;,&#34;599&#34;,&#34;600&#34;,&#34;601&#34;,&#34;602&#34;,&#34;603&#34;,&#34;604&#34;,&#34;605&#34;,&#34;606&#34;,&#34;607&#34;,&#34;608&#34;,&#34;609&#34;,&#34;610&#34;,&#34;611&#34;,&#34;612&#34;,&#34;613&#34;,&#34;614&#34;,&#34;615&#34;,&#34;616&#34;,&#34;617&#34;,&#34;618&#34;,&#34;619&#34;,&#34;620&#34;,&#34;621&#34;,&#34;622&#34;,&#34;623&#34;,&#34;624&#34;,&#34;625&#34;,&#34;626&#34;,&#34;627&#34;,&#34;628&#34;,&#34;629&#34;,&#34;630&#34;,&#34;631&#34;,&#34;632&#34;,&#34;633&#34;,&#34;634&#34;,&#34;635&#34;,&#34;636&#34;,&#34;637&#34;,&#34;638&#34;,&#34;639&#34;,&#34;640&#34;,&#34;641&#34;,&#34;642&#34;,&#34;643&#34;,&#34;644&#34;,&#34;645&#34;,&#34;646&#34;,&#34;647&#34;,&#34;648&#34;,&#34;649&#34;,&#34;650&#34;,&#34;651&#34;,&#34;652&#34;,&#34;653&#34;,&#34;654&#34;,&#34;655&#34;,&#34;656&#34;,&#34;657&#34;,&#34;658&#34;,&#34;659&#34;,&#34;660&#34;,&#34;661&#34;,&#34;662&#34;,&#34;663&#34;,&#34;664&#34;,&#34;665&#34;,&#34;666&#34;,&#34;667&#34;,&#34;668&#34;,&#34;669&#34;,&#34;670&#34;,&#34;671&#34;,&#34;672&#34;,&#34;673&#34;,&#34;674&#34;,&#34;675&#34;,&#34;676&#34;,&#34;677&#34;,&#34;678&#34;,&#34;679&#34;,&#34;680&#34;,&#34;681&#34;,&#34;682&#34;,&#34;683&#34;,&#34;684&#34;,&#34;685&#34;,&#34;686&#34;,&#34;687&#34;,&#34;688&#34;,&#34;689&#34;,&#34;690&#34;,&#34;691&#34;,&#34;692&#34;,&#34;693&#34;,&#34;694&#34;,&#34;695&#34;,&#34;696&#34;,&#34;697&#34;,&#34;698&#34;,&#34;699&#34;,&#34;700&#34;,&#34;701&#34;,&#34;702&#34;,&#34;703&#34;,&#34;704&#34;,&#34;705&#34;,&#34;706&#34;,&#34;707&#34;,&#34;708&#34;,&#34;709&#34;,&#34;710&#34;,&#34;711&#34;,&#34;712&#34;,&#34;713&#34;,&#34;714&#34;,&#34;715&#34;,&#34;716&#34;,&#34;717&#34;,&#34;718&#34;,&#34;719&#34;,&#34;720&#34;,&#34;721&#34;,&#34;722&#34;,&#34;723&#34;,&#34;724&#34;,&#34;725&#34;,&#34;726&#34;,&#34;727&#34;,&#34;728&#34;,&#34;729&#34;,&#34;730&#34;,&#34;731&#34;,&#34;732&#34;,&#34;733&#34;,&#34;734&#34;,&#34;735&#34;,&#34;736&#34;,&#34;737&#34;,&#34;738&#34;,&#34;739&#34;,&#34;740&#34;,&#34;741&#34;,&#34;742&#34;,&#34;743&#34;,&#34;744&#34;,&#34;745&#34;,&#34;746&#34;,&#34;747&#34;,&#34;748&#34;,&#34;749&#34;,&#34;750&#34;,&#34;751&#34;,&#34;752&#34;,&#34;753&#34;,&#34;754&#34;,&#34;755&#34;,&#34;756&#34;,&#34;757&#34;,&#34;758&#34;,&#34;759&#34;,&#34;760&#34;,&#34;761&#34;,&#34;762&#34;,&#34;763&#34;,&#34;764&#34;,&#34;765&#34;,&#34;766&#34;,&#34;767&#34;,&#34;768&#34;,&#34;769&#34;,&#34;770&#34;,&#34;771&#34;,&#34;772&#34;,&#34;773&#34;,&#34;774&#34;,&#34;775&#34;,&#34;776&#34;,&#34;777&#34;,&#34;778&#34;,&#34;779&#34;,&#34;780&#34;,&#34;781&#34;,&#34;782&#34;,&#34;783&#34;,&#34;784&#34;,&#34;785&#34;,&#34;786&#34;,&#34;787&#34;,&#34;788&#34;,&#34;789&#34;,&#34;790&#34;,&#34;791&#34;,&#34;792&#34;,&#34;793&#34;,&#34;794&#34;,&#34;795&#34;,&#34;796&#34;,&#34;797&#34;,&#34;798&#34;,&#34;799&#34;,&#34;800&#34;,&#34;801&#34;,&#34;802&#34;,&#34;803&#34;,&#34;804&#34;,&#34;805&#34;,&#34;806&#34;,&#34;807&#34;,&#34;808&#34;,&#34;809&#34;,&#34;810&#34;,&#34;811&#34;,&#34;812&#34;,&#34;813&#34;,&#34;814&#34;,&#34;815&#34;,&#34;816&#34;,&#34;817&#34;,&#34;818&#34;,&#34;819&#34;,&#34;820&#34;,&#34;821&#34;,&#34;822&#34;,&#34;823&#34;,&#34;824&#34;,&#34;825&#34;,&#34;826&#34;,&#34;827&#34;,&#34;828&#34;,&#34;829&#34;,&#34;830&#34;,&#34;831&#34;,&#34;832&#34;,&#34;833&#34;,&#34;834&#34;,&#34;835&#34;,&#34;836&#34;,&#34;837&#34;,&#34;838&#34;,&#34;839&#34;,&#34;840&#34;,&#34;841&#34;,&#34;842&#34;,&#34;843&#34;,&#34;844&#34;,&#34;845&#34;,&#34;846&#34;,&#34;847&#34;,&#34;848&#34;,&#34;849&#34;,&#34;850&#34;,&#34;851&#34;,&#34;852&#34;,&#34;853&#34;,&#34;854&#34;,&#34;855&#34;,&#34;856&#34;,&#34;857&#34;,&#34;858&#34;,&#34;859&#34;,&#34;860&#34;,&#34;861&#34;,&#34;862&#34;,&#34;863&#34;,&#34;864&#34;,&#34;865&#34;,&#34;866&#34;,&#34;867&#34;,&#34;868&#34;,&#34;869&#34;,&#34;870&#34;,&#34;871&#34;,&#34;872&#34;,&#34;873&#34;,&#34;874&#34;,&#34;875&#34;,&#34;876&#34;,&#34;877&#34;,&#34;878&#34;,&#34;879&#34;,&#34;880&#34;,&#34;881&#34;,&#34;882&#34;,&#34;883&#34;,&#34;884&#34;,&#34;885&#34;,&#34;886&#34;,&#34;887&#34;,&#34;888&#34;,&#34;889&#34;,&#34;890&#34;,&#34;891&#34;],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891],[0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,1,0,1,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,1,1,0,0,1,0,0,0,0,1,1,0,1,1,0,1,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,1,1,1,0,1,0,0,0,1,1,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,0,1,0,0,0,0,0,1,1,1,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,1,0,1,1,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,0,0,0,1,1,0,1,0,0,0,1,0,1,1,1,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,1,0,0,0,0,0,0,0,0,1,1,0,1,1,1,1,0,0,1,0,1,0,0,1,0,0,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,1,1,0,1,1,0,0,1,0,1,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,0,1,1,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,1,1,1,0,0,1,0,0,1,0,0,1,0,0,1,1,0,0,0,0,1,0,0,1,0,1,0,0,1,0,0,0,0,0,1,0,1,1,1,0,1,0,1,0,1,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,1,1,0,0,0,0,1,1,1,1,1,0,1,0,0,0,1,1,0,0,1,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,0,1,0,0,1,1,0,0,1,1,0,0,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,1,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,0,0,1,0,1,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,1,0,1,0],[3,1,3,1,3,3,1,3,3,2,3,1,3,3,3,2,3,2,3,3,2,2,3,1,3,3,3,1,3,3,1,1,3,2,1,1,3,3,3,3,3,2,3,2,3,3,3,3,3,3,3,3,1,2,1,1,2,3,2,3,3,1,1,3,1,3,2,3,3,3,2,3,2,3,3,3,3,3,2,3,3,3,3,1,2,3,3,3,1,3,3,3,1,3,3,3,1,1,2,2,3,3,1,3,3,3,3,3,3,3,1,3,3,3,3,3,3,2,1,3,2,3,2,2,1,3,3,3,3,3,3,3,3,2,2,2,1,1,3,1,3,3,3,3,2,2,3,3,2,2,2,1,3,3,3,1,3,3,3,3,3,2,3,3,3,3,1,3,1,3,1,3,3,3,1,3,3,1,2,3,3,2,3,2,3,1,3,1,3,3,2,2,3,2,1,1,3,3,3,2,3,3,3,3,3,3,3,3,3,1,3,2,3,2,3,1,3,2,1,2,3,2,3,3,1,3,2,3,2,3,1,3,2,3,2,3,2,2,2,2,3,3,2,3,3,1,3,2,1,2,3,3,1,3,3,3,1,1,1,2,3,3,1,1,3,2,3,3,1,1,1,3,2,1,3,1,3,2,3,3,3,3,3,3,1,3,3,3,2,3,1,1,2,3,3,1,3,1,1,1,3,3,3,2,3,1,1,1,2,1,1,1,2,3,2,3,2,2,1,1,3,3,2,2,3,1,3,2,3,1,3,1,1,3,1,3,1,1,3,1,2,1,2,2,2,2,2,3,3,3,3,1,3,3,3,3,1,2,3,3,3,2,3,3,3,3,1,3,3,1,1,3,3,1,3,1,3,1,3,3,1,3,3,1,3,2,3,2,3,2,1,3,3,1,3,3,3,2,2,2,3,3,3,3,3,2,3,2,3,3,3,3,1,2,3,3,2,2,2,3,3,3,3,3,3,3,2,2,3,3,1,3,2,3,1,1,3,2,1,2,2,3,3,2,3,1,2,1,3,1,2,3,1,1,3,3,1,1,2,3,1,3,1,2,3,3,2,1,3,3,3,3,2,2,3,1,2,3,3,3,3,2,3,3,1,3,1,1,3,3,3,3,1,1,3,3,1,3,1,3,3,3,3,3,1,1,2,1,3,3,3,3,1,1,3,1,2,3,2,3,1,3,3,1,3,3,2,1,3,2,2,3,3,3,3,2,1,1,3,1,1,3,3,2,1,1,2,2,3,2,1,2,3,3,3,1,1,1,1,3,3,3,2,3,3,3,3,3,3,3,2,1,1,3,3,3,2,1,3,3,2,1,2,1,3,1,2,1,3,3,3,1,3,3,2,3,2,3,3,1,2,3,1,3,1,3,3,1,2,1,3,3,3,3,3,2,3,3,2,2,3,1,3,3,3,1,2,1,3,3,1,3,1,1,3,2,3,2,3,3,3,1,3,3,3,1,3,1,3,3,3,2,3,3,3,2,3,3,2,1,1,3,1,3,3,2,2,3,3,1,2,1,2,2,2,3,3,3,3,1,3,1,3,3,2,2,3,3,3,1,1,3,3,3,1,2,3,3,1,3,1,1,3,3,3,2,2,1,1,3,1,1,1,3,2,3,1,2,3,3,2,3,2,2,1,3,2,3,2,3,1,3,2,2,2,3,3,1,3,3,1,1,1,3,3,1,3,2,1,3,2,3,3,3,2,2,3,2,3,1,3,3,3,1,3,1,1,3,3,3,3,3,2,3,2,3,3,3,3,1,3,1,1,3,3,3,3,3,3,1,3,2,3,1,3,2,1,3,3,3,2,2,1,3,3,3,1,3,2,1,3,3,2,3,3,1,3,2,3,3,1,3,1,3,3,3,3,2,3,1,3,2,3,3,3,1,3,3,3,1,3,2,1,3,3,3,3,3,2,1,3,3,3,1,2,3,1,1,3,3,3,2,1,3,2,2,2,1,3,3,3,1,1,3,2,3,3,3,3,1,2,3,3,2,3,3,2,1,3,1,3],[&#34;Braund, Mr. Owen Harris&#34;,&#34;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&#34;,&#34;Heikkinen, Miss. Laina&#34;,&#34;Futrelle, Mrs. Jacques Heath (Lily May Peel)&#34;,&#34;Allen, Mr. William Henry&#34;,&#34;Moran, Mr. James&#34;,&#34;McCarthy, Mr. Timothy J&#34;,&#34;Palsson, Master. Gosta Leonard&#34;,&#34;Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)&#34;,&#34;Nasser, Mrs. Nicholas (Adele Achem)&#34;,&#34;Sandstrom, Miss. Marguerite Rut&#34;,&#34;Bonnell, Miss. Elizabeth&#34;,&#34;Saundercock, Mr. William Henry&#34;,&#34;Andersson, Mr. Anders Johan&#34;,&#34;Vestrom, Miss. Hulda Amanda Adolfina&#34;,&#34;Hewlett, Mrs. (Mary D Kingcome) &#34;,&#34;Rice, Master. Eugene&#34;,&#34;Williams, Mr. Charles Eugene&#34;,&#34;Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)&#34;,&#34;Masselmani, Mrs. Fatima&#34;,&#34;Fynney, Mr. Joseph J&#34;,&#34;Beesley, Mr. Lawrence&#34;,&#34;McGowan, Miss. Anna \&#34;Annie\&#34;&#34;,&#34;Sloper, Mr. William Thompson&#34;,&#34;Palsson, Miss. Torborg Danira&#34;,&#34;Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)&#34;,&#34;Emir, Mr. Farred Chehab&#34;,&#34;Fortune, Mr. Charles Alexander&#34;,&#34;O&#39;Dwyer, Miss. Ellen \&#34;Nellie\&#34;&#34;,&#34;Todoroff, Mr. Lalio&#34;,&#34;Uruchurtu, Don. Manuel E&#34;,&#34;Spencer, Mrs. William Augustus (Marie Eugenie)&#34;,&#34;Glynn, Miss. Mary Agatha&#34;,&#34;Wheadon, Mr. Edward H&#34;,&#34;Meyer, Mr. Edgar Joseph&#34;,&#34;Holverson, Mr. Alexander Oskar&#34;,&#34;Mamee, Mr. Hanna&#34;,&#34;Cann, Mr. Ernest Charles&#34;,&#34;Vander Planke, Miss. Augusta Maria&#34;,&#34;Nicola-Yarred, Miss. Jamila&#34;,&#34;Ahlin, Mrs. Johan (Johanna Persdotter Larsson)&#34;,&#34;Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)&#34;,&#34;Kraeff, Mr. Theodor&#34;,&#34;Laroche, Miss. Simonne Marie Anne Andree&#34;,&#34;Devaney, Miss. Margaret Delia&#34;,&#34;Rogers, Mr. William John&#34;,&#34;Lennon, Mr. Denis&#34;,&#34;O&#39;Driscoll, Miss. Bridget&#34;,&#34;Samaan, Mr. Youssef&#34;,&#34;Arnold-Franchi, Mrs. Josef (Josefine Franchi)&#34;,&#34;Panula, Master. Juha Niilo&#34;,&#34;Nosworthy, Mr. Richard Cater&#34;,&#34;Harper, Mrs. Henry Sleeper (Myna Haxtun)&#34;,&#34;Faunthorpe, Mrs. Lizzie (Elizabeth Anne Wilkinson)&#34;,&#34;Ostby, Mr. Engelhart Cornelius&#34;,&#34;Woolner, Mr. Hugh&#34;,&#34;Rugg, Miss. Emily&#34;,&#34;Novel, Mr. Mansouer&#34;,&#34;West, Miss. Constance Mirium&#34;,&#34;Goodwin, Master. William Frederick&#34;,&#34;Sirayanian, Mr. Orsen&#34;,&#34;Icard, Miss. Amelie&#34;,&#34;Harris, Mr. Henry Birkhardt&#34;,&#34;Skoog, Master. Harald&#34;,&#34;Stewart, Mr. Albert A&#34;,&#34;Moubarek, Master. Gerios&#34;,&#34;Nye, Mrs. (Elizabeth Ramell)&#34;,&#34;Crease, Mr. Ernest James&#34;,&#34;Andersson, Miss. Erna Alexandra&#34;,&#34;Kink, Mr. Vincenz&#34;,&#34;Jenkin, Mr. Stephen Curnow&#34;,&#34;Goodwin, Miss. Lillian Amy&#34;,&#34;Hood, Mr. Ambrose Jr&#34;,&#34;Chronopoulos, Mr. Apostolos&#34;,&#34;Bing, Mr. Lee&#34;,&#34;Moen, Mr. Sigurd Hansen&#34;,&#34;Staneff, Mr. Ivan&#34;,&#34;Moutal, Mr. Rahamin Haim&#34;,&#34;Caldwell, Master. Alden Gates&#34;,&#34;Dowdell, Miss. Elizabeth&#34;,&#34;Waelens, Mr. Achille&#34;,&#34;Sheerlinck, Mr. Jan Baptist&#34;,&#34;McDermott, Miss. Brigdet Delia&#34;,&#34;Carrau, Mr. Francisco M&#34;,&#34;Ilett, Miss. Bertha&#34;,&#34;Backstrom, Mrs. Karl Alfred (Maria Mathilda Gustafsson)&#34;,&#34;Ford, Mr. William Neal&#34;,&#34;Slocovski, Mr. Selman Francis&#34;,&#34;Fortune, Miss. Mabel Helen&#34;,&#34;Celotti, Mr. Francesco&#34;,&#34;Christmann, Mr. Emil&#34;,&#34;Andreasson, Mr. Paul Edvin&#34;,&#34;Chaffee, Mr. Herbert Fuller&#34;,&#34;Dean, Mr. Bertram Frank&#34;,&#34;Coxon, Mr. Daniel&#34;,&#34;Shorney, Mr. Charles Joseph&#34;,&#34;Goldschmidt, Mr. George B&#34;,&#34;Greenfield, Mr. William Bertram&#34;,&#34;Doling, Mrs. John T (Ada Julia Bone)&#34;,&#34;Kantor, Mr. Sinai&#34;,&#34;Petranec, Miss. Matilda&#34;,&#34;Petroff, Mr. Pastcho (\&#34;Pentcho\&#34;)&#34;,&#34;White, Mr. Richard Frasar&#34;,&#34;Johansson, Mr. Gustaf Joel&#34;,&#34;Gustafsson, Mr. Anders Vilhelm&#34;,&#34;Mionoff, Mr. Stoytcho&#34;,&#34;Salkjelsvik, Miss. Anna Kristine&#34;,&#34;Moss, Mr. Albert Johan&#34;,&#34;Rekic, Mr. Tido&#34;,&#34;Moran, Miss. Bertha&#34;,&#34;Porter, Mr. Walter Chamberlain&#34;,&#34;Zabour, Miss. Hileni&#34;,&#34;Barton, Mr. David John&#34;,&#34;Jussila, Miss. Katriina&#34;,&#34;Attalah, Miss. Malake&#34;,&#34;Pekoniemi, Mr. Edvard&#34;,&#34;Connors, Mr. Patrick&#34;,&#34;Turpin, Mr. William John Robert&#34;,&#34;Baxter, Mr. Quigg Edmond&#34;,&#34;Andersson, Miss. Ellis Anna Maria&#34;,&#34;Hickman, Mr. Stanley George&#34;,&#34;Moore, Mr. Leonard Charles&#34;,&#34;Nasser, Mr. Nicholas&#34;,&#34;Webber, Miss. Susan&#34;,&#34;White, Mr. Percival Wayland&#34;,&#34;Nicola-Yarred, Master. Elias&#34;,&#34;McMahon, Mr. Martin&#34;,&#34;Madsen, Mr. Fridtjof Arne&#34;,&#34;Peter, Miss. Anna&#34;,&#34;Ekstrom, Mr. Johan&#34;,&#34;Drazenoic, Mr. Jozef&#34;,&#34;Coelho, Mr. Domingos Fernandeo&#34;,&#34;Robins, Mrs. Alexander A (Grace Charity Laury)&#34;,&#34;Weisz, Mrs. Leopold (Mathilde Francoise Pede)&#34;,&#34;Sobey, Mr. Samuel James Hayden&#34;,&#34;Richard, Mr. Emile&#34;,&#34;Newsom, Miss. Helen Monypeny&#34;,&#34;Futrelle, Mr. Jacques Heath&#34;,&#34;Osen, Mr. Olaf Elon&#34;,&#34;Giglio, Mr. Victor&#34;,&#34;Boulos, Mrs. Joseph (Sultana)&#34;,&#34;Nysten, Miss. Anna Sofia&#34;,&#34;Hakkarainen, Mrs. Pekka Pietari (Elin Matilda Dolck)&#34;,&#34;Burke, Mr. Jeremiah&#34;,&#34;Andrew, Mr. Edgardo Samuel&#34;,&#34;Nicholls, Mr. Joseph Charles&#34;,&#34;Andersson, Mr. August Edvard (\&#34;Wennerstrom\&#34;)&#34;,&#34;Ford, Miss. Robina Maggie \&#34;Ruby\&#34;&#34;,&#34;Navratil, Mr. Michel (\&#34;Louis M Hoffman\&#34;)&#34;,&#34;Byles, Rev. Thomas Roussel Davids&#34;,&#34;Bateman, Rev. Robert James&#34;,&#34;Pears, Mrs. Thomas (Edith Wearne)&#34;,&#34;Meo, Mr. Alfonzo&#34;,&#34;van Billiard, Mr. Austin Blyler&#34;,&#34;Olsen, Mr. Ole Martin&#34;,&#34;Williams, Mr. Charles Duane&#34;,&#34;Gilnagh, Miss. Katherine \&#34;Katie\&#34;&#34;,&#34;Corn, Mr. Harry&#34;,&#34;Smiljanic, Mr. Mile&#34;,&#34;Sage, Master. Thomas Henry&#34;,&#34;Cribb, Mr. John Hatfield&#34;,&#34;Watt, Mrs. James (Elizabeth \&#34;Bessie\&#34; Inglis Milne)&#34;,&#34;Bengtsson, Mr. John Viktor&#34;,&#34;Calic, Mr. Jovo&#34;,&#34;Panula, Master. Eino Viljami&#34;,&#34;Goldsmith, Master. Frank John William \&#34;Frankie\&#34;&#34;,&#34;Chibnall, Mrs. (Edith Martha Bowerman)&#34;,&#34;Skoog, Mrs. William (Anna Bernhardina Karlsson)&#34;,&#34;Baumann, Mr. John D&#34;,&#34;Ling, Mr. Lee&#34;,&#34;Van der hoef, Mr. Wyckoff&#34;,&#34;Rice, Master. Arthur&#34;,&#34;Johnson, Miss. Eleanor Ileen&#34;,&#34;Sivola, Mr. Antti Wilhelm&#34;,&#34;Smith, Mr. James Clinch&#34;,&#34;Klasen, Mr. Klas Albin&#34;,&#34;Lefebre, Master. Henry Forbes&#34;,&#34;Isham, Miss. Ann Elizabeth&#34;,&#34;Hale, Mr. Reginald&#34;,&#34;Leonard, Mr. Lionel&#34;,&#34;Sage, Miss. Constance Gladys&#34;,&#34;Pernot, Mr. Rene&#34;,&#34;Asplund, Master. Clarence Gustaf Hugo&#34;,&#34;Becker, Master. Richard F&#34;,&#34;Kink-Heilmann, Miss. Luise Gretchen&#34;,&#34;Rood, Mr. Hugh Roscoe&#34;,&#34;O&#39;Brien, Mrs. Thomas (Johanna \&#34;Hannah\&#34; Godfrey)&#34;,&#34;Romaine, Mr. Charles Hallace (\&#34;Mr C Rolmane\&#34;)&#34;,&#34;Bourke, Mr. John&#34;,&#34;Turcin, Mr. Stjepan&#34;,&#34;Pinsky, Mrs. (Rosa)&#34;,&#34;Carbines, Mr. William&#34;,&#34;Andersen-Jensen, Miss. Carla Christine Nielsine&#34;,&#34;Navratil, Master. Michel M&#34;,&#34;Brown, Mrs. James Joseph (Margaret Tobin)&#34;,&#34;Lurette, Miss. Elise&#34;,&#34;Mernagh, Mr. Robert&#34;,&#34;Olsen, Mr. Karl Siegwart Andreas&#34;,&#34;Madigan, Miss. Margaret \&#34;Maggie\&#34;&#34;,&#34;Yrois, Miss. Henriette (\&#34;Mrs Harbeck\&#34;)&#34;,&#34;Vande Walle, Mr. Nestor Cyriel&#34;,&#34;Sage, Mr. Frederick&#34;,&#34;Johanson, Mr. Jakob Alfred&#34;,&#34;Youseff, Mr. Gerious&#34;,&#34;Cohen, Mr. Gurshon \&#34;Gus\&#34;&#34;,&#34;Strom, Miss. Telma Matilda&#34;,&#34;Backstrom, Mr. Karl Alfred&#34;,&#34;Albimona, Mr. Nassef Cassem&#34;,&#34;Carr, Miss. Helen \&#34;Ellen\&#34;&#34;,&#34;Blank, Mr. Henry&#34;,&#34;Ali, Mr. Ahmed&#34;,&#34;Cameron, Miss. Clear Annie&#34;,&#34;Perkin, Mr. John Henry&#34;,&#34;Givard, Mr. Hans Kristensen&#34;,&#34;Kiernan, Mr. Philip&#34;,&#34;Newell, Miss. Madeleine&#34;,&#34;Honkanen, Miss. Eliina&#34;,&#34;Jacobsohn, Mr. Sidney Samuel&#34;,&#34;Bazzani, Miss. Albina&#34;,&#34;Harris, Mr. Walter&#34;,&#34;Sunderland, Mr. Victor Francis&#34;,&#34;Bracken, Mr. James H&#34;,&#34;Green, Mr. George Henry&#34;,&#34;Nenkoff, Mr. Christo&#34;,&#34;Hoyt, Mr. Frederick Maxfield&#34;,&#34;Berglund, Mr. Karl Ivar Sven&#34;,&#34;Mellors, Mr. William John&#34;,&#34;Lovell, Mr. John Hall (\&#34;Henry\&#34;)&#34;,&#34;Fahlstrom, Mr. Arne Jonas&#34;,&#34;Lefebre, Miss. Mathilde&#34;,&#34;Harris, Mrs. Henry Birkhardt (Irene Wallach)&#34;,&#34;Larsson, Mr. Bengt Edvin&#34;,&#34;Sjostedt, Mr. Ernst Adolf&#34;,&#34;Asplund, Miss. Lillian Gertrud&#34;,&#34;Leyson, Mr. Robert William Norman&#34;,&#34;Harknett, Miss. Alice Phoebe&#34;,&#34;Hold, Mr. Stephen&#34;,&#34;Collyer, Miss. Marjorie \&#34;Lottie\&#34;&#34;,&#34;Pengelly, Mr. Frederick William&#34;,&#34;Hunt, Mr. George Henry&#34;,&#34;Zabour, Miss. Thamine&#34;,&#34;Murphy, Miss. Katherine \&#34;Kate\&#34;&#34;,&#34;Coleridge, Mr. Reginald Charles&#34;,&#34;Maenpaa, Mr. Matti Alexanteri&#34;,&#34;Attalah, Mr. Sleiman&#34;,&#34;Minahan, Dr. William Edward&#34;,&#34;Lindahl, Miss. Agda Thorilda Viktoria&#34;,&#34;Hamalainen, Mrs. William (Anna)&#34;,&#34;Beckwith, Mr. Richard Leonard&#34;,&#34;Carter, Rev. Ernest Courtenay&#34;,&#34;Reed, Mr. James George&#34;,&#34;Strom, Mrs. Wilhelm (Elna Matilda Persson)&#34;,&#34;Stead, Mr. William Thomas&#34;,&#34;Lobb, Mr. William Arthur&#34;,&#34;Rosblom, Mrs. Viktor (Helena Wilhelmina)&#34;,&#34;Touma, Mrs. Darwis (Hanne Youssef Razi)&#34;,&#34;Thorne, Mrs. Gertrude Maybelle&#34;,&#34;Cherry, Miss. Gladys&#34;,&#34;Ward, Miss. Anna&#34;,&#34;Parrish, Mrs. (Lutie Davis)&#34;,&#34;Smith, Mr. Thomas&#34;,&#34;Asplund, Master. Edvin Rojj Felix&#34;,&#34;Taussig, Mr. Emil&#34;,&#34;Harrison, Mr. William&#34;,&#34;Henry, Miss. Delia&#34;,&#34;Reeves, Mr. David&#34;,&#34;Panula, Mr. Ernesti Arvid&#34;,&#34;Persson, Mr. Ernst Ulrik&#34;,&#34;Graham, Mrs. William Thompson (Edith Junkins)&#34;,&#34;Bissette, Miss. Amelia&#34;,&#34;Cairns, Mr. Alexander&#34;,&#34;Tornquist, Mr. William Henry&#34;,&#34;Mellinger, Mrs. (Elizabeth Anne Maidment)&#34;,&#34;Natsch, Mr. Charles H&#34;,&#34;Healy, Miss. Hanora \&#34;Nora\&#34;&#34;,&#34;Andrews, Miss. Kornelia Theodosia&#34;,&#34;Lindblom, Miss. Augusta Charlotta&#34;,&#34;Parkes, Mr. Francis \&#34;Frank\&#34;&#34;,&#34;Rice, Master. Eric&#34;,&#34;Abbott, Mrs. Stanton (Rosa Hunt)&#34;,&#34;Duane, Mr. Frank&#34;,&#34;Olsson, Mr. Nils Johan Goransson&#34;,&#34;de Pelsmaeker, Mr. Alfons&#34;,&#34;Dorking, Mr. Edward Arthur&#34;,&#34;Smith, Mr. Richard William&#34;,&#34;Stankovic, Mr. Ivan&#34;,&#34;de Mulder, Mr. Theodore&#34;,&#34;Naidenoff, Mr. Penko&#34;,&#34;Hosono, Mr. Masabumi&#34;,&#34;Connolly, Miss. Kate&#34;,&#34;Barber, Miss. Ellen \&#34;Nellie\&#34;&#34;,&#34;Bishop, Mrs. Dickinson H (Helen Walton)&#34;,&#34;Levy, Mr. Rene Jacques&#34;,&#34;Haas, Miss. Aloisia&#34;,&#34;Mineff, Mr. Ivan&#34;,&#34;Lewy, Mr. Ervin G&#34;,&#34;Hanna, Mr. Mansour&#34;,&#34;Allison, Miss. Helen Loraine&#34;,&#34;Saalfeld, Mr. Adolphe&#34;,&#34;Baxter, Mrs. James (Helene DeLaudeniere Chaput)&#34;,&#34;Kelly, Miss. Anna Katherine \&#34;Annie Kate\&#34;&#34;,&#34;McCoy, Mr. Bernard&#34;,&#34;Johnson, Mr. William Cahoone Jr&#34;,&#34;Keane, Miss. Nora A&#34;,&#34;Williams, Mr. Howard Hugh \&#34;Harry\&#34;&#34;,&#34;Allison, Master. Hudson Trevor&#34;,&#34;Fleming, Miss. Margaret&#34;,&#34;Penasco y Castellana, Mrs. Victor de Satode (Maria Josefa Perez de Soto y Vallejo)&#34;,&#34;Abelson, Mr. Samuel&#34;,&#34;Francatelli, Miss. Laura Mabel&#34;,&#34;Hays, Miss. Margaret Bechstein&#34;,&#34;Ryerson, Miss. Emily Borie&#34;,&#34;Lahtinen, Mrs. William (Anna Sylfven)&#34;,&#34;Hendekovic, Mr. Ignjac&#34;,&#34;Hart, Mr. Benjamin&#34;,&#34;Nilsson, Miss. Helmina Josefina&#34;,&#34;Kantor, Mrs. Sinai (Miriam Sternin)&#34;,&#34;Moraweck, Dr. Ernest&#34;,&#34;Wick, Miss. Mary Natalie&#34;,&#34;Spedden, Mrs. Frederic Oakley (Margaretta Corning Stone)&#34;,&#34;Dennis, Mr. Samuel&#34;,&#34;Danoff, Mr. Yoto&#34;,&#34;Slayter, Miss. Hilda Mary&#34;,&#34;Caldwell, Mrs. Albert Francis (Sylvia Mae Harbaugh)&#34;,&#34;Sage, Mr. George John Jr&#34;,&#34;Young, Miss. Marie Grice&#34;,&#34;Nysveen, Mr. Johan Hansen&#34;,&#34;Ball, Mrs. (Ada E Hall)&#34;,&#34;Goldsmith, Mrs. Frank John (Emily Alice Brown)&#34;,&#34;Hippach, Miss. Jean Gertrude&#34;,&#34;McCoy, Miss. Agnes&#34;,&#34;Partner, Mr. Austen&#34;,&#34;Graham, Mr. George Edward&#34;,&#34;Vander Planke, Mr. Leo Edmondus&#34;,&#34;Frauenthal, Mrs. Henry William (Clara Heinsheimer)&#34;,&#34;Denkoff, Mr. Mitto&#34;,&#34;Pears, Mr. Thomas Clinton&#34;,&#34;Burns, Miss. Elizabeth Margaret&#34;,&#34;Dahl, Mr. Karl Edwart&#34;,&#34;Blackwell, Mr. Stephen Weart&#34;,&#34;Navratil, Master. Edmond Roger&#34;,&#34;Fortune, Miss. Alice Elizabeth&#34;,&#34;Collander, Mr. Erik Gustaf&#34;,&#34;Sedgwick, Mr. Charles Frederick Waddington&#34;,&#34;Fox, Mr. Stanley Hubert&#34;,&#34;Brown, Miss. Amelia \&#34;Mildred\&#34;&#34;,&#34;Smith, Miss. Marion Elsie&#34;,&#34;Davison, Mrs. Thomas Henry (Mary E Finck)&#34;,&#34;Coutts, Master. William Loch \&#34;William\&#34;&#34;,&#34;Dimic, Mr. Jovan&#34;,&#34;Odahl, Mr. Nils Martin&#34;,&#34;Williams-Lambert, Mr. Fletcher Fellows&#34;,&#34;Elias, Mr. Tannous&#34;,&#34;Arnold-Franchi, Mr. Josef&#34;,&#34;Yousif, Mr. Wazli&#34;,&#34;Vanden Steen, Mr. Leo Peter&#34;,&#34;Bowerman, Miss. Elsie Edith&#34;,&#34;Funk, Miss. Annie Clemmer&#34;,&#34;McGovern, Miss. Mary&#34;,&#34;Mockler, Miss. Helen Mary \&#34;Ellie\&#34;&#34;,&#34;Skoog, Mr. Wilhelm&#34;,&#34;del Carlo, Mr. Sebastiano&#34;,&#34;Barbara, Mrs. (Catherine David)&#34;,&#34;Asim, Mr. Adola&#34;,&#34;O&#39;Brien, Mr. Thomas&#34;,&#34;Adahl, Mr. Mauritz Nils Martin&#34;,&#34;Warren, Mrs. Frank Manley (Anna Sophia Atkinson)&#34;,&#34;Moussa, Mrs. (Mantoura Boulos)&#34;,&#34;Jermyn, Miss. Annie&#34;,&#34;Aubart, Mme. Leontine Pauline&#34;,&#34;Harder, Mr. George Achilles&#34;,&#34;Wiklund, Mr. Jakob Alfred&#34;,&#34;Beavan, Mr. William Thomas&#34;,&#34;Ringhini, Mr. Sante&#34;,&#34;Palsson, Miss. Stina Viola&#34;,&#34;Meyer, Mrs. Edgar Joseph (Leila Saks)&#34;,&#34;Landergren, Miss. Aurora Adelia&#34;,&#34;Widener, Mr. Harry Elkins&#34;,&#34;Betros, Mr. Tannous&#34;,&#34;Gustafsson, Mr. Karl Gideon&#34;,&#34;Bidois, Miss. Rosalie&#34;,&#34;Nakid, Miss. Maria (\&#34;Mary\&#34;)&#34;,&#34;Tikkanen, Mr. Juho&#34;,&#34;Holverson, Mrs. Alexander Oskar (Mary Aline Towner)&#34;,&#34;Plotcharsky, Mr. Vasil&#34;,&#34;Davies, Mr. Charles Henry&#34;,&#34;Goodwin, Master. Sidney Leonard&#34;,&#34;Buss, Miss. Kate&#34;,&#34;Sadlier, Mr. Matthew&#34;,&#34;Lehmann, Miss. Bertha&#34;,&#34;Carter, Mr. William Ernest&#34;,&#34;Jansson, Mr. Carl Olof&#34;,&#34;Gustafsson, Mr. Johan Birger&#34;,&#34;Newell, Miss. Marjorie&#34;,&#34;Sandstrom, Mrs. Hjalmar (Agnes Charlotta Bengtsson)&#34;,&#34;Johansson, Mr. Erik&#34;,&#34;Olsson, Miss. Elina&#34;,&#34;McKane, Mr. Peter David&#34;,&#34;Pain, Dr. Alfred&#34;,&#34;Trout, Mrs. William H (Jessie L)&#34;,&#34;Niskanen, Mr. Juha&#34;,&#34;Adams, Mr. John&#34;,&#34;Jussila, Miss. Mari Aina&#34;,&#34;Hakkarainen, Mr. Pekka Pietari&#34;,&#34;Oreskovic, Miss. Marija&#34;,&#34;Gale, Mr. Shadrach&#34;,&#34;Widegren, Mr. Carl/Charles Peter&#34;,&#34;Richards, Master. William Rowe&#34;,&#34;Birkeland, Mr. Hans Martin Monsen&#34;,&#34;Lefebre, Miss. Ida&#34;,&#34;Sdycoff, Mr. Todor&#34;,&#34;Hart, Mr. Henry&#34;,&#34;Minahan, Miss. Daisy E&#34;,&#34;Cunningham, Mr. Alfred Fleming&#34;,&#34;Sundman, Mr. Johan Julian&#34;,&#34;Meek, Mrs. Thomas (Annie Louise Rowley)&#34;,&#34;Drew, Mrs. James Vivian (Lulu Thorne Christian)&#34;,&#34;Silven, Miss. Lyyli Karoliina&#34;,&#34;Matthews, Mr. William John&#34;,&#34;Van Impe, Miss. Catharina&#34;,&#34;Gheorgheff, Mr. Stanio&#34;,&#34;Charters, Mr. David&#34;,&#34;Zimmerman, Mr. Leo&#34;,&#34;Danbom, Mrs. Ernst Gilbert (Anna Sigrid Maria Brogren)&#34;,&#34;Rosblom, Mr. Viktor Richard&#34;,&#34;Wiseman, Mr. Phillippe&#34;,&#34;Clarke, Mrs. Charles V (Ada Maria Winfield)&#34;,&#34;Phillips, Miss. Kate Florence (\&#34;Mrs Kate Louise Phillips Marshall\&#34;)&#34;,&#34;Flynn, Mr. James&#34;,&#34;Pickard, Mr. Berk (Berk Trembisky)&#34;,&#34;Bjornstrom-Steffansson, Mr. Mauritz Hakan&#34;,&#34;Thorneycroft, Mrs. Percival (Florence Kate White)&#34;,&#34;Louch, Mrs. Charles Alexander (Alice Adelaide Slow)&#34;,&#34;Kallio, Mr. Nikolai Erland&#34;,&#34;Silvey, Mr. William Baird&#34;,&#34;Carter, Miss. Lucile Polk&#34;,&#34;Ford, Miss. Doolina Margaret \&#34;Daisy\&#34;&#34;,&#34;Richards, Mrs. Sidney (Emily Hocking)&#34;,&#34;Fortune, Mr. Mark&#34;,&#34;Kvillner, Mr. Johan Henrik Johannesson&#34;,&#34;Hart, Mrs. Benjamin (Esther Ada Bloomfield)&#34;,&#34;Hampe, Mr. Leon&#34;,&#34;Petterson, Mr. Johan Emil&#34;,&#34;Reynaldo, Ms. Encarnacion&#34;,&#34;Johannesen-Bratthammer, Mr. Bernt&#34;,&#34;Dodge, Master. Washington&#34;,&#34;Mellinger, Miss. Madeleine Violet&#34;,&#34;Seward, Mr. Frederic Kimber&#34;,&#34;Baclini, Miss. Marie Catherine&#34;,&#34;Peuchen, Major. Arthur Godfrey&#34;,&#34;West, Mr. Edwy Arthur&#34;,&#34;Hagland, Mr. Ingvald Olai Olsen&#34;,&#34;Foreman, Mr. Benjamin Laventall&#34;,&#34;Goldenberg, Mr. Samuel L&#34;,&#34;Peduzzi, Mr. Joseph&#34;,&#34;Jalsevac, Mr. Ivan&#34;,&#34;Millet, Mr. Francis Davis&#34;,&#34;Kenyon, Mrs. Frederick R (Marion)&#34;,&#34;Toomey, Miss. Ellen&#34;,&#34;O&#39;Connor, Mr. Maurice&#34;,&#34;Anderson, Mr. Harry&#34;,&#34;Morley, Mr. William&#34;,&#34;Gee, Mr. Arthur H&#34;,&#34;Milling, Mr. Jacob Christian&#34;,&#34;Maisner, Mr. Simon&#34;,&#34;Goncalves, Mr. Manuel Estanslas&#34;,&#34;Campbell, Mr. William&#34;,&#34;Smart, Mr. John Montgomery&#34;,&#34;Scanlan, Mr. James&#34;,&#34;Baclini, Miss. Helene Barbara&#34;,&#34;Keefe, Mr. Arthur&#34;,&#34;Cacic, Mr. Luka&#34;,&#34;West, Mrs. Edwy Arthur (Ada Mary Worth)&#34;,&#34;Jerwan, Mrs. Amin S (Marie Marthe Thuillard)&#34;,&#34;Strandberg, Miss. Ida Sofia&#34;,&#34;Clifford, Mr. George Quincy&#34;,&#34;Renouf, Mr. Peter Henry&#34;,&#34;Braund, Mr. Lewis Richard&#34;,&#34;Karlsson, Mr. Nils August&#34;,&#34;Hirvonen, Miss. Hildur E&#34;,&#34;Goodwin, Master. Harold Victor&#34;,&#34;Frost, Mr. Anthony Wood \&#34;Archie\&#34;&#34;,&#34;Rouse, Mr. Richard Henry&#34;,&#34;Turkula, Mrs. (Hedwig)&#34;,&#34;Bishop, Mr. Dickinson H&#34;,&#34;Lefebre, Miss. Jeannie&#34;,&#34;Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)&#34;,&#34;Kent, Mr. Edward Austin&#34;,&#34;Somerton, Mr. Francis William&#34;,&#34;Coutts, Master. Eden Leslie \&#34;Neville\&#34;&#34;,&#34;Hagland, Mr. Konrad Mathias Reiersen&#34;,&#34;Windelov, Mr. Einar&#34;,&#34;Molson, Mr. Harry Markland&#34;,&#34;Artagaveytia, Mr. Ramon&#34;,&#34;Stanley, Mr. Edward Roland&#34;,&#34;Yousseff, Mr. Gerious&#34;,&#34;Eustis, Miss. Elizabeth Mussey&#34;,&#34;Shellard, Mr. Frederick William&#34;,&#34;Allison, Mrs. Hudson J C (Bessie Waldo Daniels)&#34;,&#34;Svensson, Mr. Olof&#34;,&#34;Calic, Mr. Petar&#34;,&#34;Canavan, Miss. Mary&#34;,&#34;O&#39;Sullivan, Miss. Bridget Mary&#34;,&#34;Laitinen, Miss. Kristina Sofia&#34;,&#34;Maioni, Miss. Roberta&#34;,&#34;Penasco y Castellana, Mr. Victor de Satode&#34;,&#34;Quick, Mrs. Frederick Charles (Jane Richards)&#34;,&#34;Bradley, Mr. George (\&#34;George Arthur Brayton\&#34;)&#34;,&#34;Olsen, Mr. Henry Margido&#34;,&#34;Lang, Mr. Fang&#34;,&#34;Daly, Mr. Eugene Patrick&#34;,&#34;Webber, Mr. James&#34;,&#34;McGough, Mr. James Robert&#34;,&#34;Rothschild, Mrs. Martin (Elizabeth L. Barrett)&#34;,&#34;Coleff, Mr. Satio&#34;,&#34;Walker, Mr. William Anderson&#34;,&#34;Lemore, Mrs. (Amelia Milley)&#34;,&#34;Ryan, Mr. Patrick&#34;,&#34;Angle, Mrs. William A (Florence \&#34;Mary\&#34; Agnes Hughes)&#34;,&#34;Pavlovic, Mr. Stefo&#34;,&#34;Perreault, Miss. Anne&#34;,&#34;Vovk, Mr. Janko&#34;,&#34;Lahoud, Mr. Sarkis&#34;,&#34;Hippach, Mrs. Louis Albert (Ida Sophia Fischer)&#34;,&#34;Kassem, Mr. Fared&#34;,&#34;Farrell, Mr. James&#34;,&#34;Ridsdale, Miss. Lucy&#34;,&#34;Farthing, Mr. John&#34;,&#34;Salonen, Mr. Johan Werner&#34;,&#34;Hocking, Mr. Richard George&#34;,&#34;Quick, Miss. Phyllis May&#34;,&#34;Toufik, Mr. Nakli&#34;,&#34;Elias, Mr. Joseph Jr&#34;,&#34;Peter, Mrs. Catherine (Catherine Rizk)&#34;,&#34;Cacic, Miss. Marija&#34;,&#34;Hart, Miss. Eva Miriam&#34;,&#34;Butt, Major. Archibald Willingham&#34;,&#34;LeRoy, Miss. Bertha&#34;,&#34;Risien, Mr. Samuel Beard&#34;,&#34;Frolicher, Miss. Hedwig Margaritha&#34;,&#34;Crosby, Miss. Harriet R&#34;,&#34;Andersson, Miss. Ingeborg Constanzia&#34;,&#34;Andersson, Miss. Sigrid Elisabeth&#34;,&#34;Beane, Mr. Edward&#34;,&#34;Douglas, Mr. Walter Donald&#34;,&#34;Nicholson, Mr. Arthur Ernest&#34;,&#34;Beane, Mrs. Edward (Ethel Clarke)&#34;,&#34;Padro y Manent, Mr. Julian&#34;,&#34;Goldsmith, Mr. Frank John&#34;,&#34;Davies, Master. John Morgan Jr&#34;,&#34;Thayer, Mr. John Borland Jr&#34;,&#34;Sharp, Mr. Percival James R&#34;,&#34;O&#39;Brien, Mr. Timothy&#34;,&#34;Leeni, Mr. Fahim (\&#34;Philip Zenni\&#34;)&#34;,&#34;Ohman, Miss. Velin&#34;,&#34;Wright, Mr. George&#34;,&#34;Duff Gordon, Lady. (Lucille Christiana Sutherland) (\&#34;Mrs Morgan\&#34;)&#34;,&#34;Robbins, Mr. Victor&#34;,&#34;Taussig, Mrs. Emil (Tillie Mandelbaum)&#34;,&#34;de Messemaeker, Mrs. Guillaume Joseph (Emma)&#34;,&#34;Morrow, Mr. Thomas Rowan&#34;,&#34;Sivic, Mr. Husein&#34;,&#34;Norman, Mr. Robert Douglas&#34;,&#34;Simmons, Mr. John&#34;,&#34;Meanwell, Miss. (Marion Ogden)&#34;,&#34;Davies, Mr. Alfred J&#34;,&#34;Stoytcheff, Mr. Ilia&#34;,&#34;Palsson, Mrs. Nils (Alma Cornelia Berglund)&#34;,&#34;Doharr, Mr. Tannous&#34;,&#34;Jonsson, Mr. Carl&#34;,&#34;Harris, Mr. George&#34;,&#34;Appleton, Mrs. Edward Dale (Charlotte Lamson)&#34;,&#34;Flynn, Mr. John Irwin (\&#34;Irving\&#34;)&#34;,&#34;Kelly, Miss. Mary&#34;,&#34;Rush, Mr. Alfred George John&#34;,&#34;Patchett, Mr. George&#34;,&#34;Garside, Miss. Ethel&#34;,&#34;Silvey, Mrs. William Baird (Alice Munger)&#34;,&#34;Caram, Mrs. Joseph (Maria Elias)&#34;,&#34;Jussila, Mr. Eiriik&#34;,&#34;Christy, Miss. Julie Rachel&#34;,&#34;Thayer, Mrs. John Borland (Marian Longstreth Morris)&#34;,&#34;Downton, Mr. William James&#34;,&#34;Ross, Mr. John Hugo&#34;,&#34;Paulner, Mr. Uscher&#34;,&#34;Taussig, Miss. Ruth&#34;,&#34;Jarvis, Mr. John Denzil&#34;,&#34;Frolicher-Stehli, Mr. Maxmillian&#34;,&#34;Gilinski, Mr. Eliezer&#34;,&#34;Murdlin, Mr. Joseph&#34;,&#34;Rintamaki, Mr. Matti&#34;,&#34;Stephenson, Mrs. Walter Bertram (Martha Eustis)&#34;,&#34;Elsbury, Mr. William James&#34;,&#34;Bourke, Miss. Mary&#34;,&#34;Chapman, Mr. John Henry&#34;,&#34;Van Impe, Mr. Jean Baptiste&#34;,&#34;Leitch, Miss. Jessie Wills&#34;,&#34;Johnson, Mr. Alfred&#34;,&#34;Boulos, Mr. Hanna&#34;,&#34;Duff Gordon, Sir. Cosmo Edmund (\&#34;Mr Morgan\&#34;)&#34;,&#34;Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)&#34;,&#34;Slabenoff, Mr. Petco&#34;,&#34;Harrington, Mr. Charles H&#34;,&#34;Torber, Mr. Ernst William&#34;,&#34;Homer, Mr. Harry (\&#34;Mr E Haven\&#34;)&#34;,&#34;Lindell, Mr. Edvard Bengtsson&#34;,&#34;Karaic, Mr. Milan&#34;,&#34;Daniel, Mr. Robert Williams&#34;,&#34;Laroche, Mrs. Joseph (Juliette Marie Louise Lafargue)&#34;,&#34;Shutes, Miss. Elizabeth W&#34;,&#34;Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)&#34;,&#34;Jardin, Mr. Jose Neto&#34;,&#34;Murphy, Miss. Margaret Jane&#34;,&#34;Horgan, Mr. John&#34;,&#34;Brocklebank, Mr. William Alfred&#34;,&#34;Herman, Miss. Alice&#34;,&#34;Danbom, Mr. Ernst Gilbert&#34;,&#34;Lobb, Mrs. William Arthur (Cordelia K Stanlick)&#34;,&#34;Becker, Miss. Marion Louise&#34;,&#34;Gavey, Mr. Lawrence&#34;,&#34;Yasbeck, Mr. Antoni&#34;,&#34;Kimball, Mr. Edwin Nelson Jr&#34;,&#34;Nakid, Mr. Sahid&#34;,&#34;Hansen, Mr. Henry Damsgaard&#34;,&#34;Bowen, Mr. David John \&#34;Dai\&#34;&#34;,&#34;Sutton, Mr. Frederick&#34;,&#34;Kirkland, Rev. Charles Leonard&#34;,&#34;Longley, Miss. Gretchen Fiske&#34;,&#34;Bostandyeff, Mr. Guentcho&#34;,&#34;O&#39;Connell, Mr. Patrick D&#34;,&#34;Barkworth, Mr. Algernon Henry Wilson&#34;,&#34;Lundahl, Mr. Johan Svensson&#34;,&#34;Stahelin-Maeglin, Dr. Max&#34;,&#34;Parr, Mr. William Henry Marsh&#34;,&#34;Skoog, Miss. Mabel&#34;,&#34;Davis, Miss. Mary&#34;,&#34;Leinonen, Mr. Antti Gustaf&#34;,&#34;Collyer, Mr. Harvey&#34;,&#34;Panula, Mrs. Juha (Maria Emilia Ojala)&#34;,&#34;Thorneycroft, Mr. Percival&#34;,&#34;Jensen, Mr. Hans Peder&#34;,&#34;Sagesser, Mlle. Emma&#34;,&#34;Skoog, Miss. Margit Elizabeth&#34;,&#34;Foo, Mr. Choong&#34;,&#34;Baclini, Miss. Eugenie&#34;,&#34;Harper, Mr. Henry Sleeper&#34;,&#34;Cor, Mr. Liudevit&#34;,&#34;Simonius-Blumer, Col. Oberst Alfons&#34;,&#34;Willey, Mr. Edward&#34;,&#34;Stanley, Miss. Amy Zillah Elsie&#34;,&#34;Mitkoff, Mr. Mito&#34;,&#34;Doling, Miss. Elsie&#34;,&#34;Kalvik, Mr. Johannes Halvorsen&#34;,&#34;O&#39;Leary, Miss. Hanora \&#34;Norah\&#34;&#34;,&#34;Hegarty, Miss. Hanora \&#34;Nora\&#34;&#34;,&#34;Hickman, Mr. Leonard Mark&#34;,&#34;Radeff, Mr. Alexander&#34;,&#34;Bourke, Mrs. John (Catherine)&#34;,&#34;Eitemiller, Mr. George Floyd&#34;,&#34;Newell, Mr. Arthur Webster&#34;,&#34;Frauenthal, Dr. Henry William&#34;,&#34;Badt, Mr. Mohamed&#34;,&#34;Colley, Mr. Edward Pomeroy&#34;,&#34;Coleff, Mr. Peju&#34;,&#34;Lindqvist, Mr. Eino William&#34;,&#34;Hickman, Mr. Lewis&#34;,&#34;Butler, Mr. Reginald Fenton&#34;,&#34;Rommetvedt, Mr. Knud Paust&#34;,&#34;Cook, Mr. Jacob&#34;,&#34;Taylor, Mrs. Elmer Zebley (Juliet Cummins Wright)&#34;,&#34;Brown, Mrs. Thomas William Solomon (Elizabeth Catherine Ford)&#34;,&#34;Davidson, Mr. Thornton&#34;,&#34;Mitchell, Mr. Henry Michael&#34;,&#34;Wilhelms, Mr. Charles&#34;,&#34;Watson, Mr. Ennis Hastings&#34;,&#34;Edvardsson, Mr. Gustaf Hjalmar&#34;,&#34;Sawyer, Mr. Frederick Charles&#34;,&#34;Turja, Miss. Anna Sofia&#34;,&#34;Goodwin, Mrs. Frederick (Augusta Tyler)&#34;,&#34;Cardeza, Mr. Thomas Drake Martinez&#34;,&#34;Peters, Miss. Katie&#34;,&#34;Hassab, Mr. Hammad&#34;,&#34;Olsvigen, Mr. Thor Anderson&#34;,&#34;Goodwin, Mr. Charles Edward&#34;,&#34;Brown, Mr. Thomas William Solomon&#34;,&#34;Laroche, Mr. Joseph Philippe Lemercier&#34;,&#34;Panula, Mr. Jaako Arnold&#34;,&#34;Dakic, Mr. Branko&#34;,&#34;Fischer, Mr. Eberhard Thelander&#34;,&#34;Madill, Miss. Georgette Alexandra&#34;,&#34;Dick, Mr. Albert Adrian&#34;,&#34;Karun, Miss. Manca&#34;,&#34;Lam, Mr. Ali&#34;,&#34;Saad, Mr. Khalil&#34;,&#34;Weir, Col. John&#34;,&#34;Chapman, Mr. Charles Henry&#34;,&#34;Kelly, Mr. James&#34;,&#34;Mullens, Miss. Katherine \&#34;Katie\&#34;&#34;,&#34;Thayer, Mr. John Borland&#34;,&#34;Humblen, Mr. Adolf Mathias Nicolai Olsen&#34;,&#34;Astor, Mrs. John Jacob (Madeleine Talmadge Force)&#34;,&#34;Silverthorne, Mr. Spencer Victor&#34;,&#34;Barbara, Miss. Saiide&#34;,&#34;Gallagher, Mr. Martin&#34;,&#34;Hansen, Mr. Henrik Juul&#34;,&#34;Morley, Mr. Henry Samuel (\&#34;Mr Henry Marshall\&#34;)&#34;,&#34;Kelly, Mrs. Florence \&#34;Fannie\&#34;&#34;,&#34;Calderhead, Mr. Edward Pennington&#34;,&#34;Cleaver, Miss. Alice&#34;,&#34;Moubarek, Master. Halim Gonios (\&#34;William George\&#34;)&#34;,&#34;Mayne, Mlle. Berthe Antonine (\&#34;Mrs de Villiers\&#34;)&#34;,&#34;Klaber, Mr. Herman&#34;,&#34;Taylor, Mr. Elmer Zebley&#34;,&#34;Larsson, Mr. August Viktor&#34;,&#34;Greenberg, Mr. Samuel&#34;,&#34;Soholt, Mr. Peter Andreas Lauritz Andersen&#34;,&#34;Endres, Miss. Caroline Louise&#34;,&#34;Troutt, Miss. Edwina Celia \&#34;Winnie\&#34;&#34;,&#34;McEvoy, Mr. Michael&#34;,&#34;Johnson, Mr. Malkolm Joackim&#34;,&#34;Harper, Miss. Annie Jessie \&#34;Nina\&#34;&#34;,&#34;Jensen, Mr. Svend Lauritz&#34;,&#34;Gillespie, Mr. William Henry&#34;,&#34;Hodges, Mr. Henry Price&#34;,&#34;Chambers, Mr. Norman Campbell&#34;,&#34;Oreskovic, Mr. Luka&#34;,&#34;Renouf, Mrs. Peter Henry (Lillian Jefferys)&#34;,&#34;Mannion, Miss. Margareth&#34;,&#34;Bryhl, Mr. Kurt Arnold Gottfrid&#34;,&#34;Ilmakangas, Miss. Pieta Sofia&#34;,&#34;Allen, Miss. Elisabeth Walton&#34;,&#34;Hassan, Mr. Houssein G N&#34;,&#34;Knight, Mr. Robert J&#34;,&#34;Berriman, Mr. William John&#34;,&#34;Troupiansky, Mr. Moses Aaron&#34;,&#34;Williams, Mr. Leslie&#34;,&#34;Ford, Mrs. Edward (Margaret Ann Watson)&#34;,&#34;Lesurer, Mr. Gustave J&#34;,&#34;Ivanoff, Mr. Kanio&#34;,&#34;Nankoff, Mr. Minko&#34;,&#34;Hawksford, Mr. Walter James&#34;,&#34;Cavendish, Mr. Tyrell William&#34;,&#34;Ryerson, Miss. Susan Parker \&#34;Suzette\&#34;&#34;,&#34;McNamee, Mr. Neal&#34;,&#34;Stranden, Mr. Juho&#34;,&#34;Crosby, Capt. Edward Gifford&#34;,&#34;Abbott, Mr. Rossmore Edward&#34;,&#34;Sinkkonen, Miss. Anna&#34;,&#34;Marvin, Mr. Daniel Warner&#34;,&#34;Connaghton, Mr. Michael&#34;,&#34;Wells, Miss. Joan&#34;,&#34;Moor, Master. Meier&#34;,&#34;Vande Velde, Mr. Johannes Joseph&#34;,&#34;Jonkoff, Mr. Lalio&#34;,&#34;Herman, Mrs. Samuel (Jane Laver)&#34;,&#34;Hamalainen, Master. Viljo&#34;,&#34;Carlsson, Mr. August Sigfrid&#34;,&#34;Bailey, Mr. Percy Andrew&#34;,&#34;Theobald, Mr. Thomas Leonard&#34;,&#34;Rothes, the Countess. of (Lucy Noel Martha Dyer-Edwards)&#34;,&#34;Garfirth, Mr. John&#34;,&#34;Nirva, Mr. Iisakki Antino Aijo&#34;,&#34;Barah, Mr. Hanna Assi&#34;,&#34;Carter, Mrs. William Ernest (Lucile Polk)&#34;,&#34;Eklund, Mr. Hans Linus&#34;,&#34;Hogeboom, Mrs. John C (Anna Andrews)&#34;,&#34;Brewe, Dr. Arthur Jackson&#34;,&#34;Mangan, Miss. Mary&#34;,&#34;Moran, Mr. Daniel J&#34;,&#34;Gronnestad, Mr. Daniel Danielsen&#34;,&#34;Lievens, Mr. Rene Aime&#34;,&#34;Jensen, Mr. Niels Peder&#34;,&#34;Mack, Mrs. (Mary)&#34;,&#34;Elias, Mr. Dibo&#34;,&#34;Hocking, Mrs. Elizabeth (Eliza Needs)&#34;,&#34;Myhrman, Mr. Pehr Fabian Oliver Malkolm&#34;,&#34;Tobin, Mr. Roger&#34;,&#34;Emanuel, Miss. Virginia Ethel&#34;,&#34;Kilgannon, Mr. Thomas J&#34;,&#34;Robert, Mrs. Edward Scott (Elisabeth Walton McMillan)&#34;,&#34;Ayoub, Miss. Banoura&#34;,&#34;Dick, Mrs. Albert Adrian (Vera Gillespie)&#34;,&#34;Long, Mr. Milton Clyde&#34;,&#34;Johnston, Mr. Andrew G&#34;,&#34;Ali, Mr. William&#34;,&#34;Harmer, Mr. Abraham (David Lishin)&#34;,&#34;Sjoblom, Miss. Anna Sofia&#34;,&#34;Rice, Master. George Hugh&#34;,&#34;Dean, Master. Bertram Vere&#34;,&#34;Guggenheim, Mr. Benjamin&#34;,&#34;Keane, Mr. Andrew \&#34;Andy\&#34;&#34;,&#34;Gaskell, Mr. Alfred&#34;,&#34;Sage, Miss. Stella Anna&#34;,&#34;Hoyt, Mr. William Fisher&#34;,&#34;Dantcheff, Mr. Ristiu&#34;,&#34;Otter, Mr. Richard&#34;,&#34;Leader, Dr. Alice (Farnham)&#34;,&#34;Osman, Mrs. Mara&#34;,&#34;Ibrahim Shawah, Mr. Yousseff&#34;,&#34;Van Impe, Mrs. Jean Baptiste (Rosalie Paula Govaert)&#34;,&#34;Ponesell, Mr. Martin&#34;,&#34;Collyer, Mrs. Harvey (Charlotte Annie Tate)&#34;,&#34;Carter, Master. William Thornton II&#34;,&#34;Thomas, Master. Assad Alexander&#34;,&#34;Hedman, Mr. Oskar Arvid&#34;,&#34;Johansson, Mr. Karl Johan&#34;,&#34;Andrews, Mr. Thomas Jr&#34;,&#34;Pettersson, Miss. Ellen Natalia&#34;,&#34;Meyer, Mr. August&#34;,&#34;Chambers, Mrs. Norman Campbell (Bertha Griggs)&#34;,&#34;Alexander, Mr. William&#34;,&#34;Lester, Mr. James&#34;,&#34;Slemen, Mr. Richard James&#34;,&#34;Andersson, Miss. Ebba Iris Alfrida&#34;,&#34;Tomlin, Mr. Ernest Portage&#34;,&#34;Fry, Mr. Richard&#34;,&#34;Heininen, Miss. Wendla Maria&#34;,&#34;Mallet, Mr. Albert&#34;,&#34;Holm, Mr. John Fredrik Alexander&#34;,&#34;Skoog, Master. Karl Thorsten&#34;,&#34;Hays, Mrs. Charles Melville (Clara Jennings Gregg)&#34;,&#34;Lulic, Mr. Nikola&#34;,&#34;Reuchlin, Jonkheer. John George&#34;,&#34;Moor, Mrs. (Beila)&#34;,&#34;Panula, Master. Urho Abraham&#34;,&#34;Flynn, Mr. John&#34;,&#34;Lam, Mr. Len&#34;,&#34;Mallet, Master. Andre&#34;,&#34;McCormack, Mr. Thomas Joseph&#34;,&#34;Stone, Mrs. George Nelson (Martha Evelyn)&#34;,&#34;Yasbeck, Mrs. Antoni (Selini Alexander)&#34;,&#34;Richards, Master. George Sibley&#34;,&#34;Saad, Mr. Amin&#34;,&#34;Augustsson, Mr. Albert&#34;,&#34;Allum, Mr. Owen George&#34;,&#34;Compton, Miss. Sara Rebecca&#34;,&#34;Pasic, Mr. Jakob&#34;,&#34;Sirota, Mr. Maurice&#34;,&#34;Chip, Mr. Chang&#34;,&#34;Marechal, Mr. Pierre&#34;,&#34;Alhomaki, Mr. Ilmari Rudolf&#34;,&#34;Mudd, Mr. Thomas Charles&#34;,&#34;Serepeca, Miss. Augusta&#34;,&#34;Lemberopolous, Mr. Peter L&#34;,&#34;Culumovic, Mr. Jeso&#34;,&#34;Abbing, Mr. Anthony&#34;,&#34;Sage, Mr. Douglas Bullen&#34;,&#34;Markoff, Mr. Marin&#34;,&#34;Harper, Rev. John&#34;,&#34;Goldenberg, Mrs. Samuel L (Edwiga Grabowska)&#34;,&#34;Andersson, Master. Sigvard Harald Elias&#34;,&#34;Svensson, Mr. Johan&#34;,&#34;Boulos, Miss. Nourelain&#34;,&#34;Lines, Miss. Mary Conover&#34;,&#34;Carter, Mrs. Ernest Courtenay (Lilian Hughes)&#34;,&#34;Aks, Mrs. Sam (Leah Rosen)&#34;,&#34;Wick, Mrs. George Dennick (Mary Hitchcock)&#34;,&#34;Daly, Mr. Peter Denis &#34;,&#34;Baclini, Mrs. Solomon (Latifa Qurban)&#34;,&#34;Razi, Mr. Raihed&#34;,&#34;Hansen, Mr. Claus Peter&#34;,&#34;Giles, Mr. Frederick Edward&#34;,&#34;Swift, Mrs. Frederick Joel (Margaret Welles Barron)&#34;,&#34;Sage, Miss. Dorothy Edith \&#34;Dolly\&#34;&#34;,&#34;Gill, Mr. John William&#34;,&#34;Bystrom, Mrs. (Karolina)&#34;,&#34;Duran y More, Miss. Asuncion&#34;,&#34;Roebling, Mr. Washington Augustus II&#34;,&#34;van Melkebeke, Mr. Philemon&#34;,&#34;Johnson, Master. Harold Theodor&#34;,&#34;Balkic, Mr. Cerin&#34;,&#34;Beckwith, Mrs. Richard Leonard (Sallie Monypeny)&#34;,&#34;Carlsson, Mr. Frans Olof&#34;,&#34;Vander Cruyssen, Mr. Victor&#34;,&#34;Abelson, Mrs. Samuel (Hannah Wizosky)&#34;,&#34;Najib, Miss. Adele Kiamie \&#34;Jane\&#34;&#34;,&#34;Gustafsson, Mr. Alfred Ossian&#34;,&#34;Petroff, Mr. Nedelio&#34;,&#34;Laleff, Mr. Kristo&#34;,&#34;Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)&#34;,&#34;Shelley, Mrs. William (Imanita Parrish Hall)&#34;,&#34;Markun, Mr. Johann&#34;,&#34;Dahlberg, Miss. Gerda Ulrika&#34;,&#34;Banfield, Mr. Frederick James&#34;,&#34;Sutehall, Mr. Henry Jr&#34;,&#34;Rice, Mrs. William (Margaret Norton)&#34;,&#34;Montvila, Rev. Juozas&#34;,&#34;Graham, Miss. Margaret Edith&#34;,&#34;Johnston, Miss. Catherine Helen \&#34;Carrie\&#34;&#34;,&#34;Behr, Mr. Karl Howell&#34;,&#34;Dooley, Mr. Patrick&#34;],[&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;],[22,38,26,35,35,null,54,2,27,14,4,58,20,39,14,55,2,null,31,null,35,34,15,28,8,38,null,19,null,null,40,null,null,66,28,42,null,21,18,14,40,27,null,3,19,null,null,null,null,18,7,21,49,29,65,null,21,28.5,5,11,22,38,45,4,null,null,29,19,17,26,32,16,21,26,32,25,null,null,0.83,30,22,29,null,28,17,33,16,null,23,24,29,20,46,26,59,null,71,23,34,34,28,null,21,33,37,28,21,null,38,null,47,14.5,22,20,17,21,70.5,29,24,2,21,null,32.5,32.5,54,12,null,24,null,45,33,20,47,29,25,23,19,37,16,24,null,22,24,19,18,19,27,9,36.5,42,51,22,55.5,40.5,null,51,16,30,null,null,44,40,26,17,1,9,null,45,null,28,61,4,1,21,56,18,null,50,30,36,null,null,9,1,4,null,null,45,40,36,32,19,19,3,44,58,null,42,null,24,28,null,34,45.5,18,2,32,26,16,40,24,35,22,30,null,31,27,42,32,30,16,27,51,null,38,22,19,20.5,18,null,35,29,59,5,24,null,44,8,19,33,null,null,29,22,30,44,25,24,37,54,null,29,62,30,41,29,null,30,35,50,null,3,52,40,null,36,16,25,58,35,null,25,41,37,null,63,45,null,7,35,65,28,16,19,null,33,30,22,42,22,26,19,36,24,24,null,23.5,2,null,50,null,null,19,null,null,0.92,null,17,30,30,24,18,26,28,43,26,24,54,31,40,22,27,30,22,null,36,61,36,31,16,null,45.5,38,16,null,null,29,41,45,45,2,24,28,25,36,24,40,null,3,42,23,null,15,25,null,28,22,38,null,null,40,29,45,35,null,30,60,null,null,24,25,18,19,22,3,null,22,27,20,19,42,1,32,35,null,18,1,36,null,17,36,21,28,23,24,22,31,46,23,28,39,26,21,28,20,34,51,3,21,null,null,null,33,null,44,null,34,18,30,10,null,21,29,28,18,null,28,19,null,32,28,null,42,17,50,14,21,24,64,31,45,20,25,28,null,4,13,34,5,52,36,null,30,49,null,29,65,null,50,null,48,34,47,48,null,38,null,56,null,0.75,null,38,33,23,22,null,34,29,22,2,9,null,50,63,25,null,35,58,30,9,null,21,55,71,21,null,54,null,25,24,17,21,null,37,16,18,33,null,28,26,29,null,36,54,24,47,34,null,36,32,30,22,null,44,null,40.5,50,null,39,23,2,null,17,null,30,7,45,30,null,22,36,9,11,32,50,64,19,null,33,8,17,27,null,22,22,62,48,null,39,36,null,40,28,null,null,24,19,29,null,32,62,53,36,null,16,19,34,39,null,32,25,39,54,36,null,18,47,60,22,null,35,52,47,null,37,36,null,49,null,49,24,null,null,44,35,36,30,27,22,40,39,null,null,null,35,24,34,26,4,26,27,42,20,21,21,61,57,21,26,null,80,51,32,null,9,28,32,31,41,null,20,24,2,null,0.75,48,19,56,null,23,null,18,21,null,18,24,null,32,23,58,50,40,47,36,20,32,25,null,43,null,40,31,70,31,null,18,24.5,18,43,36,null,27,20,14,60,25,14,19,18,15,31,4,null,25,60,52,44,null,49,42,18,35,18,25,26,39,45,42,22,null,24,null,48,29,52,19,38,27,null,33,6,17,34,50,27,20,30,null,25,25,29,11,null,23,23,28.5,48,35,null,null,null,36,21,24,31,70,16,30,19,31,4,6,33,23,48,0.67,28,18,34,33,null,41,20,36,16,51,null,30.5,null,32,24,48,57,null,54,18,null,5,null,43,13,17,29,null,25,25,18,8,1,46,null,16,null,null,25,39,49,31,30,30,34,31,11,0.42,27,31,39,18,39,33,26,39,35,6,30.5,null,23,31,43,10,52,27,38,27,2,null,null,1,null,62,15,0.83,null,23,18,39,21,null,32,null,20,16,30,34.5,17,42,null,35,28,null,4,74,9,16,44,18,45,51,24,null,41,21,48,null,24,42,27,31,null,4,26,47,33,47,28,15,20,19,null,56,25,33,22,28,25,39,27,19,null,26,32],[1,1,0,1,0,0,0,3,0,1,1,0,0,1,0,0,4,0,1,0,0,0,0,0,3,1,0,3,0,0,0,1,0,0,1,1,0,0,2,1,1,1,0,1,0,0,1,0,2,1,4,0,1,1,0,0,0,0,1,5,0,0,1,3,0,1,0,0,4,2,0,5,0,1,0,0,0,0,0,0,0,0,0,0,0,3,1,0,3,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,2,0,0,0,0,1,0,1,0,1,0,0,0,1,0,4,2,0,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,2,0,0,0,1,0,0,0,0,0,0,0,8,0,0,0,0,4,0,0,1,0,0,0,4,1,0,0,1,3,0,0,0,8,0,4,2,0,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,8,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,1,0,0,0,0,3,1,0,0,4,0,0,1,0,0,0,1,1,0,0,0,2,0,0,1,1,0,1,0,1,0,0,0,0,0,0,0,4,1,0,0,0,4,1,0,0,0,0,0,0,0,1,0,0,4,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,2,0,0,0,1,0,1,1,0,0,2,1,0,1,0,1,0,0,1,0,0,0,1,8,0,0,0,1,0,2,0,0,2,1,0,1,0,0,0,1,3,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,0,1,0,0,0,1,1,0,0,3,1,0,0,0,0,0,0,0,1,0,0,5,0,0,0,1,0,2,1,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,3,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,1,0,1,1,2,2,1,0,1,0,1,0,0,0,0,0,2,0,1,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,2,0,0,1,0,0,0,1,1,0,0,5,0,0,0,1,3,1,0,0,1,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,1,0,1,0,0,0,0,0,0,0,0,4,4,1,1,0,1,0,1,1,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,1,2,0,0,0,0,1,0,0,1,0,1,0,1,0,0,1,1,1,2,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,3,0,0,1,0,1,0,0,3,0,2,1,0,0,0,0,0,0,0,0,0,2,0,1,0,0,2,0,0,0,1,2,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,0,5,1,1,4,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,3,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,2,1,0,1,1,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,4,1,0,0,0,8,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,1,0,0,0,4,0,0,0,1,0,3,1,0,0,0,4,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,0,0,1,4,0,1,0,1,0,1,0,0,0,2,1,0,8,0,0,1,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,2,0,1,0,0,5,0,0,1,0,0,0,0,0,0,0,1,5,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,1,0,0,0,1,0,0,0,2,2,0,0,0,2,0,1,0,0,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,3,0,2,0,0,0,0,2,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,1,0,2,2,0,0,0,0,2,0,1,0,0,0,2,1,0,0,0,1,2,1,4,0,0,0,1,1,0,0,1,1,0,0,0,2,0,2,1,2,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,2,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,1,0,0,1,0,0,2,2,0,0,0,1,0,2,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0,0,0,0,0,2,0,0,0,0,0,2,1,0,1,0,0,0,2,1,0,0,0,1,2,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,2,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,4,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,2,0,0,0,2,0,0,0,0,2,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,2,0,2,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,3,4,0,1,0,0,0,0,2,1,0,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,0,1,2,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,2,0,2,0,0,0,2,2,2,2,0,0,0,0,0,1,1,2,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,2,0,1,0,0,0,0,0,2,0,1,0,0,0,0,1,0,0,0,0,0,0,0,2,0,5,0,0,0,0,2,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,2,0,0,1,5,0,0,0,2,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,6,1,0,0,0,2,1,2,1,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,2,0,0,1,1,0,0,0,1,1,0,0,2,1,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,1,0,0,0,2,0,0,0,1,2,0,0,0,2,0,0,0,0,0,0,1,0,1,2,1,0,0,0,0,0,0,0,0,0,2,0,0,0,1,0,2,1,0,0,1,1,0,0,2,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,0,1,0,2,0,1,1,0,1,1,0,3,0,0,0,0,2,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,5,0,0,2,0,0]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;PassengerId&lt;\/th&gt;\n      &lt;th&gt;Survived&lt;\/th&gt;\n      &lt;th&gt;Pclass&lt;\/th&gt;\n      &lt;th&gt;Name&lt;\/th&gt;\n      &lt;th&gt;Sex&lt;\/th&gt;\n      &lt;th&gt;Age&lt;\/th&gt;\n      &lt;th&gt;SibSp&lt;\/th&gt;\n      &lt;th&gt;Parch&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,3,6,7,8]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DT::datatable(df_test[,1:8])&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;,&#34;11&#34;,&#34;12&#34;,&#34;13&#34;,&#34;14&#34;,&#34;15&#34;,&#34;16&#34;,&#34;17&#34;,&#34;18&#34;,&#34;19&#34;,&#34;20&#34;,&#34;21&#34;,&#34;22&#34;,&#34;23&#34;,&#34;24&#34;,&#34;25&#34;,&#34;26&#34;,&#34;27&#34;,&#34;28&#34;,&#34;29&#34;,&#34;30&#34;,&#34;31&#34;,&#34;32&#34;,&#34;33&#34;,&#34;34&#34;,&#34;35&#34;,&#34;36&#34;,&#34;37&#34;,&#34;38&#34;,&#34;39&#34;,&#34;40&#34;,&#34;41&#34;,&#34;42&#34;,&#34;43&#34;,&#34;44&#34;,&#34;45&#34;,&#34;46&#34;,&#34;47&#34;,&#34;48&#34;,&#34;49&#34;,&#34;50&#34;,&#34;51&#34;,&#34;52&#34;,&#34;53&#34;,&#34;54&#34;,&#34;55&#34;,&#34;56&#34;,&#34;57&#34;,&#34;58&#34;,&#34;59&#34;,&#34;60&#34;,&#34;61&#34;,&#34;62&#34;,&#34;63&#34;,&#34;64&#34;,&#34;65&#34;,&#34;66&#34;,&#34;67&#34;,&#34;68&#34;,&#34;69&#34;,&#34;70&#34;,&#34;71&#34;,&#34;72&#34;,&#34;73&#34;,&#34;74&#34;,&#34;75&#34;,&#34;76&#34;,&#34;77&#34;,&#34;78&#34;,&#34;79&#34;,&#34;80&#34;,&#34;81&#34;,&#34;82&#34;,&#34;83&#34;,&#34;84&#34;,&#34;85&#34;,&#34;86&#34;,&#34;87&#34;,&#34;88&#34;,&#34;89&#34;,&#34;90&#34;,&#34;91&#34;,&#34;92&#34;,&#34;93&#34;,&#34;94&#34;,&#34;95&#34;,&#34;96&#34;,&#34;97&#34;,&#34;98&#34;,&#34;99&#34;,&#34;100&#34;,&#34;101&#34;,&#34;102&#34;,&#34;103&#34;,&#34;104&#34;,&#34;105&#34;,&#34;106&#34;,&#34;107&#34;,&#34;108&#34;,&#34;109&#34;,&#34;110&#34;,&#34;111&#34;,&#34;112&#34;,&#34;113&#34;,&#34;114&#34;,&#34;115&#34;,&#34;116&#34;,&#34;117&#34;,&#34;118&#34;,&#34;119&#34;,&#34;120&#34;,&#34;121&#34;,&#34;122&#34;,&#34;123&#34;,&#34;124&#34;,&#34;125&#34;,&#34;126&#34;,&#34;127&#34;,&#34;128&#34;,&#34;129&#34;,&#34;130&#34;,&#34;131&#34;,&#34;132&#34;,&#34;133&#34;,&#34;134&#34;,&#34;135&#34;,&#34;136&#34;,&#34;137&#34;,&#34;138&#34;,&#34;139&#34;,&#34;140&#34;,&#34;141&#34;,&#34;142&#34;,&#34;143&#34;,&#34;144&#34;,&#34;145&#34;,&#34;146&#34;,&#34;147&#34;,&#34;148&#34;,&#34;149&#34;,&#34;150&#34;,&#34;151&#34;,&#34;152&#34;,&#34;153&#34;,&#34;154&#34;,&#34;155&#34;,&#34;156&#34;,&#34;157&#34;,&#34;158&#34;,&#34;159&#34;,&#34;160&#34;,&#34;161&#34;,&#34;162&#34;,&#34;163&#34;,&#34;164&#34;,&#34;165&#34;,&#34;166&#34;,&#34;167&#34;,&#34;168&#34;,&#34;169&#34;,&#34;170&#34;,&#34;171&#34;,&#34;172&#34;,&#34;173&#34;,&#34;174&#34;,&#34;175&#34;,&#34;176&#34;,&#34;177&#34;,&#34;178&#34;,&#34;179&#34;,&#34;180&#34;,&#34;181&#34;,&#34;182&#34;,&#34;183&#34;,&#34;184&#34;,&#34;185&#34;,&#34;186&#34;,&#34;187&#34;,&#34;188&#34;,&#34;189&#34;,&#34;190&#34;,&#34;191&#34;,&#34;192&#34;,&#34;193&#34;,&#34;194&#34;,&#34;195&#34;,&#34;196&#34;,&#34;197&#34;,&#34;198&#34;,&#34;199&#34;,&#34;200&#34;,&#34;201&#34;,&#34;202&#34;,&#34;203&#34;,&#34;204&#34;,&#34;205&#34;,&#34;206&#34;,&#34;207&#34;,&#34;208&#34;,&#34;209&#34;,&#34;210&#34;,&#34;211&#34;,&#34;212&#34;,&#34;213&#34;,&#34;214&#34;,&#34;215&#34;,&#34;216&#34;,&#34;217&#34;,&#34;218&#34;,&#34;219&#34;,&#34;220&#34;,&#34;221&#34;,&#34;222&#34;,&#34;223&#34;,&#34;224&#34;,&#34;225&#34;,&#34;226&#34;,&#34;227&#34;,&#34;228&#34;,&#34;229&#34;,&#34;230&#34;,&#34;231&#34;,&#34;232&#34;,&#34;233&#34;,&#34;234&#34;,&#34;235&#34;,&#34;236&#34;,&#34;237&#34;,&#34;238&#34;,&#34;239&#34;,&#34;240&#34;,&#34;241&#34;,&#34;242&#34;,&#34;243&#34;,&#34;244&#34;,&#34;245&#34;,&#34;246&#34;,&#34;247&#34;,&#34;248&#34;,&#34;249&#34;,&#34;250&#34;,&#34;251&#34;,&#34;252&#34;,&#34;253&#34;,&#34;254&#34;,&#34;255&#34;,&#34;256&#34;,&#34;257&#34;,&#34;258&#34;,&#34;259&#34;,&#34;260&#34;,&#34;261&#34;,&#34;262&#34;,&#34;263&#34;,&#34;264&#34;,&#34;265&#34;,&#34;266&#34;,&#34;267&#34;,&#34;268&#34;,&#34;269&#34;,&#34;270&#34;,&#34;271&#34;,&#34;272&#34;,&#34;273&#34;,&#34;274&#34;,&#34;275&#34;,&#34;276&#34;,&#34;277&#34;,&#34;278&#34;,&#34;279&#34;,&#34;280&#34;,&#34;281&#34;,&#34;282&#34;,&#34;283&#34;,&#34;284&#34;,&#34;285&#34;,&#34;286&#34;,&#34;287&#34;,&#34;288&#34;,&#34;289&#34;,&#34;290&#34;,&#34;291&#34;,&#34;292&#34;,&#34;293&#34;,&#34;294&#34;,&#34;295&#34;,&#34;296&#34;,&#34;297&#34;,&#34;298&#34;,&#34;299&#34;,&#34;300&#34;,&#34;301&#34;,&#34;302&#34;,&#34;303&#34;,&#34;304&#34;,&#34;305&#34;,&#34;306&#34;,&#34;307&#34;,&#34;308&#34;,&#34;309&#34;,&#34;310&#34;,&#34;311&#34;,&#34;312&#34;,&#34;313&#34;,&#34;314&#34;,&#34;315&#34;,&#34;316&#34;,&#34;317&#34;,&#34;318&#34;,&#34;319&#34;,&#34;320&#34;,&#34;321&#34;,&#34;322&#34;,&#34;323&#34;,&#34;324&#34;,&#34;325&#34;,&#34;326&#34;,&#34;327&#34;,&#34;328&#34;,&#34;329&#34;,&#34;330&#34;,&#34;331&#34;,&#34;332&#34;,&#34;333&#34;,&#34;334&#34;,&#34;335&#34;,&#34;336&#34;,&#34;337&#34;,&#34;338&#34;,&#34;339&#34;,&#34;340&#34;,&#34;341&#34;,&#34;342&#34;,&#34;343&#34;,&#34;344&#34;,&#34;345&#34;,&#34;346&#34;,&#34;347&#34;,&#34;348&#34;,&#34;349&#34;,&#34;350&#34;,&#34;351&#34;,&#34;352&#34;,&#34;353&#34;,&#34;354&#34;,&#34;355&#34;,&#34;356&#34;,&#34;357&#34;,&#34;358&#34;,&#34;359&#34;,&#34;360&#34;,&#34;361&#34;,&#34;362&#34;,&#34;363&#34;,&#34;364&#34;,&#34;365&#34;,&#34;366&#34;,&#34;367&#34;,&#34;368&#34;,&#34;369&#34;,&#34;370&#34;,&#34;371&#34;,&#34;372&#34;,&#34;373&#34;,&#34;374&#34;,&#34;375&#34;,&#34;376&#34;,&#34;377&#34;,&#34;378&#34;,&#34;379&#34;,&#34;380&#34;,&#34;381&#34;,&#34;382&#34;,&#34;383&#34;,&#34;384&#34;,&#34;385&#34;,&#34;386&#34;,&#34;387&#34;,&#34;388&#34;,&#34;389&#34;,&#34;390&#34;,&#34;391&#34;,&#34;392&#34;,&#34;393&#34;,&#34;394&#34;,&#34;395&#34;,&#34;396&#34;,&#34;397&#34;,&#34;398&#34;,&#34;399&#34;,&#34;400&#34;,&#34;401&#34;,&#34;402&#34;,&#34;403&#34;,&#34;404&#34;,&#34;405&#34;,&#34;406&#34;,&#34;407&#34;,&#34;408&#34;,&#34;409&#34;,&#34;410&#34;,&#34;411&#34;,&#34;412&#34;,&#34;413&#34;,&#34;414&#34;,&#34;415&#34;,&#34;416&#34;,&#34;417&#34;,&#34;418&#34;],[892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309],[3,3,2,3,3,3,3,2,3,3,3,1,1,2,1,2,2,3,3,3,1,3,1,1,1,3,1,3,1,3,2,2,3,3,1,3,3,3,3,3,3,1,3,2,1,3,1,3,1,3,1,2,2,1,2,3,3,3,3,1,3,2,3,3,1,2,3,1,1,1,3,3,3,1,1,1,3,1,2,3,3,1,1,3,2,3,3,3,3,2,3,3,1,3,1,3,1,3,3,3,1,2,3,3,3,3,3,3,3,2,2,3,1,3,1,3,3,3,1,2,2,3,1,3,3,3,3,3,2,3,3,1,3,3,3,3,3,2,3,3,3,1,1,2,1,3,1,3,1,2,1,3,3,3,3,3,1,3,1,3,3,3,2,3,2,3,1,3,1,3,3,3,3,3,3,2,2,1,2,1,2,1,1,3,1,2,2,3,3,2,2,1,3,2,2,3,1,3,2,3,3,3,1,2,2,1,3,2,1,3,3,3,2,2,3,1,3,1,1,3,2,3,2,3,1,3,3,3,3,2,2,1,3,3,1,3,1,3,2,1,1,2,1,3,3,1,2,2,2,3,2,3,1,3,3,3,3,3,2,3,3,3,2,3,2,3,1,3,3,3,1,3,1,3,3,2,2,2,2,2,3,3,3,3,3,3,3,1,3,3,1,3,3,1,3,3,2,3,1,3,3,2,2,3,3,1,1,3,1,3,3,3,3,3,1,3,1,2,3,2,3,3,2,1,1,3,2,1,2,2,2,1,3,3,3,1,2,3,2,3,2,3,3,1,3,3,2,3,2,2,1,2,2,2,3,1,1,3,3,3,3,2,2,3,1,3,3,3,1,2,2,1,1,2,1,1,3,2,1,3,3,3,3,3,2,2,3,2,3,3,1,1,3,2,3,1,3,1,3,3,1,2,1,1,1,2,2,1,3,3,3,1,3,3,1,3,3,3],[&#34;Kelly, Mr. James&#34;,&#34;Wilkes, Mrs. James (Ellen Needs)&#34;,&#34;Myles, Mr. Thomas Francis&#34;,&#34;Wirz, Mr. Albert&#34;,&#34;Hirvonen, Mrs. Alexander (Helga E Lindqvist)&#34;,&#34;Svensson, Mr. Johan Cervin&#34;,&#34;Connolly, Miss. Kate&#34;,&#34;Caldwell, Mr. Albert Francis&#34;,&#34;Abrahim, Mrs. Joseph (Sophie Halaut Easu)&#34;,&#34;Davies, Mr. John Samuel&#34;,&#34;Ilieff, Mr. Ylio&#34;,&#34;Jones, Mr. Charles Cresson&#34;,&#34;Snyder, Mrs. John Pillsbury (Nelle Stevenson)&#34;,&#34;Howard, Mr. Benjamin&#34;,&#34;Chaffee, Mrs. Herbert Fuller (Carrie Constance Toogood)&#34;,&#34;del Carlo, Mrs. Sebastiano (Argenia Genovesi)&#34;,&#34;Keane, Mr. Daniel&#34;,&#34;Assaf, Mr. Gerios&#34;,&#34;Ilmakangas, Miss. Ida Livija&#34;,&#34;Assaf Khalil, Mrs. Mariana (Miriam\&#34;)\&#34;&#34;,&#34;Rothschild, Mr. Martin&#34;,&#34;Olsen, Master. Artur Karl&#34;,&#34;Flegenheim, Mrs. Alfred (Antoinette)&#34;,&#34;Williams, Mr. Richard Norris II&#34;,&#34;Ryerson, Mrs. Arthur Larned (Emily Maria Borie)&#34;,&#34;Robins, Mr. Alexander A&#34;,&#34;Ostby, Miss. Helene Ragnhild&#34;,&#34;Daher, Mr. Shedid&#34;,&#34;Brady, Mr. John Bertram&#34;,&#34;Samaan, Mr. Elias&#34;,&#34;Louch, Mr. Charles Alexander&#34;,&#34;Jefferys, Mr. Clifford Thomas&#34;,&#34;Dean, Mrs. Bertram (Eva Georgetta Light)&#34;,&#34;Johnston, Mrs. Andrew G (Elizabeth Lily\&#34; Watson)\&#34;&#34;,&#34;Mock, Mr. Philipp Edmund&#34;,&#34;Katavelas, Mr. Vassilios (Catavelas Vassilios\&#34;)\&#34;&#34;,&#34;Roth, Miss. Sarah A&#34;,&#34;Cacic, Miss. Manda&#34;,&#34;Sap, Mr. Julius&#34;,&#34;Hee, Mr. Ling&#34;,&#34;Karun, Mr. Franz&#34;,&#34;Franklin, Mr. Thomas Parham&#34;,&#34;Goldsmith, Mr. Nathan&#34;,&#34;Corbett, Mrs. Walter H (Irene Colvin)&#34;,&#34;Kimball, Mrs. Edwin Nelson Jr (Gertrude Parsons)&#34;,&#34;Peltomaki, Mr. Nikolai Johannes&#34;,&#34;Chevre, Mr. Paul Romaine&#34;,&#34;Shaughnessy, Mr. Patrick&#34;,&#34;Bucknell, Mrs. William Robert (Emma Eliza Ward)&#34;,&#34;Coutts, Mrs. William (Winnie Minnie\&#34; Treanor)\&#34;&#34;,&#34;Smith, Mr. Lucien Philip&#34;,&#34;Pulbaum, Mr. Franz&#34;,&#34;Hocking, Miss. Ellen Nellie\&#34;\&#34;&#34;,&#34;Fortune, Miss. Ethel Flora&#34;,&#34;Mangiavacchi, Mr. Serafino Emilio&#34;,&#34;Rice, Master. Albert&#34;,&#34;Cor, Mr. Bartol&#34;,&#34;Abelseth, Mr. Olaus Jorgensen&#34;,&#34;Davison, Mr. Thomas Henry&#34;,&#34;Chaudanson, Miss. Victorine&#34;,&#34;Dika, Mr. Mirko&#34;,&#34;McCrae, Mr. Arthur Gordon&#34;,&#34;Bjorklund, Mr. Ernst Herbert&#34;,&#34;Bradley, Miss. Bridget Delia&#34;,&#34;Ryerson, Master. John Borie&#34;,&#34;Corey, Mrs. Percy C (Mary Phyllis Elizabeth Miller)&#34;,&#34;Burns, Miss. Mary Delia&#34;,&#34;Moore, Mr. Clarence Bloomfield&#34;,&#34;Tucker, Mr. Gilbert Milligan Jr&#34;,&#34;Fortune, Mrs. Mark (Mary McDougald)&#34;,&#34;Mulvihill, Miss. Bertha E&#34;,&#34;Minkoff, Mr. Lazar&#34;,&#34;Nieminen, Miss. Manta Josefina&#34;,&#34;Ovies y Rodriguez, Mr. Servando&#34;,&#34;Geiger, Miss. Amalie&#34;,&#34;Keeping, Mr. Edwin&#34;,&#34;Miles, Mr. Frank&#34;,&#34;Cornell, Mrs. Robert Clifford (Malvina Helen Lamson)&#34;,&#34;Aldworth, Mr. Charles Augustus&#34;,&#34;Doyle, Miss. Elizabeth&#34;,&#34;Boulos, Master. Akar&#34;,&#34;Straus, Mr. Isidor&#34;,&#34;Case, Mr. Howard Brown&#34;,&#34;Demetri, Mr. Marinko&#34;,&#34;Lamb, Mr. John Joseph&#34;,&#34;Khalil, Mr. Betros&#34;,&#34;Barry, Miss. Julia&#34;,&#34;Badman, Miss. Emily Louisa&#34;,&#34;O&#39;Donoghue, Ms. Bridget&#34;,&#34;Wells, Master. Ralph Lester&#34;,&#34;Dyker, Mrs. Adolf Fredrik (Anna Elisabeth Judith Andersson)&#34;,&#34;Pedersen, Mr. Olaf&#34;,&#34;Davidson, Mrs. Thornton (Orian Hays)&#34;,&#34;Guest, Mr. Robert&#34;,&#34;Birnbaum, Mr. Jakob&#34;,&#34;Tenglin, Mr. Gunnar Isidor&#34;,&#34;Cavendish, Mrs. Tyrell William (Julia Florence Siegel)&#34;,&#34;Makinen, Mr. Kalle Edvard&#34;,&#34;Braf, Miss. Elin Ester Maria&#34;,&#34;Nancarrow, Mr. William Henry&#34;,&#34;Stengel, Mrs. Charles Emil Henry (Annie May Morris)&#34;,&#34;Weisz, Mr. Leopold&#34;,&#34;Foley, Mr. William&#34;,&#34;Johansson Palmquist, Mr. Oskar Leander&#34;,&#34;Thomas, Mrs. Alexander (Thamine Thelma\&#34;)\&#34;&#34;,&#34;Holthen, Mr. Johan Martin&#34;,&#34;Buckley, Mr. Daniel&#34;,&#34;Ryan, Mr. Edward&#34;,&#34;Willer, Mr. Aaron (Abi Weller\&#34;)\&#34;&#34;,&#34;Swane, Mr. George&#34;,&#34;Stanton, Mr. Samuel Ward&#34;,&#34;Shine, Miss. Ellen Natalia&#34;,&#34;Evans, Miss. Edith Corse&#34;,&#34;Buckley, Miss. Katherine&#34;,&#34;Straus, Mrs. Isidor (Rosalie Ida Blun)&#34;,&#34;Chronopoulos, Mr. Demetrios&#34;,&#34;Thomas, Mr. John&#34;,&#34;Sandstrom, Miss. Beatrice Irene&#34;,&#34;Beattie, Mr. Thomson&#34;,&#34;Chapman, Mrs. John Henry (Sara Elizabeth Lawry)&#34;,&#34;Watt, Miss. Bertha J&#34;,&#34;Kiernan, Mr. John&#34;,&#34;Schabert, Mrs. Paul (Emma Mock)&#34;,&#34;Carver, Mr. Alfred John&#34;,&#34;Kennedy, Mr. John&#34;,&#34;Cribb, Miss. Laura Alice&#34;,&#34;Brobeck, Mr. Karl Rudolf&#34;,&#34;McCoy, Miss. Alicia&#34;,&#34;Bowenur, Mr. Solomon&#34;,&#34;Petersen, Mr. Marius&#34;,&#34;Spinner, Mr. Henry John&#34;,&#34;Gracie, Col. Archibald IV&#34;,&#34;Lefebre, Mrs. Frank (Frances)&#34;,&#34;Thomas, Mr. Charles P&#34;,&#34;Dintcheff, Mr. Valtcho&#34;,&#34;Carlsson, Mr. Carl Robert&#34;,&#34;Zakarian, Mr. Mapriededer&#34;,&#34;Schmidt, Mr. August&#34;,&#34;Drapkin, Miss. Jennie&#34;,&#34;Goodwin, Mr. Charles Frederick&#34;,&#34;Goodwin, Miss. Jessie Allis&#34;,&#34;Daniels, Miss. Sarah&#34;,&#34;Ryerson, Mr. Arthur Larned&#34;,&#34;Beauchamp, Mr. Henry James&#34;,&#34;Lindeberg-Lind, Mr. Erik Gustaf (Mr Edward Lingrey\&#34;)\&#34;&#34;,&#34;Vander Planke, Mr. Julius&#34;,&#34;Hilliard, Mr. Herbert Henry&#34;,&#34;Davies, Mr. Evan&#34;,&#34;Crafton, Mr. John Bertram&#34;,&#34;Lahtinen, Rev. William&#34;,&#34;Earnshaw, Mrs. Boulton (Olive Potter)&#34;,&#34;Matinoff, Mr. Nicola&#34;,&#34;Storey, Mr. Thomas&#34;,&#34;Klasen, Mrs. (Hulda Kristina Eugenia Lofqvist)&#34;,&#34;Asplund, Master. Filip Oscar&#34;,&#34;Duquemin, Mr. Joseph&#34;,&#34;Bird, Miss. Ellen&#34;,&#34;Lundin, Miss. Olga Elida&#34;,&#34;Borebank, Mr. John James&#34;,&#34;Peacock, Mrs. Benjamin (Edith Nile)&#34;,&#34;Smyth, Miss. Julia&#34;,&#34;Touma, Master. Georges Youssef&#34;,&#34;Wright, Miss. Marion&#34;,&#34;Pearce, Mr. Ernest&#34;,&#34;Peruschitz, Rev. Joseph Maria&#34;,&#34;Kink-Heilmann, Mrs. Anton (Luise Heilmann)&#34;,&#34;Brandeis, Mr. Emil&#34;,&#34;Ford, Mr. Edward Watson&#34;,&#34;Cassebeer, Mrs. Henry Arthur Jr (Eleanor Genevieve Fosdick)&#34;,&#34;Hellstrom, Miss. Hilda Maria&#34;,&#34;Lithman, Mr. Simon&#34;,&#34;Zakarian, Mr. Ortin&#34;,&#34;Dyker, Mr. Adolf Fredrik&#34;,&#34;Torfa, Mr. Assad&#34;,&#34;Asplund, Mr. Carl Oscar Vilhelm Gustafsson&#34;,&#34;Brown, Miss. Edith Eileen&#34;,&#34;Sincock, Miss. Maude&#34;,&#34;Stengel, Mr. Charles Emil Henry&#34;,&#34;Becker, Mrs. Allen Oliver (Nellie E Baumgardner)&#34;,&#34;Compton, Mrs. Alexander Taylor (Mary Eliza Ingersoll)&#34;,&#34;McCrie, Mr. James Matthew&#34;,&#34;Compton, Mr. Alexander Taylor Jr&#34;,&#34;Marvin, Mrs. Daniel Warner (Mary Graham Carmichael Farquarson)&#34;,&#34;Lane, Mr. Patrick&#34;,&#34;Douglas, Mrs. Frederick Charles (Mary Helene Baxter)&#34;,&#34;Maybery, Mr. Frank Hubert&#34;,&#34;Phillips, Miss. Alice Frances Louisa&#34;,&#34;Davies, Mr. Joseph&#34;,&#34;Sage, Miss. Ada&#34;,&#34;Veal, Mr. James&#34;,&#34;Angle, Mr. William A&#34;,&#34;Salomon, Mr. Abraham L&#34;,&#34;van Billiard, Master. Walter John&#34;,&#34;Lingane, Mr. John&#34;,&#34;Drew, Master. Marshall Brines&#34;,&#34;Karlsson, Mr. Julius Konrad Eugen&#34;,&#34;Spedden, Master. Robert Douglas&#34;,&#34;Nilsson, Miss. Berta Olivia&#34;,&#34;Baimbrigge, Mr. Charles Robert&#34;,&#34;Rasmussen, Mrs. (Lena Jacobsen Solvang)&#34;,&#34;Murphy, Miss. Nora&#34;,&#34;Danbom, Master. Gilbert Sigvard Emanuel&#34;,&#34;Astor, Col. John Jacob&#34;,&#34;Quick, Miss. Winifred Vera&#34;,&#34;Andrew, Mr. Frank Thomas&#34;,&#34;Omont, Mr. Alfred Fernand&#34;,&#34;McGowan, Miss. Katherine&#34;,&#34;Collett, Mr. Sidney C Stuart&#34;,&#34;Rosenbaum, Miss. Edith Louise&#34;,&#34;Delalic, Mr. Redjo&#34;,&#34;Andersen, Mr. Albert Karvin&#34;,&#34;Finoli, Mr. Luigi&#34;,&#34;Deacon, Mr. Percy William&#34;,&#34;Howard, Mrs. Benjamin (Ellen Truelove Arman)&#34;,&#34;Andersson, Miss. Ida Augusta Margareta&#34;,&#34;Head, Mr. Christopher&#34;,&#34;Mahon, Miss. Bridget Delia&#34;,&#34;Wick, Mr. George Dennick&#34;,&#34;Widener, Mrs. George Dunton (Eleanor Elkins)&#34;,&#34;Thomson, Mr. Alexander Morrison&#34;,&#34;Duran y More, Miss. Florentina&#34;,&#34;Reynolds, Mr. Harold J&#34;,&#34;Cook, Mrs. (Selena Rogers)&#34;,&#34;Karlsson, Mr. Einar Gervasius&#34;,&#34;Candee, Mrs. Edward (Helen Churchill Hungerford)&#34;,&#34;Moubarek, Mrs. George (Omine Amenia\&#34; Alexander)\&#34;&#34;,&#34;Asplund, Mr. Johan Charles&#34;,&#34;McNeill, Miss. Bridget&#34;,&#34;Everett, Mr. Thomas James&#34;,&#34;Hocking, Mr. Samuel James Metcalfe&#34;,&#34;Sweet, Mr. George Frederick&#34;,&#34;Willard, Miss. Constance&#34;,&#34;Wiklund, Mr. Karl Johan&#34;,&#34;Linehan, Mr. Michael&#34;,&#34;Cumings, Mr. John Bradley&#34;,&#34;Vendel, Mr. Olof Edvin&#34;,&#34;Warren, Mr. Frank Manley&#34;,&#34;Baccos, Mr. Raffull&#34;,&#34;Hiltunen, Miss. Marta&#34;,&#34;Douglas, Mrs. Walter Donald (Mahala Dutton)&#34;,&#34;Lindstrom, Mrs. Carl Johan (Sigrid Posse)&#34;,&#34;Christy, Mrs. (Alice Frances)&#34;,&#34;Spedden, Mr. Frederic Oakley&#34;,&#34;Hyman, Mr. Abraham&#34;,&#34;Johnston, Master. William Arthur Willie\&#34;\&#34;&#34;,&#34;Kenyon, Mr. Frederick R&#34;,&#34;Karnes, Mrs. J Frank (Claire Bennett)&#34;,&#34;Drew, Mr. James Vivian&#34;,&#34;Hold, Mrs. Stephen (Annie Margaret Hill)&#34;,&#34;Khalil, Mrs. Betros (Zahie Maria\&#34; Elias)\&#34;&#34;,&#34;West, Miss. Barbara J&#34;,&#34;Abrahamsson, Mr. Abraham August Johannes&#34;,&#34;Clark, Mr. Walter Miller&#34;,&#34;Salander, Mr. Karl Johan&#34;,&#34;Wenzel, Mr. Linhart&#34;,&#34;MacKay, Mr. George William&#34;,&#34;Mahon, Mr. John&#34;,&#34;Niklasson, Mr. Samuel&#34;,&#34;Bentham, Miss. Lilian W&#34;,&#34;Midtsjo, Mr. Karl Albert&#34;,&#34;de Messemaeker, Mr. Guillaume Joseph&#34;,&#34;Nilsson, Mr. August Ferdinand&#34;,&#34;Wells, Mrs. Arthur Henry (Addie\&#34; Dart Trevaskis)\&#34;&#34;,&#34;Klasen, Miss. Gertrud Emilia&#34;,&#34;Portaluppi, Mr. Emilio Ilario Giuseppe&#34;,&#34;Lyntakoff, Mr. Stanko&#34;,&#34;Chisholm, Mr. Roderick Robert Crispin&#34;,&#34;Warren, Mr. Charles William&#34;,&#34;Howard, Miss. May Elizabeth&#34;,&#34;Pokrnic, Mr. Mate&#34;,&#34;McCaffry, Mr. Thomas Francis&#34;,&#34;Fox, Mr. Patrick&#34;,&#34;Clark, Mrs. Walter Miller (Virginia McDowell)&#34;,&#34;Lennon, Miss. Mary&#34;,&#34;Saade, Mr. Jean Nassr&#34;,&#34;Bryhl, Miss. Dagmar Jenny Ingeborg &#34;,&#34;Parker, Mr. Clifford Richard&#34;,&#34;Faunthorpe, Mr. Harry&#34;,&#34;Ware, Mr. John James&#34;,&#34;Oxenham, Mr. Percy Thomas&#34;,&#34;Oreskovic, Miss. Jelka&#34;,&#34;Peacock, Master. Alfred Edward&#34;,&#34;Fleming, Miss. Honora&#34;,&#34;Touma, Miss. Maria Youssef&#34;,&#34;Rosblom, Miss. Salli Helena&#34;,&#34;Dennis, Mr. William&#34;,&#34;Franklin, Mr. Charles (Charles Fardon)&#34;,&#34;Snyder, Mr. John Pillsbury&#34;,&#34;Mardirosian, Mr. Sarkis&#34;,&#34;Ford, Mr. Arthur&#34;,&#34;Rheims, Mr. George Alexander Lucien&#34;,&#34;Daly, Miss. Margaret Marcella Maggie\&#34;\&#34;&#34;,&#34;Nasr, Mr. Mustafa&#34;,&#34;Dodge, Dr. Washington&#34;,&#34;Wittevrongel, Mr. Camille&#34;,&#34;Angheloff, Mr. Minko&#34;,&#34;Laroche, Miss. Louise&#34;,&#34;Samaan, Mr. Hanna&#34;,&#34;Loring, Mr. Joseph Holland&#34;,&#34;Johansson, Mr. Nils&#34;,&#34;Olsson, Mr. Oscar Wilhelm&#34;,&#34;Malachard, Mr. Noel&#34;,&#34;Phillips, Mr. Escott Robert&#34;,&#34;Pokrnic, Mr. Tome&#34;,&#34;McCarthy, Miss. Catherine Katie\&#34;\&#34;&#34;,&#34;Crosby, Mrs. Edward Gifford (Catherine Elizabeth Halstead)&#34;,&#34;Allison, Mr. Hudson Joshua Creighton&#34;,&#34;Aks, Master. Philip Frank&#34;,&#34;Hays, Mr. Charles Melville&#34;,&#34;Hansen, Mrs. Claus Peter (Jennie L Howard)&#34;,&#34;Cacic, Mr. Jego Grga&#34;,&#34;Vartanian, Mr. David&#34;,&#34;Sadowitz, Mr. Harry&#34;,&#34;Carr, Miss. Jeannie&#34;,&#34;White, Mrs. John Stuart (Ella Holmes)&#34;,&#34;Hagardon, Miss. Kate&#34;,&#34;Spencer, Mr. William Augustus&#34;,&#34;Rogers, Mr. Reginald Harry&#34;,&#34;Jonsson, Mr. Nils Hilding&#34;,&#34;Jefferys, Mr. Ernest Wilfred&#34;,&#34;Andersson, Mr. Johan Samuel&#34;,&#34;Krekorian, Mr. Neshan&#34;,&#34;Nesson, Mr. Israel&#34;,&#34;Rowe, Mr. Alfred G&#34;,&#34;Kreuchen, Miss. Emilie&#34;,&#34;Assam, Mr. Ali&#34;,&#34;Becker, Miss. Ruth Elizabeth&#34;,&#34;Rosenshine, Mr. George (Mr George Thorne\&#34;)\&#34;&#34;,&#34;Clarke, Mr. Charles Valentine&#34;,&#34;Enander, Mr. Ingvar&#34;,&#34;Davies, Mrs. John Morgan (Elizabeth Agnes Mary White) &#34;,&#34;Dulles, Mr. William Crothers&#34;,&#34;Thomas, Mr. Tannous&#34;,&#34;Nakid, Mrs. Said (Waika Mary\&#34; Mowad)\&#34;&#34;,&#34;Cor, Mr. Ivan&#34;,&#34;Maguire, Mr. John Edward&#34;,&#34;de Brito, Mr. Jose Joaquim&#34;,&#34;Elias, Mr. Joseph&#34;,&#34;Denbury, Mr. Herbert&#34;,&#34;Betros, Master. Seman&#34;,&#34;Fillbrook, Mr. Joseph Charles&#34;,&#34;Lundstrom, Mr. Thure Edvin&#34;,&#34;Sage, Mr. John George&#34;,&#34;Cardeza, Mrs. James Warburton Martinez (Charlotte Wardle Drake)&#34;,&#34;van Billiard, Master. James William&#34;,&#34;Abelseth, Miss. Karen Marie&#34;,&#34;Botsford, Mr. William Hull&#34;,&#34;Whabee, Mrs. George Joseph (Shawneene Abi-Saab)&#34;,&#34;Giles, Mr. Ralph&#34;,&#34;Walcroft, Miss. Nellie&#34;,&#34;Greenfield, Mrs. Leo David (Blanche Strouse)&#34;,&#34;Stokes, Mr. Philip Joseph&#34;,&#34;Dibden, Mr. William&#34;,&#34;Herman, Mr. Samuel&#34;,&#34;Dean, Miss. Elizabeth Gladys Millvina\&#34;\&#34;&#34;,&#34;Julian, Mr. Henry Forbes&#34;,&#34;Brown, Mrs. John Murray (Caroline Lane Lamson)&#34;,&#34;Lockyer, Mr. Edward&#34;,&#34;O&#39;Keefe, Mr. Patrick&#34;,&#34;Lindell, Mrs. Edvard Bengtsson (Elin Gerda Persson)&#34;,&#34;Sage, Master. William Henry&#34;,&#34;Mallet, Mrs. Albert (Antoinette Magnin)&#34;,&#34;Ware, Mrs. John James (Florence Louise Long)&#34;,&#34;Strilic, Mr. Ivan&#34;,&#34;Harder, Mrs. George Achilles (Dorothy Annan)&#34;,&#34;Sage, Mrs. John (Annie Bullen)&#34;,&#34;Caram, Mr. Joseph&#34;,&#34;Riihivouri, Miss. Susanna Juhantytar Sanni\&#34;\&#34;&#34;,&#34;Gibson, Mrs. Leonard (Pauline C Boeson)&#34;,&#34;Pallas y Castello, Mr. Emilio&#34;,&#34;Giles, Mr. Edgar&#34;,&#34;Wilson, Miss. Helen Alice&#34;,&#34;Ismay, Mr. Joseph Bruce&#34;,&#34;Harbeck, Mr. William H&#34;,&#34;Dodge, Mrs. Washington (Ruth Vidaver)&#34;,&#34;Bowen, Miss. Grace Scott&#34;,&#34;Kink, Miss. Maria&#34;,&#34;Cotterill, Mr. Henry Harry\&#34;\&#34;&#34;,&#34;Hipkins, Mr. William Edward&#34;,&#34;Asplund, Master. Carl Edgar&#34;,&#34;O&#39;Connor, Mr. Patrick&#34;,&#34;Foley, Mr. Joseph&#34;,&#34;Risien, Mrs. Samuel (Emma)&#34;,&#34;McNamee, Mrs. Neal (Eileen O&#39;Leary)&#34;,&#34;Wheeler, Mr. Edwin Frederick\&#34;\&#34;&#34;,&#34;Herman, Miss. Kate&#34;,&#34;Aronsson, Mr. Ernst Axel Algot&#34;,&#34;Ashby, Mr. John&#34;,&#34;Canavan, Mr. Patrick&#34;,&#34;Palsson, Master. Paul Folke&#34;,&#34;Payne, Mr. Vivian Ponsonby&#34;,&#34;Lines, Mrs. Ernest H (Elizabeth Lindsey James)&#34;,&#34;Abbott, Master. Eugene Joseph&#34;,&#34;Gilbert, Mr. William&#34;,&#34;Kink-Heilmann, Mr. Anton&#34;,&#34;Smith, Mrs. Lucien Philip (Mary Eloise Hughes)&#34;,&#34;Colbert, Mr. Patrick&#34;,&#34;Frolicher-Stehli, Mrs. Maxmillian (Margaretha Emerentia Stehli)&#34;,&#34;Larsson-Rondberg, Mr. Edvard A&#34;,&#34;Conlon, Mr. Thomas Henry&#34;,&#34;Bonnell, Miss. Caroline&#34;,&#34;Gale, Mr. Harry&#34;,&#34;Gibson, Miss. Dorothy Winifred&#34;,&#34;Carrau, Mr. Jose Pedro&#34;,&#34;Frauenthal, Mr. Isaac Gerald&#34;,&#34;Nourney, Mr. Alfred (Baron von Drachstedt\&#34;)\&#34;&#34;,&#34;Ware, Mr. William Jeffery&#34;,&#34;Widener, Mr. George Dunton&#34;,&#34;Riordan, Miss. Johanna Hannah\&#34;\&#34;&#34;,&#34;Peacock, Miss. Treasteall&#34;,&#34;Naughton, Miss. Hannah&#34;,&#34;Minahan, Mrs. William Edward (Lillian E Thorpe)&#34;,&#34;Henriksson, Miss. Jenny Lovisa&#34;,&#34;Spector, Mr. Woolf&#34;,&#34;Oliva y Ocana, Dona. Fermina&#34;,&#34;Saether, Mr. Simon Sivertsen&#34;,&#34;Ware, Mr. Frederick&#34;,&#34;Peter, Master. Michael J&#34;],[&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;female&#34;,&#34;male&#34;,&#34;female&#34;,&#34;male&#34;,&#34;male&#34;,&#34;male&#34;],[34.5,47,62,27,22,14,30,26,18,21,null,46,23,63,47,24,35,21,27,45,55,9,null,21,48,50,22,22.5,41,null,50,24,33,null,30,18.5,null,21,25,null,39,null,41,30,45,25,45,null,60,36,24,27,20,28,null,10,35,25,null,36,17,32,18,22,13,null,18,47,31,60,24,21,29,28.5,35,32.5,null,55,30,24,6,67,49,null,null,null,27,18,null,2,22,null,27,null,25,25,76,29,20,33,43,27,null,26,16,28,21,null,null,18.5,41,null,36,18.5,63,18,null,1,36,29,12,null,35,28,null,17,22,null,42,24,32,53,null,null,43,24,26.5,26,23,40,10,33,61,28,42,31,null,22,null,30,23,null,60.5,36,13,24,29,23,42,26,null,7,26,null,41,26,48,18,null,22,null,27,23,null,40,15,20,54,36,64,30,37,18,null,27,40,21,17,null,40,34,null,11.5,61,8,33,6,18,23,null,null,0.33,47,8,25,null,35,24,33,25,32,null,17,60,38,42,null,57,50,null,30,21,22,21,53,null,23,null,40.5,36,14,21,21,null,39,20,64,20,18,48,55,45,45,null,null,41,22,42,29,null,0.92,20,27,24,32.5,null,null,28,19,21,36.5,21,29,1,30,null,null,null,null,17,46,null,26,null,null,20,28,40,30,22,23,0.75,null,9,2,36,null,24,null,null,null,30,null,53,36,26,1,null,30,29,32,null,43,24,null,64,30,0.83,55,45,18,22,null,37,55,17,57,19,27,22,26,25,26,33,39,23,12,46,29,21,48,39,null,19,27,30,32,39,25,null,18,32,null,58,null,16,26,38,24,31,45,25,18,49,0.17,50,59,null,null,30,14.5,24,31,27,25,null,null,22,45,29,21,31,49,44,54,45,22,21,55,5,null,26,null,19,null,24,24,57,21,6,23,51,13,47,29,18,24,48,22,31,30,38,22,17,43,20,23,50,null,3,null,37,28,null,39,38.5,null,null],[0,1,0,0,1,0,0,1,0,2,0,0,1,1,1,1,0,0,1,0,1,0,0,0,1,1,0,0,0,2,1,2,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,2,3,0,4,0,0,1,0,0,0,0,0,2,0,0,0,0,1,0,0,0,0,0,0,0,2,0,0,1,1,0,0,0,1,0,0,0,1,1,0,1,0,0,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,1,1,0,0,0,0,2,0,0,0,0,0,1,0,0,0,0,0,1,5,0,1,0,0,3,0,0,0,1,0,0,0,0,4,0,0,0,0,0,0,1,0,0,0,1,0,2,0,0,0,0,1,0,1,0,0,1,0,0,0,1,1,0,1,0,0,2,8,0,1,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,4,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,1,1,0,0,1,0,1,1,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,1,1,0,0,1,0,1,1,0,0,1,0,0,0,0,0,1,0,0,1,2,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,0,2,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,1,0,2,0,0,1,8,1,0,0,1,1,1,0,0,0,1,0,0,0,1,0,2,0,0,4,0,0,0,1,0,1,0,0,0,3,0,0,0,0,3,1,0,1,0,0,0,1,0,0,1,0,1,1,0,1,0,1,0,0,0,0,0,1],[0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,3,0,1,0,0,0,0,0,2,2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,2,0,0,1,2,0,1,0,0,0,0,0,0,0,0,2,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,4,0,0,0,0,0,0,6,2,0,3,0,0,0,0,0,0,1,1,0,0,2,2,0,0,0,0,2,0,1,0,0,0,1,0,2,0,0,0,0,0,0,5,2,0,0,3,2,0,1,0,0,1,0,1,0,2,0,0,0,1,0,2,0,2,0,0,0,0,2,0,1,0,0,0,0,0,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,2,1,0,2,0,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,1,0,0,1,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,2,0,0,1,0,0,0,2,0,0,0,0,9,1,1,0,0,0,0,0,1,0,0,2,2,0,0,0,0,0,2,1,0,0,0,9,0,0,1,0,0,0,0,0,1,0,0,0,0,2,0,0,0,0,0,2,0,0,0,1,0,1,2,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1],[&#34;330911&#34;,&#34;363272&#34;,&#34;240276&#34;,&#34;315154&#34;,&#34;3101298&#34;,&#34;7538&#34;,&#34;330972&#34;,&#34;248738&#34;,&#34;2657&#34;,&#34;A/4 48871&#34;,&#34;349220&#34;,&#34;694&#34;,&#34;21228&#34;,&#34;24065&#34;,&#34;W.E.P. 5734&#34;,&#34;SC/PARIS 2167&#34;,&#34;233734&#34;,&#34;2692&#34;,&#34;STON/O2. 3101270&#34;,&#34;2696&#34;,&#34;PC 17603&#34;,&#34;C 17368&#34;,&#34;PC 17598&#34;,&#34;PC 17597&#34;,&#34;PC 17608&#34;,&#34;A/5. 3337&#34;,&#34;113509&#34;,&#34;2698&#34;,&#34;113054&#34;,&#34;2662&#34;,&#34;SC/AH 3085&#34;,&#34;C.A. 31029&#34;,&#34;C.A. 2315&#34;,&#34;W./C. 6607&#34;,&#34;13236&#34;,&#34;2682&#34;,&#34;342712&#34;,&#34;315087&#34;,&#34;345768&#34;,&#34;1601&#34;,&#34;349256&#34;,&#34;113778&#34;,&#34;SOTON/O.Q. 3101263&#34;,&#34;237249&#34;,&#34;11753&#34;,&#34;STON/O 2. 3101291&#34;,&#34;PC 17594&#34;,&#34;370374&#34;,&#34;11813&#34;,&#34;C.A. 37671&#34;,&#34;13695&#34;,&#34;SC/PARIS 2168&#34;,&#34;29105&#34;,&#34;19950&#34;,&#34;SC/A.3 2861&#34;,&#34;382652&#34;,&#34;349230&#34;,&#34;348122&#34;,&#34;386525&#34;,&#34;PC 17608&#34;,&#34;349232&#34;,&#34;237216&#34;,&#34;347090&#34;,&#34;334914&#34;,&#34;PC 17608&#34;,&#34;F.C.C. 13534&#34;,&#34;330963&#34;,&#34;113796&#34;,&#34;2543&#34;,&#34;19950&#34;,&#34;382653&#34;,&#34;349211&#34;,&#34;3101297&#34;,&#34;PC 17562&#34;,&#34;113503&#34;,&#34;113503&#34;,&#34;359306&#34;,&#34;11770&#34;,&#34;248744&#34;,&#34;368702&#34;,&#34;2678&#34;,&#34;PC 17483&#34;,&#34;19924&#34;,&#34;349238&#34;,&#34;240261&#34;,&#34;2660&#34;,&#34;330844&#34;,&#34;A/4 31416&#34;,&#34;364856&#34;,&#34;29103&#34;,&#34;347072&#34;,&#34;345498&#34;,&#34;F.C. 12750&#34;,&#34;376563&#34;,&#34;13905&#34;,&#34;350033&#34;,&#34;19877&#34;,&#34;STON/O 2. 3101268&#34;,&#34;347471&#34;,&#34;A./5. 3338&#34;,&#34;11778&#34;,&#34;228414&#34;,&#34;365235&#34;,&#34;347070&#34;,&#34;2625&#34;,&#34;C 4001&#34;,&#34;330920&#34;,&#34;383162&#34;,&#34;3410&#34;,&#34;248734&#34;,&#34;237734&#34;,&#34;330968&#34;,&#34;PC 17531&#34;,&#34;329944&#34;,&#34;PC 17483&#34;,&#34;2680&#34;,&#34;2681&#34;,&#34;PP 9549&#34;,&#34;13050&#34;,&#34;SC/AH 29037&#34;,&#34;C.A. 33595&#34;,&#34;367227&#34;,&#34;13236&#34;,&#34;392095&#34;,&#34;368783&#34;,&#34;371362&#34;,&#34;350045&#34;,&#34;367226&#34;,&#34;211535&#34;,&#34;342441&#34;,&#34;STON/OQ. 369943&#34;,&#34;113780&#34;,&#34;4133&#34;,&#34;2621&#34;,&#34;349226&#34;,&#34;350409&#34;,&#34;2656&#34;,&#34;248659&#34;,&#34;SOTON/OQ 392083&#34;,&#34;CA 2144&#34;,&#34;CA 2144&#34;,&#34;113781&#34;,&#34;PC 17608&#34;,&#34;244358&#34;,&#34;17475&#34;,&#34;345763&#34;,&#34;17463&#34;,&#34;SC/A4 23568&#34;,&#34;113791&#34;,&#34;250651&#34;,&#34;11767&#34;,&#34;349255&#34;,&#34;3701&#34;,&#34;350405&#34;,&#34;347077&#34;,&#34;S.O./P.P. 752&#34;,&#34;PC 17483&#34;,&#34;347469&#34;,&#34;110489&#34;,&#34;SOTON/O.Q. 3101315&#34;,&#34;335432&#34;,&#34;2650&#34;,&#34;220844&#34;,&#34;343271&#34;,&#34;237393&#34;,&#34;315153&#34;,&#34;PC 17591&#34;,&#34;W./C. 6608&#34;,&#34;17770&#34;,&#34;7548&#34;,&#34;S.O./P.P. 251&#34;,&#34;2670&#34;,&#34;347072&#34;,&#34;2673&#34;,&#34;347077&#34;,&#34;29750&#34;,&#34;C.A. 33112&#34;,&#34;11778&#34;,&#34;230136&#34;,&#34;PC 17756&#34;,&#34;233478&#34;,&#34;PC 17756&#34;,&#34;113773&#34;,&#34;7935&#34;,&#34;PC 17558&#34;,&#34;239059&#34;,&#34;S.O./P.P. 2&#34;,&#34;A/4 48873&#34;,&#34;CA. 2343&#34;,&#34;28221&#34;,&#34;226875&#34;,&#34;111163&#34;,&#34;A/5. 851&#34;,&#34;235509&#34;,&#34;28220&#34;,&#34;347465&#34;,&#34;16966&#34;,&#34;347066&#34;,&#34;C.A. 31030&#34;,&#34;65305&#34;,&#34;36568&#34;,&#34;347080&#34;,&#34;PC 17757&#34;,&#34;26360&#34;,&#34;C.A. 34050&#34;,&#34;F.C. 12998&#34;,&#34;9232&#34;,&#34;28034&#34;,&#34;PC 17613&#34;,&#34;349250&#34;,&#34;C 4001&#34;,&#34;SOTON/O.Q. 3101308&#34;,&#34;S.O.C. 14879&#34;,&#34;24065&#34;,&#34;347091&#34;,&#34;113038&#34;,&#34;330924&#34;,&#34;36928&#34;,&#34;113503&#34;,&#34;32302&#34;,&#34;SC/PARIS 2148&#34;,&#34;342684&#34;,&#34;W./C. 14266&#34;,&#34;350053&#34;,&#34;PC 17606&#34;,&#34;2661&#34;,&#34;350054&#34;,&#34;370368&#34;,&#34;C.A. 6212&#34;,&#34;242963&#34;,&#34;220845&#34;,&#34;113795&#34;,&#34;3101266&#34;,&#34;330971&#34;,&#34;PC 17599&#34;,&#34;350416&#34;,&#34;110813&#34;,&#34;2679&#34;,&#34;250650&#34;,&#34;PC 17761&#34;,&#34;112377&#34;,&#34;237789&#34;,&#34;16966&#34;,&#34;3470&#34;,&#34;W./C. 6607&#34;,&#34;17464&#34;,&#34;F.C.C. 13534&#34;,&#34;28220&#34;,&#34;26707&#34;,&#34;2660&#34;,&#34;C.A. 34651&#34;,&#34;SOTON/O2 3101284&#34;,&#34;13508&#34;,&#34;7266&#34;,&#34;345775&#34;,&#34;C.A. 42795&#34;,&#34;AQ/4 3130&#34;,&#34;363611&#34;,&#34;28404&#34;,&#34;345501&#34;,&#34;345572&#34;,&#34;350410&#34;,&#34;29103&#34;,&#34;350405&#34;,&#34;C.A. 34644&#34;,&#34;349235&#34;,&#34;112051&#34;,&#34;C.A. 49867&#34;,&#34;A. 2. 39186&#34;,&#34;315095&#34;,&#34;13050&#34;,&#34;368573&#34;,&#34;13508&#34;,&#34;370371&#34;,&#34;2676&#34;,&#34;236853&#34;,&#34;SC 14888&#34;,&#34;2926&#34;,&#34;CA 31352&#34;,&#34;W./C. 14260&#34;,&#34;315085&#34;,&#34;SOTON/O.Q. 3101315&#34;,&#34;364859&#34;,&#34;2650&#34;,&#34;370129&#34;,&#34;A/5 21175&#34;,&#34;SOTON/O.Q. 3101314&#34;,&#34;21228&#34;,&#34;2655&#34;,&#34;A/5 1478&#34;,&#34;PC 17607&#34;,&#34;382650&#34;,&#34;2652&#34;,&#34;33638&#34;,&#34;345771&#34;,&#34;349202&#34;,&#34;SC/Paris 2123&#34;,&#34;2662&#34;,&#34;113801&#34;,&#34;347467&#34;,&#34;347079&#34;,&#34;237735&#34;,&#34;S.O./P.P. 2&#34;,&#34;315092&#34;,&#34;383123&#34;,&#34;112901&#34;,&#34;113781&#34;,&#34;392091&#34;,&#34;12749&#34;,&#34;350026&#34;,&#34;315091&#34;,&#34;2658&#34;,&#34;LP 1588&#34;,&#34;368364&#34;,&#34;PC 17760&#34;,&#34;AQ/3. 30631&#34;,&#34;PC 17569&#34;,&#34;28004&#34;,&#34;350408&#34;,&#34;C.A. 31029&#34;,&#34;347075&#34;,&#34;2654&#34;,&#34;244368&#34;,&#34;113790&#34;,&#34;24160&#34;,&#34;SOTON/O.Q. 3101309&#34;,&#34;230136&#34;,&#34;PC 17585&#34;,&#34;2003&#34;,&#34;236854&#34;,&#34;C.A. 33112&#34;,&#34;PC 17580&#34;,&#34;2684&#34;,&#34;2653&#34;,&#34;349229&#34;,&#34;110469&#34;,&#34;244360&#34;,&#34;2675&#34;,&#34;C.A. 31029&#34;,&#34;2622&#34;,&#34;C.A. 15185&#34;,&#34;350403&#34;,&#34;CA. 2343&#34;,&#34;PC 17755&#34;,&#34;A/5. 851&#34;,&#34;348125&#34;,&#34;237670&#34;,&#34;2688&#34;,&#34;248726&#34;,&#34;F.C.C. 13528&#34;,&#34;PC 17759&#34;,&#34;F.C.C. 13540&#34;,&#34;S.O.C. 14879&#34;,&#34;220845&#34;,&#34;C.A. 2315&#34;,&#34;113044&#34;,&#34;11769&#34;,&#34;1222&#34;,&#34;368402&#34;,&#34;349910&#34;,&#34;CA. 2343&#34;,&#34;S.C./PARIS 2079&#34;,&#34;CA 31352&#34;,&#34;315083&#34;,&#34;11765&#34;,&#34;CA. 2343&#34;,&#34;2689&#34;,&#34;3101295&#34;,&#34;112378&#34;,&#34;SC/PARIS 2147&#34;,&#34;28133&#34;,&#34;16966&#34;,&#34;112058&#34;,&#34;248746&#34;,&#34;33638&#34;,&#34;PC 17608&#34;,&#34;315152&#34;,&#34;29107&#34;,&#34;680&#34;,&#34;347077&#34;,&#34;366713&#34;,&#34;330910&#34;,&#34;364498&#34;,&#34;376566&#34;,&#34;SC/PARIS 2159&#34;,&#34;220845&#34;,&#34;349911&#34;,&#34;244346&#34;,&#34;364858&#34;,&#34;349909&#34;,&#34;12749&#34;,&#34;PC 17592&#34;,&#34;C.A. 2673&#34;,&#34;C.A. 30769&#34;,&#34;315153&#34;,&#34;13695&#34;,&#34;371109&#34;,&#34;13567&#34;,&#34;347065&#34;,&#34;21332&#34;,&#34;36928&#34;,&#34;28664&#34;,&#34;112378&#34;,&#34;113059&#34;,&#34;17765&#34;,&#34;SC/PARIS 2166&#34;,&#34;28666&#34;,&#34;113503&#34;,&#34;334915&#34;,&#34;SOTON/O.Q. 3101315&#34;,&#34;365237&#34;,&#34;19928&#34;,&#34;347086&#34;,&#34;A.5. 3236&#34;,&#34;PC 17758&#34;,&#34;SOTON/O.Q. 3101262&#34;,&#34;359309&#34;,&#34;2668&#34;]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;PassengerId&lt;\/th&gt;\n      &lt;th&gt;Pclass&lt;\/th&gt;\n      &lt;th&gt;Name&lt;\/th&gt;\n      &lt;th&gt;Sex&lt;\/th&gt;\n      &lt;th&gt;Age&lt;\/th&gt;\n      &lt;th&gt;SibSp&lt;\/th&gt;\n      &lt;th&gt;Parch&lt;\/th&gt;\n      &lt;th&gt;Ticket&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,5,6,7]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;&lt;strong&gt;From above it is vividly true that training set has 891 entries white test set data has 418 entries.&lt;/strong&gt;
&lt;strong&gt;Next is a code snippet checking the summary of both train and test data given from &lt;code&gt;kaggle&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId       Survived          Pclass          Name          
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##      Sex                 Age            SibSp           Parch       
##  Length:891         Min.   : 0.42   Min.   :0.000   Min.   :0.0000  
##  Class :character   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  
##  Mode  :character   Median :28.00   Median :0.000   Median :0.0000  
##                     Mean   :29.70   Mean   :0.523   Mean   :0.3816  
##                     3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  
##                     Max.   :80.00   Max.   :8.000   Max.   :6.0000  
##                     NA&amp;#39;s   :177                                     
##     Ticket               Fare           Cabin             Embarked        
##  Length:891         Min.   :  0.00   Length:891         Length:891        
##  Class :character   1st Qu.:  7.91   Class :character   Class :character  
##  Mode  :character   Median : 14.45   Mode  :character   Mode  :character  
##                     Mean   : 32.20                                        
##                     3rd Qu.: 31.00                                        
##                     Max.   :512.33                                        
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above summary it is evident that training set data average age that was in titanic was 30 while the average fare that was being paid was 32 U.S Dollar.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId         Pclass          Name               Sex           
##  Min.   : 892.0   Min.   :1.000   Length:418         Length:418        
##  1st Qu.: 996.2   1st Qu.:1.000   Class :character   Class :character  
##  Median :1100.5   Median :3.000   Mode  :character   Mode  :character  
##  Mean   :1100.5   Mean   :2.266                                        
##  3rd Qu.:1204.8   3rd Qu.:3.000                                        
##  Max.   :1309.0   Max.   :3.000                                        
##                                                                        
##       Age            SibSp            Parch           Ticket         
##  Min.   : 0.17   Min.   :0.0000   Min.   :0.0000   Length:418        
##  1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.0000   Class :character  
##  Median :27.00   Median :0.0000   Median :0.0000   Mode  :character  
##  Mean   :30.27   Mean   :0.4474   Mean   :0.3923                     
##  3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.0000                     
##  Max.   :76.00   Max.   :8.0000   Max.   :9.0000                     
##  NA&amp;#39;s   :86                                                          
##       Fare            Cabin             Embarked        
##  Min.   :  0.000   Length:418         Length:418        
##  1st Qu.:  7.896   Class :character   Class :character  
##  Median : 14.454   Mode  :character   Mode  :character  
##  Mean   : 35.627                                        
##  3rd Qu.: 31.500                                        
##  Max.   :512.329                                        
##  NA&amp;#39;s   :1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above summary it is evident that test set data average age that was in titanic was 30 while the average fare that was being paid was 36 U.S Dollar.&lt;/strong&gt;
&lt;strong&gt;Next I’ll create a table that I’ll use in presenting my result in the Kaggle competion with only two columns &lt;code&gt;Survived&lt;/code&gt; and &lt;code&gt;PassengersId&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_test$Survived &amp;lt;- 0
df_new &amp;lt;- data.frame(PassengerId = df_test$PassengerId, Survived = df_test$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next is to see if the number of rows of the table created corresponds with the test data rows given from &lt;code&gt;Kaggle&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(df_new)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 418&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From the function &lt;code&gt;nrow&lt;/code&gt; 418 rows have been seen which corresponds with what the &lt;code&gt;Kaggle&lt;/code&gt; test set entries shows.&lt;/strong&gt;
&lt;strong&gt;Next I’ll store the table created above in a csv file using &lt;code&gt;write.csv&lt;/code&gt; function but it has only 0 entries in the Survived columns which I’ll fix in the coming code snippet&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(df_new, file = &amp;quot;Predictor.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The code shown below is to create a table where 1 represent female and 0 represent male and have two columns with &lt;code&gt;PassengerId&lt;/code&gt; column and &lt;code&gt;Survived&lt;/code&gt; column with &lt;code&gt;Survived&lt;/code&gt; column showing 1 for female and 0 for male survival rate for two genders.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_test$Survived [df_test$Sex == &amp;quot;female&amp;quot;] &amp;lt;- 1

df_new &amp;lt;- data.frame(PassengerId = df_test$PassengerId, Survived = df_test$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(df_new)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 418&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above &lt;code&gt;nrow&lt;/code&gt; function it evident that the number of rows corresponds with what is given in &lt;code&gt;Kaggle&lt;/code&gt; test set data.&lt;/strong&gt;
&lt;strong&gt;Next I’ll store the result the same as what &lt;code&gt;Kaggle&lt;/code&gt; competition expect the result to be presented in their titanic competition portal.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(df_new, file = &amp;quot;Gender_Model.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Now that I have presented my prediction to the &lt;code&gt;Kaggle&lt;/code&gt; competion it’s time to combine both data set for full analysis purposes using &lt;code&gt;rbind&lt;/code&gt; function shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat &amp;lt;- rbind(df_train,df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;I’ll quickly check the summary of my combined data set using &lt;code&gt;summary&lt;/code&gt; function shown below and remove some judgement from it.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df_concat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId      Survived          Pclass          Name          
##  Min.   :   1   Min.   :0.0000   Min.   :1.000   Length:1309       
##  1st Qu.: 328   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median : 655   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   : 655   Mean   :0.3774   Mean   :2.295                     
##  3rd Qu.: 982   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :1309   Max.   :1.0000   Max.   :3.000                     
##                                                                    
##      Sex                 Age            SibSp            Parch      
##  Length:1309        Min.   : 0.17   Min.   :0.0000   Min.   :0.000  
##  Class :character   1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000  
##  Mode  :character   Median :28.00   Median :0.0000   Median :0.000  
##                     Mean   :29.88   Mean   :0.4989   Mean   :0.385  
##                     3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000  
##                     Max.   :80.00   Max.   :8.0000   Max.   :9.000  
##                     NA&amp;#39;s   :263                                     
##     Ticket               Fare            Cabin             Embarked        
##  Length:1309        Min.   :  0.000   Length:1309        Length:1309       
##  Class :character   1st Qu.:  7.896   Class :character   Class :character  
##  Mode  :character   Median : 14.454   Mode  :character   Mode  :character  
##                     Mean   : 33.295                                        
##                     3rd Qu.: 31.275                                        
##                     Max.   :512.329                                        
##                     NA&amp;#39;s   :1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above it is like the fare did not change from above two data set and also fare.&lt;/strong&gt;
&lt;strong&gt;Next I’ll create a table to see the number of people who died and who survived.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(df_concat$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   0   1 
## 815 494&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From the table above it is like the number of people who died are more than who survived but the proportion is not yet realised I’ll used &lt;code&gt;prop.table&lt;/code&gt; function to find proportionality of the death and survival rate.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(df_concat$Survived))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##         0         1 
## 0.6226127 0.3773873&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above table is evident that 62% of the people in the titanic died while 38% survived.&lt;/strong&gt;
&lt;strong&gt;Next I’ll see the structure of the data using &lt;code&gt;str&lt;/code&gt; function as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df_concat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1309 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : num  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &amp;quot;Braund, Mr. Owen Harris&amp;quot; &amp;quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&amp;quot; &amp;quot;Heikkinen, Miss. Laina&amp;quot; &amp;quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&amp;quot; ...
##  $ Sex        : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &amp;quot;A/5 21171&amp;quot; &amp;quot;PC 17599&amp;quot; &amp;quot;STON/O2. 3101282&amp;quot; &amp;quot;113803&amp;quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  NA &amp;quot;C85&amp;quot; NA &amp;quot;C123&amp;quot; ...
##  $ Embarked   : chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above &lt;code&gt;Pclass&lt;/code&gt; column is treated as an &lt;code&gt;int&lt;/code&gt; which is not the case I’ll have to convert it into a factor of the three classes as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Pclass = as.factor(df_concat$Pclass)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll make some inference to see if it has changes using the same &lt;code&gt;str&lt;/code&gt; function as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df_concat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1309 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : num  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : Factor w/ 3 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;: 3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &amp;quot;Braund, Mr. Owen Harris&amp;quot; &amp;quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&amp;quot; &amp;quot;Heikkinen, Miss. Laina&amp;quot; &amp;quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&amp;quot; ...
##  $ Sex        : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &amp;quot;A/5 21171&amp;quot; &amp;quot;PC 17599&amp;quot; &amp;quot;STON/O2. 3101282&amp;quot; &amp;quot;113803&amp;quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  NA &amp;quot;C85&amp;quot; NA &amp;quot;C123&amp;quot; ...
##  $ Embarked   : chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Bingo!! &lt;code&gt;Pclass&lt;/code&gt; has changed to factor with three levels i.e first class, second class and third class.&lt;/strong&gt;
&lt;strong&gt;Next is to see the columns that have missing data and do some imputation to them using &lt;code&gt;sapply&lt;/code&gt; function.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(df_concat, function(df)
{
  sum(is.na(df)==T)/length(df)
})&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  PassengerId     Survived       Pclass         Name          Sex          Age 
## 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2009167303 
##        SibSp        Parch       Ticket         Fare        Cabin     Embarked 
## 0.0000000000 0.0000000000 0.0000000000 0.0007639419 0.7746371276 0.0015278839&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above function &lt;code&gt;Age&lt;/code&gt;, &lt;code&gt;Fare&lt;/code&gt;,&lt;code&gt;Cabin&lt;/code&gt; and &lt;code&gt;Embarked&lt;/code&gt; columns have missing values.I’ll also use &lt;code&gt;Amelia&lt;/code&gt; package from Amelia who is my favorite female follower in twitter to see how missing data is distributed.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Amelia&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ## 
## ## Amelia II: Multiple Imputation
## ## (Version 1.7.6, built: 2019-11-24)
## ## Copyright (C) 2005-2022 James Honaker, Gary King and Matthew Blackwell
## ## Refer to http://gking.harvard.edu/amelia/ for more information
## ##&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missmap(df_concat, main = &amp;quot;Missing Map&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above &lt;code&gt;Amelia&lt;/code&gt; package it shows that missing data composes of 8% of the total data set.&lt;/strong&gt;
&lt;strong&gt;Next is to impute missing data in &lt;code&gt;Age&lt;/code&gt; column using mean of the total age as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Age[is.na(df_concat$Age)] &amp;lt;- mean(df_concat$Age,na.rm=T)
sum(is.na(df_concat$Age))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll create a table to see the number of people in three towns and the missing values also in &lt;code&gt;Embarked&lt;/code&gt; column. &lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(df_concat$Embarked, useNA = &amp;quot;always&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    C    Q    S &amp;lt;NA&amp;gt; 
##  270  123  914    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Fom above table it seems like most of the people who boarded the titanic were from &lt;code&gt;Southampton&lt;/code&gt; with 914 people with only 2 missing values wich I’ll use the &lt;code&gt;Southampton&lt;/code&gt; to impute them as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Embarked[is.na(df_concat$Embarked)] &amp;lt;- &amp;#39;S&amp;#39;
sum(is.na(df_concat$Embarked))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(df_concat$Embarked, useNA = &amp;quot;always&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    C    Q    S &amp;lt;NA&amp;gt; 
##  270  123  916    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next is to remove missing value in &lt;code&gt;Fare&lt;/code&gt; column using the mean of the &lt;code&gt;Fare&lt;/code&gt; as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Fare[is.na(df_concat$Fare)] &amp;lt;- mean(df_concat$Fare,na.rm=T)
sum(is.na(df_concat$Fare))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll remove &lt;code&gt;Cabin&lt;/code&gt; column from the dataset since it has no impact as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat &amp;lt;- df_concat[-11]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Let me countercheck if the data is now clean and has no missing value using below code snippet.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(df_concat, function(df)
{
  sum(is.na(df)==T)/length(df)
})&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PassengerId    Survived      Pclass        Name         Sex         Age 
##           0           0           0           0           0           0 
##       SibSp       Parch      Ticket        Fare    Embarked 
##           0           0           0           0           0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Let me countercheck if the data is now clean and has no missing value by visual representation using &lt;code&gt;Amelia&lt;/code&gt; package.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Amelia&amp;quot;)
missmap(df_concat, main = &amp;quot;Missing Map&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bingo!! now all the missing values have been cleaned now I’ll split my data into training and test set then use them in visual analysis of the two data set before actual machine learning is done on them as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df.train.cleaned &amp;lt;- df_concat[1:891,]
df.test.cleaned &amp;lt;- df_concat[892:1309,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll call &lt;code&gt;ggplot2&lt;/code&gt; package which plays a crucial part in data visualization as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll be using both trainig and test set in visualization I’ll begin by creating a table of people who survived and died in my traing set as below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~Survived, df.train.cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Survived
##   0   1 
## 549 342&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From the table above 549 people died and 342 survived in the training set.&lt;/strong&gt;
&lt;strong&gt;I’ll represent the above table using bar plot as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Survived))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll visualize &lt;code&gt;Sex&lt;/code&gt; column in the traing set to see the gender that were many in the titanic as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Sex))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above table it is evident that more males were in the titanic compared to females.&lt;/strong&gt;
&lt;strong&gt;Next I’ll draw a bar plot to see how people were distributed in three classes using the below code.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Pclass))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above table there were more people in the third class compared to other classes while second class being least.&lt;/strong&gt;
&lt;strong&gt;Next I’ll use a histogram to show if &lt;code&gt;Fare&lt;/code&gt; is normaally distributed and also see its skewness.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_histogram(aes(x=Fare),fill = &amp;quot;white&amp;quot;, colour = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From the table above it is evident that &lt;code&gt;Fare&lt;/code&gt; is right skewed hence the price is not normal distributed.&lt;/strong&gt;
&lt;strong&gt;Next is to draw a boxplot to see how price is distributed and also capture the outliers.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_boxplot(aes(x=factor(0),y=Fare)) + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above boxplot it can be seen that &lt;code&gt;Fare&lt;/code&gt; is right skewed with even a person paying more than 500.&lt;/strong&gt;
&lt;strong&gt;Next I’ll draw a histogram to show how &lt;code&gt;Age&lt;/code&gt; is distributed.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_histogram(aes(x=Age),fill = &amp;quot;white&amp;quot;, colour = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above it is evident that &lt;code&gt;Age&lt;/code&gt; is normally distributed with a mean of roughly 30.&lt;/strong&gt;
&lt;strong&gt;Next I’ll draw a boxplot for further inference on the &lt;code&gt;Age&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_boxplot(aes(x=factor(0),y=Age)) + coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above boxplot it can be seen that &lt;code&gt;Age&lt;/code&gt; is normally distributed.&lt;/strong&gt;
&lt;strong&gt;Next I’ll make a table to show how survival count in the &lt;code&gt;Sex&lt;/code&gt; is distributed.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~Survived+Sex,df.train.cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Sex
## Survived female male
##        0     81  468
##        1    233  109&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above table it is evident that more males died compared to females while more females survived compared to males.&lt;/strong&gt;
&lt;strong&gt;Next I’ll draw a bar plot to visualize above table.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Sex, fill=factor(Survived)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll make a table to show the survival rate in the three classes as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~Survived+Pclass,df.train.cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Pclass
## Survived   1   2   3
##        0  80  97 372
##        1 136  87 119&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above table more people died in the third class as compared to rest of classes while people who were in the first class survived to compared to the rest of the classes.&lt;/strong&gt;
&lt;strong&gt;Next I’ll visualize above table using bar plot for visual inference.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Pclass, fill=factor(Survived)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll make a table to see survival rate from three stations as below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~Survived+Embarked,df.train.cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Embarked
## Survived   C   Q   S
##        0  75  47 427
##        1  93  30 219&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From the table above it is evident that more people from &lt;code&gt;Southampton&lt;/code&gt; boarded the titanic and also it is them that died in large number than the rest and also it is them that died most while least death and survival registered by &lt;code&gt;Queenstown&lt;/code&gt;.&lt;/strong&gt;
&lt;strong&gt;I’ll visualize the above table using a bar plot for further visual inference.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Embarked, fill=factor(Survived)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll draw a boxplot to show how suvival rate for &lt;code&gt;Age&lt;/code&gt; column .&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_boxplot(aes(x = factor(Survived), y = Age))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above box plot it is evident that a maximum &lt;code&gt;Age&lt;/code&gt; of about 40 survived and died while the least &lt;code&gt;Age&lt;/code&gt; being 24 for for those who died while 22 being who survived.&lt;/strong&gt;
&lt;strong&gt;Next I’ll visualize above scenario using a histogram as below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_histogram(aes(x = Age),fill = &amp;quot;white&amp;quot;, colour = &amp;quot;black&amp;quot;) + facet_grid(factor(Survived) ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll draw a boxplot of &lt;code&gt;Fare&lt;/code&gt; against &lt;code&gt;Survived&lt;/code&gt; to see if &lt;code&gt;Fare&lt;/code&gt; played a major role in survival rate.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_boxplot(aes(x = factor(Survived), y = Fare))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above table it is evident that those who paid high &lt;code&gt;Fare&lt;/code&gt; survived as compared to those who paid least.&lt;/strong&gt;
&lt;strong&gt;Next I’ll draw a histogram from above boxplot for further visual representation.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_histogram(aes(x = Fare),fill = &amp;quot;white&amp;quot;, colour = &amp;quot;black&amp;quot;) + facet_grid(factor(Survived) ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll draw a table of both males and females to show their survival count in three classes as shown below.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~factor(Survived)+Pclass+Sex,df.train.cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , Sex = female
## 
##                 Pclass
## factor(Survived)   1   2   3
##                0   3   6  72
##                1  91  70  72
## 
## , , Sex = male
## 
##                 Pclass
## factor(Survived)   1   2   3
##                0  77  91 300
##                1  45  17  47&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above table it is evident that more males died compared to females while more females survived compared to males.&lt;/strong&gt;
&lt;strong&gt;Next I’ll draw a bar plot using &lt;code&gt;facet_grid&lt;/code&gt; option to visualise above table.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Sex, fill=factor(Survived))) + facet_grid(Pclass ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll draw a table of males and females differently in three classes to compare survival rates in three classes of the two genders.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~Survived+Embarked+Sex,df.train.cleaned)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , Sex = female
## 
##         Embarked
## Survived   C   Q   S
##        0   9   9  63
##        1  64  27 142
## 
## , , Sex = male
## 
##         Embarked
## Survived   C   Q   S
##        0  66  38 364
##        1  29   3  77&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above table it is evident that more males died in &lt;code&gt;Cherbourg&lt;/code&gt; and &lt;code&gt;Southamptton&lt;/code&gt; as comparedto males with only &lt;code&gt;Queenstown&lt;/code&gt; females died more than males, while survival rate also for females in two stations was high compared to males only &lt;code&gt;Queenstown&lt;/code&gt; was vicevasa.&lt;/strong&gt;
&lt;strong&gt;I’ll represent the above table using bar plot using &lt;code&gt;facet_grid&lt;/code&gt; option for visualisation purposes.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.train.cleaned) + geom_bar(aes(x=Sex, fill=factor(Survived))) + facet_grid(Embarked ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-50-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll create a child column giving an age an range of 18 a child and rest adult after that I’ll draw a bar plot to represent survival rate for a child.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Child &amp;lt;- NA
df_concat$Child[df_concat$Age &amp;lt; 18] &amp;lt;- 1
df_concat$Child[df_concat$Age &amp;gt;= 18] &amp;lt;- 0
str(df_concat$Child)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:1309] 0 0 0 0 0 0 0 1 0 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_concat) + geom_bar(aes(x=Child))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-51-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From above table it evident that more childrens died than those who survived.&lt;/strong&gt;
&lt;strong&gt;Next is to make some inferetial analysis from &lt;code&gt;Name&lt;/code&gt; column by using regex expression which will help get initials from the name and see people’s initials survival rate both visually ad using table.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Title &amp;lt;- sapply(df_concat$Name, FUN=function(x) {strsplit(x, split=&amp;#39;[,.]&amp;#39;)[[1]][2]})
df_concat$Title &amp;lt;- sub(&amp;#39; &amp;#39;, &amp;#39;&amp;#39;,df_concat$Title)  # Remove the white space or blank
table(df_concat$Title)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##         Capt          Col          Don         Dona           Dr     Jonkheer 
##            1            4            1            1            8            1 
##         Lady        Major       Master         Miss         Mlle          Mme 
##            1            2           61          260            2            1 
##           Mr          Mrs           Ms          Rev          Sir the Countess 
##          757          197            2            8            1            1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above table it is like most people who were in the titanic were Mr.&lt;/strong&gt;
&lt;strong&gt;Next I’ll represent above table using bar plot.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_concat) + geom_bar(aes(x=Title))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-53-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll reduce the above initails into manageable size using below snippet of code.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$Title[df_concat$Title %in% c(&amp;#39;Mme&amp;#39;, &amp;#39;Mlle&amp;#39;)] &amp;lt;- &amp;#39;Mlle&amp;#39;
df_concat$Title[df_concat$Title %in% c(&amp;#39;Capt&amp;#39;, &amp;#39;Don&amp;#39;, &amp;#39;Major&amp;#39;, &amp;#39;Sir&amp;#39;)] &amp;lt;- &amp;#39;Sir&amp;#39;
df_concat$Title[df_concat$Title %in% c(&amp;#39;Dona&amp;#39;, &amp;#39;Lady&amp;#39;, &amp;#39;the Countess&amp;#39;, &amp;#39;Jonkheer&amp;#39;)] &amp;lt;- &amp;#39;Lady&amp;#39;
df_concat$Title &amp;lt;- factor(df_concat$Title)
table(df_concat$Title)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    Col     Dr   Lady Master   Miss   Mlle     Mr    Mrs     Ms    Rev    Sir 
##      4      8      4     61    260      3    757    197      2      8      5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;After reducing the initials I’ll draw the same bar plot to see the difference.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_concat) + geom_bar(aes(x=Title))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next I’ll create aa table to make some inference on how the family size were distributed in the ship.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_concat$FamilySize &amp;lt;- df_concat$SibSp + df_concat$Parch + 1
table(df_concat$FamilySize)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   1   2   3   4   5   6   7   8  11 
## 790 235 159  43  22  25  16   8  11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;From above it’s like a &lt;code&gt;Familysize&lt;/code&gt; of 1 person were many than others.Lastly I’ll draw a bar plot to visualize the result.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_concat) + geom_bar(aes(x=FamilySize))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2feature-engineering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2:Feature Engineering&lt;/h2&gt;
&lt;p&gt;Before doing machine learning processes data need to be cleaned in order to get good models with an accuracy that is to the point.
Here I’ll start with converting the data into factors and normalizing numerical columns as shown below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#reading data
df &amp;lt;- read.csv(&amp;quot;titan.csv&amp;quot;)
#removing the unwanted column x
df &amp;lt;- df[,-c(1)]
df_clean &amp;lt;- df[,-c(1,4,9)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I confirm the structure of my dataframe the do column conversion as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1309 obs. of  11 variables:
##  $ Survived  : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass    : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex       : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age       : num  22 38 26 35 35 ...
##  $ SibSp     : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch     : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Fare      : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Embarked  : chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...
##  $ Child     : int  0 0 0 0 0 0 0 1 0 1 ...
##  $ Title     : chr  &amp;quot;Mr&amp;quot; &amp;quot;Mrs&amp;quot; &amp;quot;Miss&amp;quot; &amp;quot;Mrs&amp;quot; ...
##  $ FamilySize: int  2 2 1 2 1 1 1 5 3 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are many columns that need to be converted into factor, where below snippet of code will do.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data transformation
to.factor &amp;lt;- function(df, variables){
  for (variable in variables){
    df[[variable]] &amp;lt;- as.factor(df[[variable]])
  }
  return(df)
}

cate_variable &amp;lt;- c(&amp;quot;Pclass&amp;quot;,&amp;quot;Sex&amp;quot;,  &amp;quot;Embarked&amp;quot;, &amp;quot;Title&amp;quot;, &amp;quot;Survived&amp;quot;)
df_clean &amp;lt;- to.factor(df = df_clean, variables = cate_variable)
str(df_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1309 obs. of  11 variables:
##  $ Survived  : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 1 2 2 2 1 1 1 1 2 2 ...
##  $ Pclass    : Factor w/ 3 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;: 3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex       : Factor w/ 2 levels &amp;quot;female&amp;quot;,&amp;quot;male&amp;quot;: 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age       : num  22 38 26 35 35 ...
##  $ SibSp     : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch     : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Fare      : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Embarked  : Factor w/ 3 levels &amp;quot;C&amp;quot;,&amp;quot;Q&amp;quot;,&amp;quot;S&amp;quot;: 3 1 3 3 3 2 3 3 3 1 ...
##  $ Child     : int  0 0 0 0 0 0 0 1 0 1 ...
##  $ Title     : Factor w/ 11 levels &amp;quot;Col&amp;quot;,&amp;quot;Dr&amp;quot;,&amp;quot;Lady&amp;quot;,..: 7 8 5 8 7 7 7 4 8 8 ...
##  $ FamilySize: int  2 2 1 2 1 1 1 5 3 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After converting the above columns into categorical factors I’ll need to normalize age column and fare with below snippet of code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#normalizing - scaling
normalize &amp;lt;- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
df_clean2 &amp;lt;- as.data.frame(lapply(df_clean[,c(4,7)], normalize))
titan_clean &amp;lt;- as.data.frame(cbind(df_clean[-c(4,7)], df_clean2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After I have finished feature engineering I’ll split the data into training and test set using caret package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;caret&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     lift&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split_index &amp;lt;- createDataPartition(titan_clean$Survived, p=0.8, list = FALSE)
testing_set &amp;lt;- titan_clean[-split_index,]
training_set &amp;lt;- titan_clean[split_index,]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-3machine-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 3:Machine Learning&lt;/h2&gt;
&lt;p&gt;In this section I’ll be using the saved models that I saved after training if you need code on how I came up with the model I have hosted the in my GitHub account in this &lt;a href=&#34;https://github.com/musanyaks/Titanic-Machine-Learning&#34;&gt;LINK&lt;/a&gt; for the source code.&lt;/p&gt;
&lt;p&gt;For the documentation and explanation of the models please refer my project for more details on the algorithms and explanation of terms in this link.&lt;/p&gt;
&lt;p&gt;I’ll load all the packages that I’ll use in my project as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#packages loading
library(rpart.plot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: rpart&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(caret)
library(e1071)
library(ROCR)
library(e1071)
library(dplyr)
library(gbm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loaded gbm 2.1.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3.0.0 Logistic Regression.&lt;/h1&gt;
&lt;p&gt;I had trained the logistic model and saved as an rds model and loaded here with &lt;strong&gt;readRDS&lt;/strong&gt; function.
The model had an &lt;strong&gt;Accuracy&lt;/strong&gt; of &lt;strong&gt;86%&lt;/strong&gt; and &lt;strong&gt;Cohen’s Kappa statistic(Kappa)&lt;/strong&gt; of &lt;strong&gt;70%&lt;/strong&gt; with a 10 fold cross-validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trained.log.model &amp;lt;- readRDS(&amp;quot;./logistic.model.rds&amp;quot;)
print(trained.log.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Generalized Linear Model 
## 
## 1048 samples
##   10 predictor
##    2 classes: &amp;#39;0&amp;#39;, &amp;#39;1&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 943, 943, 943, 943, 943, 943, ... 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.864533  0.7072219&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model had &lt;strong&gt;Aikake’s Information Criterion (AIC)&lt;/strong&gt; of &lt;strong&gt;778.2&lt;/strong&gt; which hepls in model complexity penalization.
Most of the terms are directly proportion to survival rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(trained.log.model$finalModel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  NULL
## 
## Coefficients:
## (Intercept)      Pclass2      Pclass3      Sexmale        SibSp        Parch  
##   34.234124    -1.110276    -2.050782   -33.092484     0.553404     0.552330  
##   EmbarkedQ    EmbarkedS        Child      TitleDr    TitleLady  TitleMaster  
##    0.066021    -0.245861     0.021165     0.607688   -15.635388     2.548495  
##   TitleMiss    TitleMlle      TitleMr     TitleMrs      TitleMs     TitleRev  
##  -29.114702   -16.227355    -0.006556   -28.434349   -14.344976   -14.322320  
##    TitleSir   FamilySize          Age         Fare  
##    0.882737    -1.001760    -1.740304     0.658579  
## 
## Degrees of Freedom: 1047 Total (i.e. Null);  1026 Residual
## Null Deviance:       1390 
## Residual Deviance: 734.2     AIC: 778.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model &lt;strong&gt;62%&lt;/strong&gt; of the largest class did not survive(&lt;strong&gt;No Information Rate&lt;/strong&gt;),&lt;strong&gt;Sensitivity&lt;/strong&gt; of &lt;strong&gt;95%&lt;/strong&gt; which is not bad and &lt;strong&gt;Specificity&lt;/strong&gt; of &lt;strong&gt;79%&lt;/strong&gt;,&lt;strong&gt;Pos Pred Value&lt;/strong&gt; of &lt;strong&gt;88%&lt;/strong&gt; which means 88% of the obsevation predicted that truly the population died in the titan,&lt;strong&gt;Neg Pred Value&lt;/strong&gt; of &lt;strong&gt;88%&lt;/strong&gt; this is rate of those who did not die as per the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(127)
predictions &amp;lt;- predict(trained.log.model, newdata = testing_set)
confusionMatrix(predictions, testing_set$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 152  22
##          1  11  76
##                                          
##                Accuracy : 0.8736         
##                  95% CI : (0.827, 0.9113)
##     No Information Rate : 0.6245         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2e-16        
##                                          
##                   Kappa : 0.7242         
##                                          
##  Mcnemar&amp;#39;s Test P-Value : 0.08172        
##                                          
##             Sensitivity : 0.9325         
##             Specificity : 0.7755         
##          Pos Pred Value : 0.8736         
##          Neg Pred Value : 0.8736         
##              Prevalence : 0.6245         
##          Detection Rate : 0.5824         
##    Detection Prevalence : 0.6667         
##       Balanced Accuracy : 0.8540         
##                                          
##        &amp;#39;Positive&amp;#39; Class : 0              
## &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;roc-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.0.1 ROC Curve&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;plot_utils.R&amp;quot;)
logistic.preds.values &amp;lt;- predict(trained.log.model, testing_set[,-1],
                                 type = &amp;quot;prob&amp;quot;)
logistic.predictions.values &amp;lt;- logistic.preds.values[,2]
predictions &amp;lt;- prediction(logistic.predictions.values,
                          testing_set$Survived)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text=&amp;quot;Logistic Regression ROC Curve&amp;quot;)
plot.pr.curve(predictions, title.text=&amp;quot;Logistic Regression Precision/Recall Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-67-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;Area Under Curve (AUC)&lt;/strong&gt; of &lt;strong&gt;0.91&lt;/strong&gt; truly the model will most of time predict randomly chosen positive class instance well than negative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.1.0 Random Forest.&lt;/h2&gt;
&lt;p&gt;The random forest confusion matrix had an error rate of &lt;strong&gt;14%&lt;/strong&gt;, I used &lt;strong&gt;1800&lt;/strong&gt; trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf.model &amp;lt;- readRDS(&amp;quot;./inal_model_rf.rds&amp;quot;)
print(rf.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
##  randomForest(formula = Survived ~ ., data = training_set, mtry = 1,      ntree = 1800) 
##                Type of random forest: classification
##                      Number of trees: 1800
## No. of variables tried at each split: 1
## 
##         OOB estimate of  error rate: 13.65%
## Confusion matrix:
##     0   1 class.error
## 0 595  57  0.08742331
## 1  86 310  0.21717172&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model &lt;strong&gt;62%&lt;/strong&gt; of the largest class did not survive(&lt;strong&gt;No Information Rate&lt;/strong&gt;),&lt;strong&gt;Sensitivity&lt;/strong&gt; of &lt;strong&gt;93%&lt;/strong&gt; which is not bad and &lt;strong&gt;Specificity&lt;/strong&gt; of &lt;strong&gt;78%&lt;/strong&gt;,&lt;strong&gt;Pos Pred Value&lt;/strong&gt; of &lt;strong&gt;87%&lt;/strong&gt; which means 88% of the obsevation predicted that truly the population died in the titan,&lt;strong&gt;Neg Pred Value&lt;/strong&gt; of &lt;strong&gt;86%&lt;/strong&gt; this is rate of those who did not die as per the model and an &lt;strong&gt;Accuracy&lt;/strong&gt; of &lt;strong&gt;87&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(rf.model, newdata = testing_set)
confusionMatrix(predictions, testing_set$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 152  20
##          1  11  78
##                                           
##                Accuracy : 0.8812          
##                  95% CI : (0.8357, 0.9179)
##     No Information Rate : 0.6245          
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16          
##                                           
##                   Kappa : 0.742           
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.1508          
##                                           
##             Sensitivity : 0.9325          
##             Specificity : 0.7959          
##          Pos Pred Value : 0.8837          
##          Neg Pred Value : 0.8764          
##              Prevalence : 0.6245          
##          Detection Rate : 0.5824          
##    Detection Prevalence : 0.6590          
##       Balanced Accuracy : 0.8642          
##                                           
##        &amp;#39;Positive&amp;#39; Class : 0               
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.1.1 ROC Curve.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf.preds.values &amp;lt;- predict(rf.model, testing_set[,-1],
                             type = &amp;quot;prob&amp;quot;)
rf.predictions.values &amp;lt;- rf.preds.values[,2]
predictions &amp;lt;- prediction(rf.predictions.values,
                          testing_set$Survived)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text=&amp;quot;Random Forest ROC Curve&amp;quot;)
plot.pr.curve(predictions, title.text=&amp;quot;Random Forest Precision/Recall Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-70-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;Area Under Curve (AUC)&lt;/strong&gt; of &lt;strong&gt;0.93&lt;/strong&gt; truly the model will most of time predict randomly chosen positive class instance well than negative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boosting-classification-modeling.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.2.0 Gradient boosting classification Modeling.&lt;/h2&gt;
&lt;p&gt;From the tuning process using &lt;strong&gt;caret&lt;/strong&gt; package I got a shrinkage of &lt;strong&gt;0.01&lt;/strong&gt;, number of trees &lt;strong&gt;300&lt;/strong&gt; and interaction depth of &lt;strong&gt;3&lt;/strong&gt;. For modelling purpose I used&lt;strong&gt;gbm&lt;/strong&gt; function with the parameters to get the model information(see code information in above link to my GitHub)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(127)
gbm.model &amp;lt;- readRDS(&amp;quot;./titan_gbm_train.rds&amp;quot;)
print(gbm.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Stochastic Gradient Boosting 
## 
## 1048 samples
##   10 predictor
##    2 classes: &amp;#39;0&amp;#39;, &amp;#39;1&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 943, 943, 943, 943, 943, 943, ... 
## Resampling results across tuning parameters:
## 
##   shrinkage  interaction.depth  n.trees  Accuracy   Kappa    
##   0.001      1                  100      0.6221451  0.0000000
##   0.001      1                  300      0.6221451  0.0000000
##   0.001      1                  500      0.8493287  0.6768831
##   0.001      2                  100      0.6221451  0.0000000
##   0.001      2                  300      0.7939614  0.5119273
##   0.001      2                  500      0.8608579  0.6965139
##   0.001      3                  100      0.6221451  0.0000000
##   0.001      3                  300      0.7939614  0.5119273
##   0.001      3                  500      0.8570392  0.6869535
##   0.001      4                  100      0.6221451  0.0000000
##   0.001      4                  300      0.7949229  0.5144569
##   0.001      4                  500      0.8474604  0.6641369
##   0.010      1                  100      0.8493287  0.6768831
##   0.010      1                  300      0.8493287  0.6768831
##   0.010      1                  500      0.8598507  0.6974685
##   0.010      2                  100      0.8656199  0.7076749
##   0.010      2                  300      0.8665540  0.7103976
##   0.010      2                  500      0.8675064  0.7136556
##   0.010      3                  100      0.8656199  0.7077374
##   0.010      3                  300      0.8713251  0.7212588
##   0.010      3                  500      0.8684498  0.7160126
##   0.010      4                  100      0.8665723  0.7100134
##   0.010      4                  300      0.8684679  0.7152505
##   0.010      4                  500      0.8684498  0.7161244
##   0.100      1                  100      0.8626897  0.7036403
##   0.100      1                  300      0.8617467  0.7023040
##   0.100      1                  500      0.8646038  0.7086194
##   0.100      2                  100      0.8636514  0.7066087
##   0.100      2                  300      0.8589166  0.6954070
##   0.100      2                  500      0.8617465  0.7014588
##   0.100      3                  100      0.8549623  0.6891051
##   0.100      3                  300      0.8560004  0.6902403
##   0.100      3                  500      0.8592155  0.6989324
##   0.100      4                  100      0.8549829  0.6877505
##   0.100      4                  300      0.8560309  0.6892170
##   0.100      4                  500      0.8496917  0.6762789
## 
## Tuning parameter &amp;#39;n.minobsinnode&amp;#39; was held constant at a value of 10
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 300, interaction.depth =
##  3, shrinkage = 0.01 and n.minobsinnode = 10.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model &lt;strong&gt;62%&lt;/strong&gt; of the largest class did not survive(&lt;strong&gt;No Information Rate&lt;/strong&gt;),&lt;strong&gt;Sensitivity&lt;/strong&gt; of &lt;strong&gt;93%&lt;/strong&gt; which is not bad and &lt;strong&gt;Specificity&lt;/strong&gt; of &lt;strong&gt;76%&lt;/strong&gt;,&lt;strong&gt;Pos Pred Value&lt;/strong&gt; of &lt;strong&gt;87%&lt;/strong&gt; which means 88% of the obsevation predicted that truly the population died in the titan,&lt;strong&gt;Neg Pred Value&lt;/strong&gt; of &lt;strong&gt;87%&lt;/strong&gt; this is rate of those who did not die as per the model and an &lt;strong&gt;Accuracy&lt;/strong&gt; of &lt;strong&gt;87&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(gbm.model, newdata = testing_set)
confusionMatrix(predictions, testing_set$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 154  24
##          1   9  74
##                                          
##                Accuracy : 0.8736         
##                  95% CI : (0.827, 0.9113)
##     No Information Rate : 0.6245         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2e-16        
##                                          
##                   Kappa : 0.7219         
##                                          
##  Mcnemar&amp;#39;s Test P-Value : 0.01481        
##                                          
##             Sensitivity : 0.9448         
##             Specificity : 0.7551         
##          Pos Pred Value : 0.8652         
##          Neg Pred Value : 0.8916         
##              Prevalence : 0.6245         
##          Detection Rate : 0.5900         
##    Detection Prevalence : 0.6820         
##       Balanced Accuracy : 0.8499         
##                                          
##        &amp;#39;Positive&amp;#39; Class : 0              
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.2.1 ROC Curve&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gbm.preds.values &amp;lt;- predict(gbm.model, testing_set[,-1],
                                 type = &amp;quot;prob&amp;quot;)
gbm.predictions.values &amp;lt;- gbm.preds.values[,2]
predictions &amp;lt;- prediction(gbm.predictions.values,
                          testing_set$Survived)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text=&amp;quot;GBBOOST  ROC Curve&amp;quot;)
plot.pr.curve(predictions, title.text=&amp;quot;GBBOOST  Precision/Recall Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-73-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;Area Under Curve (AUC)&lt;/strong&gt; of &lt;strong&gt;0.91&lt;/strong&gt; truly the model will most of time predict randomly chosen positive class instance well than negative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-and-regression-tree-cart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.3.0 Classification and Regression Tree (CART)&lt;/h2&gt;
&lt;p&gt;From the tuning process using &lt;strong&gt;caret&lt;/strong&gt; package I got &lt;strong&gt;cp&lt;/strong&gt; of &lt;strong&gt;0.01388889&lt;/strong&gt;. For modelling purpose I used &lt;strong&gt;rpart&lt;/strong&gt; function with the parameters to get the model information(see code information in above link to my GitHub)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cart.model &amp;lt;- readRDS(&amp;quot;./fit_cart.rds&amp;quot;)
print(cart.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CART 
## 
## 1048 samples
##   10 predictor
##    2 classes: &amp;#39;0&amp;#39;, &amp;#39;1&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 942, 944, 943, 943, 944, 943, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.01388889  0.8629034  0.7025245
##   0.01641414  0.8492493  0.6727766
##   0.61111111  0.7335351  0.3369915
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.01388889.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(127)
predictions &amp;lt;- predict(cart.model, newdata = testing_set)
confusionMatrix(predictions, testing_set$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 154  25
##          1   9  73
##                                           
##                Accuracy : 0.8697          
##                  95% CI : (0.8227, 0.9081)
##     No Information Rate : 0.6245          
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16          
##                                           
##                   Kappa : 0.7129          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.0101          
##                                           
##             Sensitivity : 0.9448          
##             Specificity : 0.7449          
##          Pos Pred Value : 0.8603          
##          Neg Pred Value : 0.8902          
##              Prevalence : 0.6245          
##          Detection Rate : 0.5900          
##    Detection Prevalence : 0.6858          
##       Balanced Accuracy : 0.8448          
##                                           
##        &amp;#39;Positive&amp;#39; Class : 0               
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model &lt;strong&gt;62%&lt;/strong&gt; of the largest class did not survive(&lt;strong&gt;No Information Rate&lt;/strong&gt;),&lt;strong&gt;Sensitivity&lt;/strong&gt; of &lt;strong&gt;94%&lt;/strong&gt; which is not bad and &lt;strong&gt;Specificity&lt;/strong&gt; of &lt;strong&gt;78%&lt;/strong&gt;,&lt;strong&gt;Pos Pred Value&lt;/strong&gt; of &lt;strong&gt;87%&lt;/strong&gt; which means 88% of the obsevation predicted that truly the population died in the titan,&lt;strong&gt;Neg Pred Value&lt;/strong&gt; of &lt;strong&gt;88%&lt;/strong&gt; this is rate of those who did not die as per the model and an &lt;strong&gt;Accuracy&lt;/strong&gt; of &lt;strong&gt;88&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve.-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.3.1 ROC Curve.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cart.preds.values &amp;lt;- predict(cart.model, testing_set[,-1],
                                 type = &amp;quot;prob&amp;quot;)
cart.predictions.values &amp;lt;- cart.preds.values[,2]
predictions &amp;lt;- prediction(cart.predictions.values,
                          testing_set$Survived)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text=&amp;quot;CART ROC Curve&amp;quot;)
plot.pr.curve(predictions, title.text=&amp;quot;CART Precision/Recall Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-76-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;Area Under Curve (AUC)&lt;/strong&gt; of &lt;strong&gt;0.87&lt;/strong&gt; truly the model will most of time predict randomly chosen positive class instance well than negative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.4.0 Neural Network.&lt;/h2&gt;
&lt;p&gt;For Neural Network parameters that are supposed to be tuned are size and decay which I got as &lt;strong&gt;1&lt;/strong&gt; and &lt;strong&gt;0.1&lt;/strong&gt; respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet.model &amp;lt;- readRDS(&amp;quot;./fit_nnet.rds&amp;quot;)
print(nnet.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Neural Network 
## 
## 1048 samples
##   10 predictor
##    2 classes: &amp;#39;0&amp;#39;, &amp;#39;1&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 944, 943, 944, 943, 943, 943, ... 
## Resampling results across tuning parameters:
## 
##   size  decay  Accuracy   Kappa    
##   1     0e+00  0.8350014  0.6228230
##   1     1e-04  0.8537273  0.6846832
##   1     1e-01  0.8686601  0.7150352
##   3     0e+00  0.8569045  0.6890369
##   3     1e-04  0.8483238  0.6699612
##   3     1e-01  0.8632752  0.7031294
##   5     0e+00  0.8419713  0.6570566
##   5     1e-04  0.8451310  0.6635986
##   5     1e-01  0.8553233  0.6856418
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were size = 1 and decay = 0.1.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(127)
predictions &amp;lt;- predict(nnet.model, newdata = testing_set)
confusionMatrix(predictions, testing_set$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 153  23
##          1  10  75
##                                          
##                Accuracy : 0.8736         
##                  95% CI : (0.827, 0.9113)
##     No Information Rate : 0.6245         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2e-16        
##                                          
##                   Kappa : 0.7231         
##                                          
##  Mcnemar&amp;#39;s Test P-Value : 0.03671        
##                                          
##             Sensitivity : 0.9387         
##             Specificity : 0.7653         
##          Pos Pred Value : 0.8693         
##          Neg Pred Value : 0.8824         
##              Prevalence : 0.6245         
##          Detection Rate : 0.5862         
##    Detection Prevalence : 0.6743         
##       Balanced Accuracy : 0.8520         
##                                          
##        &amp;#39;Positive&amp;#39; Class : 0              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the model &lt;strong&gt;62%&lt;/strong&gt; of the largest class did not survive(&lt;strong&gt;No Information Rate&lt;/strong&gt;),&lt;strong&gt;Sensitivity&lt;/strong&gt; of &lt;strong&gt;93%&lt;/strong&gt; which is not bad and &lt;strong&gt;Specificity&lt;/strong&gt; of &lt;strong&gt;77%&lt;/strong&gt;,&lt;strong&gt;Pos Pred Value&lt;/strong&gt; of &lt;strong&gt;87%&lt;/strong&gt; which means 88% of the obsevation predicted that truly the population died in the titan,&lt;strong&gt;Neg Pred Value&lt;/strong&gt; of &lt;strong&gt;86%&lt;/strong&gt; this is rate of those who did not die as per the model and an &lt;strong&gt;Accuracy&lt;/strong&gt; of &lt;strong&gt;87&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve.-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.4.1 Roc Curve.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet.preds.values &amp;lt;- predict(nnet.model, testing_set[,-1],
                            type = &amp;quot;prob&amp;quot;)
nnet.predictions.values &amp;lt;- nnet.preds.values[,2]
predictions &amp;lt;- prediction(nnet.predictions.values,
                          testing_set$Survived)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text=&amp;quot;Neural Network ROC Curve&amp;quot;)
plot.pr.curve(predictions, title.text=&amp;quot;Neural Network Precision/Recall Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-titanic-project-using-r_files/figure-html/unnamed-chunk-79-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;Area Under Curve (AUC)&lt;/strong&gt; of &lt;strong&gt;0.91&lt;/strong&gt; truly the model will most of time predict randomly chosen positive class instance well than negative.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Machine Learning from Disaster.</title>
  <link>/2020/07/27/machine-learning-from-disaster./</link>
  <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
  
<guid>/2020/07/27/machine-learning-from-disaster./</guid>
  <description>&lt;h2 id=&#34;1-business-understanding&#34;&gt;1. BUSINESS UNDERSTANDING.&lt;/h2&gt;
&lt;p&gt;Source:&lt;a href=&#34;https://www.kaggle.com/c/titanic/&#34;&gt;Machine Learning Titanic Competition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data: &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Titanic Data Set from Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;context&#34;&gt;Context:&lt;/h3&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://www.kaggle.com/c/titanic/discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;problem-statement-&#34;&gt;Problem statement :&lt;/h3&gt;
&lt;p&gt;Using machine learning to create a model that predicts which passengers survived the Titanic shipwreck.&lt;/p&gt;
&lt;h2 id=&#34;21-data-description-&#34;&gt;2.1 Data Description :&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Column Name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Key&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;survival&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Survival&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0&lt;/strong&gt; = No, &lt;strong&gt;1&lt;/strong&gt; = Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;pclass&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Ticket class&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;1&lt;/strong&gt; = 1st, &lt;strong&gt;2&lt;/strong&gt; = 2nd, &lt;strong&gt;3&lt;/strong&gt; = 3rd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;sex&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;C&lt;/strong&gt; = Cherbourg, &lt;strong&gt;Q&lt;/strong&gt; = Queenstown, &lt;strong&gt;S&lt;/strong&gt; = Southampton&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Age&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Age in years&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;sibsp&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of siblings / spouses aboard the Titanic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;parch&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Number of parents / children aboard the Titanic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;ticket&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Ticket number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;fare&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Passenger fare&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;cabin&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Cabin number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;embarked&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Port of Embarkation&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;211-importing-libraries&#34;&gt;2.1.1 Importing Libraries.&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll load the important libraries needed for this machine learning project task.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#loading libraries&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pearsonr
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LogisticRegression
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AdaBoostClassifier, BaggingClassifier, RandomForestClassifier
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SVC
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.tree &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; DecisionTreeClassifier
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (accuracy_score, auc, classification_report, 
                             confusion_matrix, fbeta_score,precision_score,recall_score,f1_score)
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; MinMaxScaler, StandardScaler
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GridSearchCV, train_test_split,KFold
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; warnings &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; filterwarnings
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; modeling_util &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; meu
filterwarnings(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ignore&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&amp;rsquo;ll get thed data set using pandas library by storing it as df and also get the data description using &lt;strong&gt;describe()&lt;/strong&gt; and &lt;strong&gt;info()&lt;/strong&gt; methods.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train.csv&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Getting the first four rows of the data set using &lt;strong&gt;.head()&lt;/strong&gt; method.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table1.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;212-statistical-summary&#34;&gt;2.1.2 Statistical Summary.&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;describe(include&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;number)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T &lt;span style=&#34;color:#75715e&#34;&gt;# transposing the data set description&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table2.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above the maximum Number of parents / children aboard the Titanic(&lt;strong&gt;Parch&lt;/strong&gt;) were &lt;strong&gt;6&lt;/strong&gt; while minimum being &lt;strong&gt;0&lt;/strong&gt;, while maximum &lt;strong&gt;Age&lt;/strong&gt; being &lt;strong&gt;80&lt;/strong&gt; and minimum age being less than a year for passenges who boarded titanic and averagely most people who were in the titanic were about &lt;strong&gt;30&lt;/strong&gt; years of age, while the maximum number of siblings / spouses aboard the Titanic(&lt;strong&gt;SibSp&lt;/strong&gt;) was &lt;strong&gt;8&lt;/strong&gt; while minimum being &lt;strong&gt;0&lt;/strong&gt;, roughly the maximum amount of fare was &lt;strong&gt;512&lt;/strong&gt; while minimum being &lt;strong&gt;0&lt;/strong&gt;. Why is minimum &lt;strong&gt;Fare&lt;/strong&gt; is &lt;strong&gt;0&lt;/strong&gt; maybe children within a given threshhold are not being charged.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info() &lt;span style=&#34;color:#75715e&#34;&gt;# information about data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above there is some rows with missing &lt;strong&gt;Age&lt;/strong&gt;, &lt;strong&gt;Cabin&lt;/strong&gt; and &lt;strong&gt;Embarked&lt;/strong&gt; i&amp;rsquo;ll make some data imputation on them.&lt;/p&gt;
&lt;p&gt;Let me group the three classes and see the average &lt;strong&gt;Age&lt;/strong&gt; of people in each class using &lt;strong&gt;.groupby()&lt;/strong&gt; method of the dataframe type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;AGE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Age&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
],axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    
    
AGE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table3.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the boxplot above the second class(&lt;strong&gt;Pclass&lt;/strong&gt;) age has got outliers as shown above maybe &lt;strong&gt;standarndadization&lt;/strong&gt; if not &lt;strong&gt;normalization&lt;/strong&gt; will handle this.From above it is vividly depicted that old people boarded first class while middle aged boarded second class while children boarded third class reason being known little explation to the reason being attached since it&amp;rsquo;s common sense.Let me use this data to make imputation on Age column for missing values with first class people age being roughly 39 from my above &lt;strong&gt;groupby()&lt;/strong&gt; method of means in 3 classes, and 30 for second class while the third class being 25 years old.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;age_na&lt;/span&gt;(fill):
    Age &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fill[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
    Pclass &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fill[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isnull(Age):

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; Pclass &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;39&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; Pclass &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Age
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;]]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apply(age_na,axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          891 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The missing values of &lt;strong&gt;Age&lt;/strong&gt; column have been filled up the remaining column with missing values are &lt;strong&gt;Embarked&lt;/strong&gt; and &lt;strong&gt;Cabin&lt;/strong&gt; i&amp;rsquo;ll fill the &lt;strong&gt;Embarked&lt;/strong&gt; missing values with largest number of station &lt;strong&gt;Embarked&lt;/strong&gt; and also since &lt;strong&gt;Embarked&lt;/strong&gt; has &lt;strong&gt;3&lt;/strong&gt; distinct value i&amp;rsquo;ll convert it to categorical data type while &lt;strong&gt;Cabin&lt;/strong&gt; has alot of missing values hence i&amp;rsquo;ll drop it during machine learning session.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;dfn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()&lt;span style=&#34;color:#75715e&#34;&gt;#finding total number of people embarkedin each station&lt;/span&gt;

    
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#75715e&#34;&gt;#finding total number of people embarkedin each station&lt;/span&gt;
dfn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Count&amp;#39;&lt;/span&gt;]
dfn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table4.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above the &lt;strong&gt;Southampton&lt;/strong&gt; station had a lot of people than rest so ill use it in filling the 2 missing values in the &lt;strong&gt;Embarked&lt;/strong&gt; column.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fillna(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;S&amp;#34;&lt;/span&gt;,inplace &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True)&lt;span style=&#34;color:#75715e&#34;&gt;# filling Embarked missing values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          891 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     891 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me convert Embarked to categorical data type using &lt;strong&gt;.astype()&lt;/strong&gt; method.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;category&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#converting Embarked column to categorical data type&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Confirming if it has been converted successfly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype &lt;span style=&#34;color:#75715e&#34;&gt;# checking type of embarked column&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;CategoricalDtype(categories=[&#39;C&#39;, &#39;Q&#39;, &#39;S&#39;], ordered=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me divide the people in the ship into two i.e child or adult. With year less than 18 I consider being child while anything above 18 as adult. I&amp;rsquo;ll use &lt;strong&gt;Age&lt;/strong&gt; in dividing them and I&amp;rsquo;ll create another column called &lt;strong&gt;Category&lt;/strong&gt; to hold &lt;strong&gt;child&lt;/strong&gt; and &lt;strong&gt;adult&lt;/strong&gt; variables.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Asumptions are based on my area of jurisdiction constitution stipulation as to who an adult and child should be according to number of years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cut(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;18&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;], labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;child&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adult&amp;#39;&lt;/span&gt;]) &lt;span style=&#34;color:#75715e&#34;&gt;#creating category column&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table5.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Bingo!!! the &lt;strong&gt;Category&lt;/strong&gt; column have been created with pandas &lt;strong&gt;.cut()&lt;/strong&gt; method as shown in  the above table.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll also add a &lt;strong&gt;Family&lt;/strong&gt; column where I&amp;rsquo;ll take into consideration &lt;strong&gt;SibSp&lt;/strong&gt; and &lt;strong&gt;Parch&lt;/strong&gt; columns where I&amp;rsquo;ll write an expresion where the two columns should be more than one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; My assumption is for a family to be called a family,the number of siblings / spouses aboard the Titanic and number of parents / children aboard the Titanic	should be greater than zero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SibSp&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; ([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Parch&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#creating family column&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table6.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Bingo!!! the &lt;strong&gt;Family&lt;/strong&gt; column have been created with pandas &lt;strong&gt;.cut()&lt;/strong&gt; method as shown in  the above table. 
I&amp;rsquo;ll inspect the data frame to see if I have handled missing values and columns I&amp;rsquo;ve created their data type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 14 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   PassengerId  891 non-null    int64   
 1   Survived     891 non-null    int64   
 2   Pclass       891 non-null    int64   
 3   Name         891 non-null    object  
 4   Sex          891 non-null    object  
 5   Age          891 non-null    float64 
 6   SibSp        891 non-null    int64   
 7   Parch        891 non-null    int64   
 8   Ticket       891 non-null    object  
 9   Fare         891 non-null    float64 
 10  Cabin        204 non-null    object  
 11  Embarked     891 non-null    category
 12  Category     891 non-null    category
 13  Family       891 non-null    bool    
dtypes: bool(1), category(2), float64(2), int64(5), object(4)
memory usage: 79.5+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me inspect the uniqueness in each columns.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nunique()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;PassengerId    891
Survived         2
Pclass           3
Name           891
Sex              2
Age             88
SibSp            7
Parch            7
Ticket         681
Fare           248
Cabin          147
Embarked         3
Category         2
Family           2
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me convert &lt;strong&gt;Family&lt;/strong&gt; to categorical since it has two variables either &lt;strong&gt;True&lt;/strong&gt; or &lt;strong&gt;False&lt;/strong&gt; for &lt;strong&gt;Family&lt;/strong&gt; and no &lt;strong&gt;Family&lt;/strong&gt; respectively.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;category&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#75715e&#34;&gt;# converting family from bool to categorical&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 14 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   PassengerId  891 non-null    int64   
 1   Survived     891 non-null    int64   
 2   Pclass       891 non-null    int64   
 3   Name         891 non-null    object  
 4   Sex          891 non-null    object  
 5   Age          891 non-null    float64 
 6   SibSp        891 non-null    int64   
 7   Parch        891 non-null    int64   
 8   Ticket       891 non-null    object  
 9   Fare         891 non-null    float64 
 10  Cabin        204 non-null    object  
 11  Embarked     891 non-null    category
 12  Category     891 non-null    category
 13  Family       891 non-null    category
dtypes: category(3), float64(2), int64(5), object(4)
memory usage: 79.6+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me inspect the fare that was being paid by those who had family and those who didn&amp;rsquo;t have in each station. Using below snippet of code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
df3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table7.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is evident from above that those who had family paid more than those who were alone in each station.
Let me include an &lt;strong&gt;adult&lt;/strong&gt; and a &lt;strong&gt;child&lt;/strong&gt; from &lt;strong&gt;Category&lt;/strong&gt; column and &lt;strong&gt;Sex&lt;/strong&gt; columm of &lt;strong&gt;Female&lt;/strong&gt; and &lt;strong&gt;Male&lt;/strong&gt; in each station and see who paid more in average.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Category&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sex&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Embarked&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
df3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table8.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that females who are adults paid more than child who is female and a child of male gender from Cherbourg and Queenstown paid more than their adult counterpart.
Let me see how many were family member and between family and those who were single who died more.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
df3&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Count&amp;#34;&lt;/span&gt;]
df3

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table9.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that in first class &lt;strong&gt;20&lt;/strong&gt; family members died and &lt;strong&gt;59&lt;/strong&gt; survived while &lt;strong&gt;60&lt;/strong&gt; who had no family died and &lt;strong&gt;77&lt;/strong&gt; survived.In the second class &lt;strong&gt;27&lt;/strong&gt; family members died while &lt;strong&gt;37&lt;/strong&gt; survived while their counterpart &lt;strong&gt;70&lt;/strong&gt; died while &lt;strong&gt;50&lt;/strong&gt; survived lastly in the third class &lt;strong&gt;104&lt;/strong&gt; family members died and &lt;strong&gt;36&lt;/strong&gt; survived while those who had no family &lt;strong&gt;268&lt;/strong&gt; died and &lt;strong&gt;83&lt;/strong&gt; survived.
Let me inspect the &lt;strong&gt;Fare&lt;/strong&gt; that those who were family members and singles paid.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df4 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Family&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
], axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

df4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table10.PNG&#34; alt=&#34;&#34;&gt;
From above it is evident that family members(&lt;strong&gt;True&lt;/strong&gt;) paid more than those who were single(&lt;strong&gt;False&lt;/strong&gt;).&lt;/p&gt;
&lt;h1 id=&#34;213-data-visualization&#34;&gt;2.1.3 Data Visualization.&lt;/h1&gt;
&lt;p&gt;Let me draw a &lt;strong&gt;boxplot&lt;/strong&gt; to show how people are distributed across the three classes in the ship using &lt;strong&gt;seaborn&lt;/strong&gt; and &lt;strong&gt;matplotlib&lt;/strong&gt; libraries .&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# figure size&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;whitegrid&amp;#34;&lt;/span&gt;, color_codes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True) &lt;span style=&#34;color:#75715e&#34;&gt;#  background appearance &lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;boxplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;,y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;#plotting boxplot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_55_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above boxplot old boarded first class while childrens boarded third class.&lt;/p&gt;
&lt;p&gt;Let me draw a countplot using seaborn library to see how adults and children distributed themselves among three classes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;# background style&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;#plotting countplot&lt;/span&gt;

plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_57_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is evident from above class that adults were more in all classes as compared to childrens.&lt;/p&gt;
&lt;p&gt;Next let me draw a graph to see survival rate of both sex.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;);
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sex&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_59_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above graph &lt;strong&gt;males&lt;/strong&gt; died more than &lt;strong&gt;females&lt;/strong&gt; whle &lt;strong&gt;female&lt;/strong&gt; survived more than &lt;strong&gt;male&lt;/strong&gt;.
Let me also show the survival rate in each class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;)
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_61_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;People in the first class survived more than the rest while people in third class died more than the rest.&lt;/p&gt;
&lt;p&gt;Next let me see between children and adults who survived more.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;);
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Set1&amp;#39;&lt;/span&gt;);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Category&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_63_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In both scenarios more adults died and survived than childrens.
Next let me draw a histogram to see how fare is distributed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fare&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(bins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;darkred&amp;#39;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_65_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that Fare is not normal distributed and it needs to be standardized.&lt;/p&gt;
&lt;p&gt;Next let me see how the Number of siblings / spouses who aboarded the Titanic are distributed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SibSp&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Set1&amp;#34;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_67_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fro above it is evident those who were single with no sibling and spouses were more than the rest in all classes in the Titanic.&lt;/p&gt;
&lt;p&gt;Next let me see how the number of parents / children who aboard the Titanic are distributed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;countplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Parch&amp;#39;&lt;/span&gt;,hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;,palette&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Set1&amp;#34;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;df);
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(bbox_to_anchor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt; );&lt;span style=&#34;color:#75715e&#34;&gt;#locating legend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_69_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above it is evident that those who had 0 number of parents/children in the Titanic were more than the rest.&lt;/p&gt;
&lt;p&gt;Next let me see if there is a correlation between different variables.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;));&lt;span style=&#34;color:#75715e&#34;&gt;# size of graph&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_style(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whitegrid&amp;#39;&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;# background style&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;corr(), annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_71_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above the correlation between different column is minimal.&lt;/p&gt;
&lt;h1 id=&#34;3-machine-learning&#34;&gt;3. MACHINE LEARNING.&lt;/h1&gt;
&lt;h2 id=&#34;31-feature-engineering&#34;&gt;3.1 Feature Engineering.&lt;/h2&gt;
&lt;p&gt;Before dividing the data into features and labels I&amp;rsquo;ll drop &lt;strong&gt;Cabin&lt;/strong&gt; column since its less important and it has a lot of missing variables, I&amp;rsquo;ll set &lt;strong&gt;inplace&lt;/strong&gt; to &lt;strong&gt;True&lt;/strong&gt; so that it si compltetely removed from the dataframe.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#dropping the Cabin column&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;drop(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cabin&amp;#39;&lt;/span&gt;,axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,inplace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let me see if the &lt;strong&gt;Cabin&lt;/strong&gt; column has been removed by viewing the first few rows using &lt;strong&gt;.head()&lt;/strong&gt; method.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#printing first four values.&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table11.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#info about column and data&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;RangeIndex: 891 entries, 0 to 890
Data columns (total 13 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   PassengerId  891 non-null    int64   
 1   Survived     891 non-null    int64   
 2   Pclass       891 non-null    int64   
 3   Name         891 non-null    object  
 4   Sex          891 non-null    object  
 5   Age          891 non-null    float64 
 6   SibSp        891 non-null    int64   
 7   Parch        891 non-null    int64   
 8   Ticket       891 non-null    object  
 9   Fare         891 non-null    float64 
 10  Embarked     891 non-null    category
 11  Category     891 non-null    category
 12  Family       891 non-null    category
dtypes: category(3), float64(2), int64(5), object(3)
memory usage: 72.6+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above I can see I don&amp;rsquo;t have any missing values then my data is ready for further engineering and modelling&lt;/p&gt;
&lt;h2 id=&#34;311-converting-categorical-features&#34;&gt;3.1.1 Converting Categorical Features.&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won&amp;rsquo;t be able to directly take in those features as inputs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Converting  the categorical variables to dummy variables&lt;/span&gt;
sex &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_dummies(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;],drop_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
embark &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_dummies(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;],drop_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;drop([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Category&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Family&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Ticket&amp;#39;&lt;/span&gt;],axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,inplace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&amp;rsquo;ll return the &lt;strong&gt;Sex&lt;/strong&gt; and &lt;strong&gt;Embarked&lt;/strong&gt; column that I had dropped durring encoding stage using a method called &lt;strong&gt;concat()&lt;/strong&gt; from pandas library&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt; &lt;span style=&#34;color:#75715e&#34;&gt;#concatinating sex and embark column to data&lt;/span&gt;
df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concat([df,sex,embark],axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#printing first four rows&lt;/span&gt;
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/table12.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Infering my  few rows of data I can see I have my data in numerical form for my model to work best.&lt;/p&gt;
&lt;p&gt;Before I build the model I&amp;rsquo;ll have to split  data into features and labels i.e X and y.&lt;/p&gt;
&lt;p&gt;After splitting  data into X and y i&amp;rsquo;ll further split into training set and test set whereby i&amp;rsquo;ll use training set to train the model and also use it in evaluation stage in the process of developing the model.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll use the trained model to predict the model using the unseen test set whereby it will help in assessing how the model is performing and also its robustness.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Preparing the X  and y variables.&lt;/span&gt;
X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;PassengerId&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pclass&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SibSp&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Parch&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fare&amp;#39;&lt;/span&gt;,
       &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;S&amp;#39;&lt;/span&gt;]]
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From &lt;strong&gt;sklearn&lt;/strong&gt; package I&amp;rsquo;ll use a module called &lt;strong&gt;model_selection&lt;/strong&gt; which has a function called &lt;strong&gt;train_test_split()&lt;/strong&gt; which divides the data into training and test set.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;train_test_split()&lt;/strong&gt; function I have to pass features and labels and also split my data using &lt;strong&gt;test_size&lt;/strong&gt; argument whereby I&amp;rsquo;ll set it to 30% meaning  30% of my data will be used as test set while rest as traing set.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll set my &lt;strong&gt;random_state&lt;/strong&gt; to 101 for reproducibility purposes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#loading library and function&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Splitting the data into train and test sets.&lt;/span&gt;
X_train, X_test, y_train, y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(X, y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.30&lt;/span&gt;, 
                                                    random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;101&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;32-building-and-evaluating-model&#34;&gt;3.2 Building and Evaluating Model.&lt;/h2&gt;
&lt;p&gt;Since our task at hand has labelled data and also predicted data is known so it fall under supervised typer of machine learning.&lt;/p&gt;
&lt;p&gt;Under supervised machine learning it is divided into two classification and regression type. Classification answers the question of the form yes/no  and regession answers the questions how much?.&lt;/p&gt;
&lt;p&gt;Since my task at hand is to predict whether a persion died or survived so it falls under classication problem and under classification i&amp;rsquo;ll use below machine learning algorithms in training my model which are available in sklearn package read more about them in this link &lt;a href=&#34;http://scikit-learn.org/stable/supervised_learning.html&#34;&gt;&lt;code&gt;scikit-learn&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian Naive Bayes (GaussianNB).&lt;/li&gt;
&lt;li&gt;Decision Trees.&lt;/li&gt;
&lt;li&gt;Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting).&lt;/li&gt;
&lt;li&gt;K-Nearest Neighbors (KNeighbors).&lt;/li&gt;
&lt;li&gt;Support Vector Machines (SVM).&lt;/li&gt;
&lt;li&gt;Logistic Regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;321-implementation&#34;&gt;3.2.1 Implementation.&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ll implement my model using the following procedure in each algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;ll start by importing the module&lt;/li&gt;
&lt;li&gt;Next I&amp;rsquo;ll initialize the module using the classifier.&lt;/li&gt;
&lt;li&gt;After that train the model by fitting it to my data using &lt;strong&gt;.fit()&lt;/strong&gt; method.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ll use trained model to make prediction using &lt;strong&gt;predict()&lt;/strong&gt; function.The &lt;strong&gt;predict()&lt;/strong&gt; function will return an array of prediction for each data instance in test set.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;330-building-a-logistic-regression-model&#34;&gt;3.3.0 Building a Logistic Regression model.&lt;/h1&gt;
&lt;p&gt;Logistic regression is a type of regression where its class variable is not continous but categorical, as our cas to survival rate where it is categorical with two classes.Since I&amp;rsquo;m dealing with binary classification problem I&amp;rsquo;ll use binomial logistic regression, which is of this form 
$$ Y =  \frac{e^x}{1+e^x}  $$&lt;br&gt;
$$ where\ X = B_0 + B{_1}x $$&lt;/p&gt;
&lt;p&gt;I had imported the Logistic classifier in the beggining of this project what I&amp;rsquo;ll do is to initialize the classifier then fit in the model using &lt;strong&gt;.fit method&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the models from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing the LogisticRegression Classifier&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LogisticRegression
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier as below.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# instatiating the classifier.&lt;/span&gt;
logmodel &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LogisticRegression()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting your LogisticRegression class to the training data&lt;/span&gt;
logmodel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using LogisticRegression on the test data&lt;/span&gt;
predictions_log &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; logmodel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model - this will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;331-metrics-for-evaluating-model-performance&#34;&gt;3.3.1 Metrics for Evaluating Model Performance.&lt;/h3&gt;
&lt;p&gt;Let me define metrics that ill use in evaluating my model accurcy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;True Positive (TP)&lt;/strong&gt;: Correctly classified as the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;True Negative (TN)&lt;/strong&gt;: Correctly classified as not the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;False Positive (FP)&lt;/strong&gt;: Incorrectly classified as the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;False Negative (FN)&lt;/strong&gt;: Incorrectly classified as not the class of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; : The proportion of correct predictions out of all the predictions given by:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ Accurcy  =  \frac{TP + TN}{TP + FP + TN  + FN} $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; (also known as the positive predictive value) is defined as the proportion of positive examples that are truly positive, given by the formula:
$$ Precision = \frac{TP}{TP + FP}  $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt;: This is defined as the number of true positives over the total number of positives, given by:
$$ Recall = \frac{Tp}{TP + FN}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;F1 score&lt;/strong&gt;: It combines precision and recall using &lt;strong&gt;harmonic mean&lt;/strong&gt; also known as &lt;strong&gt;F-measure&lt;/strong&gt; or &lt;strong&gt;F-score&lt;/strong&gt;, iven by:
$$ F-MEASURE = \frac{2 \cdot precision \cdot recall}{recall + precision} = \frac{2 \cdot TP }{2 \cdot TP + FP + FN}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confusion matrix&lt;/strong&gt;: This is a &lt;strong&gt;n x n&lt;/strong&gt;  matrix that is used to compare predicted values to actual values.As shown below.
&lt;img src=&#34;/post/2020-07-27-mine_files/Confusion_Matrix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;model_metrics&lt;/span&gt;(y_true, preds, model_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    INPUT:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    y_true - the y values that are actually true in the dataset (numpy array or pandas series)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    preds - the predictions for those values from some model (numpy array or pandas series)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    model_name - (str - optional) a name associated with the model if you would like to add it to the print
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    statements 
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    OUTPUT:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    None - prints the accuracy, precision, recall, and F1 score
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; None:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Accuracy score for : &amp;#39;&lt;/span&gt;, format(accuracy_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Precision score for : &amp;#39;&lt;/span&gt;, format(precision_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Recall score for : &amp;#39;&lt;/span&gt;, format(recall_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F1 score for: &amp;#39;&lt;/span&gt;, format(f1_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Accuracy score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt; , format(accuracy_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Precision score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt;, format(precision_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Recall score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt;, format(recall_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F1 score for &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; model_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; :&amp;#39;&lt;/span&gt;, format(f1_score(y_true, preds)))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing Logistic scores&lt;/span&gt;
model_metrics(y_test, predictions_log, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;LOGISTIC&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for LOGISTIC : 0.7686567164179104
Precision score for LOGISTIC : 0.782608695652174
Recall score for LOGISTIC : 0.631578947368421
F1 score for LOGISTIC : 0.6990291262135923
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;Logistic regession&lt;/strong&gt; has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;77%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;78%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;63%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;70%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with &lt;strong&gt;confusion matrix&lt;/strong&gt; and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([logmodel]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plotting the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_109_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;87%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;63%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;37%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;13%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;34-building-a-knn-model&#34;&gt;3.4 Building a KNN model.&lt;/h1&gt;
&lt;p&gt;This method is called lazy learning because parameters for modelling purposes is not defined.For new data point prediction, the train data is searched for an instance resembling the new instance to be predicted.During modelling purposes the training themselves conjures the knowledge.In determining the proper class during modelling phace KNN looks for the closest points-the nearest neighbors.In order to determine the neighbors to be modelled by algorithm the k comes into play, for example if k=6, six nearest neighbors will be examined.IThe weakness of the algorithm is of equal weight attached traning all the six points even if some of them are meaningless.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the model from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Import the KNeighbors Classifier&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.neighbors &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; KNeighborsClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate a KNeighborsClassifier with:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# n_neighbors=1 and everything else as default values&lt;/span&gt;
mod_knn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; KNeighborsClassifier(n_neighbors&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting your KNeighborsClassifier to the training data&lt;/span&gt;
mod_knn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,
                     weights=&#39;uniform&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using KNeighborsClassifier on the test data&lt;/span&gt;
preds_knn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_knn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model .This will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing KNeighbors scores&lt;/span&gt;
model_metrics(y_test, preds_knn, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;KNN&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for KNN : 0.5932835820895522
Precision score for KNN : 0.5242718446601942
Recall score for KNN : 0.47368421052631576
F1 score for KNN : 0.4976958525345622
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;KNeighbors&lt;/strong&gt; has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;59%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;52%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;47%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;50%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([mod_knn]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_123_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;68%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;47%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;52%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;31%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;35-building-a-decision-tree-model&#34;&gt;3.5 Building a DECISION TREE model.&lt;/h1&gt;
&lt;p&gt;A decision tree works by generating a separating hyperplane or a threshold for the features in data. It does this by considering every feature and finding the correlation between the spread of the values in that feature and the label that you are trying to predict.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the model from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing the  DecisionTree Classifier&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.tree &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; DecisionTreeClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiating a DecisionTreeClassifier&lt;/span&gt;
mod_tree &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DecisionTreeClassifier()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fiting your DecisionTreeClassifier to the training data&lt;/span&gt;
mod_tree&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;,
                       random_state=None, splitter=&#39;best&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting  using DecisionTreeClassifie on the test data&lt;/span&gt;
predictions_tree &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_tree&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model . This will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing the Decision Tree classifier.&lt;/span&gt;
model_metrics(y_test, predictions_tree, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Decision Tree&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for Decision Tree : 0.7388059701492538
Precision score for Decision Tree : 0.7340425531914894
Recall score for Decision Tree : 0.6052631578947368
F1 score for Decision Tree : 0.6634615384615384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;Decision Tree&lt;/strong&gt; has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;74%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;783%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;61%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;66%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([mod_tree]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_137_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;83%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;61%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;40%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;16%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;36-building-support-vector-model&#34;&gt;3.6 Building Support Vector Model.&lt;/h1&gt;
&lt;p&gt;Support vector is like a points of plotted in multidimensional expessing values and their examples.The support vector machine(SVM) creates a boundary called hyperplane, which divides the space creating homogeneous partition on either side, hence combining Nearest Neighbor and Linear Regression methods.SVM uses the idea of what is known as Maximum Marginal Hyperplane which create greater separation between two classes.Kernel Trick is also applied in seperating the two classes.&lt;/p&gt;
&lt;p&gt;Some of the kernel trick applied include:
Kernell function takes this form:
&lt;img src=&#34;/post/2020-07-27-mine_files/kernell.PNG&#34; alt=&#34;&#34;&gt;
where Greek letter phi, that is, ϕ(x), is a mapping of the data into another space. Therefore,  the general kernel function applies some transformation to the feature vectors xi and 
xj and combines them using the dot product, which takes two vectors and returns a  single number.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;linear kernel which does not transform data at all, given by the formula:
&lt;img src=&#34;/post/2020-07-27-mine_files/ln.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Polynomial kernel of degree d which adds a simple nonlinear transformation of data.
&lt;img src=&#34;/post/2020-07-27-mine_files/poly.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sigmoid Kernel: Is almost like sigmoid activation function used in neural network.Given by the formula.
&lt;img src=&#34;/post/2020-07-27-mine_files/sigmoid.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian RBF Kernel which is anologous to RBF neural network.It does well in many types of data.
&lt;img src=&#34;/post/2020-07-27-mine_files/gaussian.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First I&amp;rsquo;ll load the package necessary for the modelling from &lt;strong&gt;sklearn&lt;/strong&gt; where it has a method called &lt;strong&gt;svm&lt;/strong&gt; which has &lt;strong&gt;SVC&lt;/strong&gt; function embedded for training support vector machine modells as below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the model from &lt;code&gt;sklearn&lt;/code&gt; and use it to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SVC &lt;span style=&#34;color:#75715e&#34;&gt;#loading library necessary for modelling&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; the classifier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model_svc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SVC() &lt;span style=&#34;color:#75715e&#34;&gt;#instatiate the model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated the model, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; it using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model_svc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train) &lt;span style=&#34;color:#75715e&#34;&gt;#fitting trained model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;,
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit  the model, I will use it to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;predictions_svm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model_svc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test) &lt;span style=&#34;color:#75715e&#34;&gt;#predicting the feature labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for  the model . This will give  the &lt;code&gt;score&lt;/code&gt; for how well  the model is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Printing Support Vector Machine scores.&lt;/span&gt;
model_metrics(y_test, predictions_svm, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Support Vector Machine&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for Support Vector Machine : 0.6044776119402985
Precision score for Support Vector Machine : 0.75
Recall score for Support Vector Machine : 0.10526315789473684
F1 score for Support Vector Machine : 0.1846153846153846
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the &lt;strong&gt;Support Vector Machine&lt;/strong&gt;  has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;60%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;75%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;11%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;18%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([model_svc]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] 

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_151_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From above &lt;strong&gt;97%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;11%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;90%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;3%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use ensemble type of classification in modelling the classification problem at hand and see if there will be some improvement on perfomance of the model&lt;/p&gt;
&lt;h1 id=&#34;36-ensemble-learning-model&#34;&gt;3.6 Ensemble Learning Model.&lt;/h1&gt;
&lt;p&gt;Ensemble learning, as the name denotes, is a method that combines several machine learning models to generate a superior model, thereby decreasing variability/variance and bias, and boosting performance, as shown below.
&lt;img src=&#34;/post/2020-07-27-mine_files/Ensemble.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ensemble models combine many weaker models that differ in variance and bias, thereby creating a better model, outperforming the individual weaker models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/ensemble_chart.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In  modelling  the  ensemble  learning  type  of  classification  I&amp;rsquo;ll  follow the following 5 methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Importing&lt;/strong&gt; the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instantiating&lt;/strong&gt; the model with the hyperparameters of interest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fitting&lt;/strong&gt; the model to the training data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predicting&lt;/strong&gt; on the test data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scoring&lt;/strong&gt; the model by comparing the predictions to the actual values.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;ll follow the above steps in modelling each of the ensemble methods: &lt;strong&gt;BaggingClassifier&lt;/strong&gt;, &lt;strong&gt;RandomForestClassifier&lt;/strong&gt;, and &lt;strong&gt;AdaBoostClassifier&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For more info about the above ensemble methods can be found below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier&#34;&gt;BaggingClassifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier&#34;&gt;RandomForestClassifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier&#34;&gt;AdaBoostClassifier&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another really useful guide for ensemble methods can be found &lt;a href=&#34;http://scikit-learn.org/stable/modules/ensemble.html&#34;&gt;in the documentation here&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: First I&amp;rsquo;ll &lt;code&gt;import&lt;/code&gt; the models from &lt;code&gt;sklearn&lt;/code&gt; and use them to instatiate the class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing the Bagging, RandomForest, and AdaBoost Classifier.&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; RandomForestClassifier,AdaBoostClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Now that I  have imported each of the classifiers, I&amp;rsquo;ll &lt;code&gt;instantiate&lt;/code&gt; each with the hyperparameters specified in each comment.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiating a RandomForestClassifier with:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 200 weak learners (n_estimators) and everything else as default values&lt;/span&gt;
mod_rf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RandomForestClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Instantiating an a AdaBoostClassifier with:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# With 300 weak learners (n_estimators) and a learning_rate of 0.2&lt;/span&gt;
mod_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AdaBoostClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;600&lt;/span&gt;, learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Now that I have instantiated each of the models, I&amp;rsquo;ll &lt;code&gt;fit&lt;/code&gt; them using the &lt;strong&gt;X_train&lt;/strong&gt; and &lt;strong&gt;y_train&lt;/strong&gt;.  This may take a bit of time, I&amp;rsquo;m fitting 1100 weak learners after all!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fit your RandomForestClassifier to the training data&lt;/span&gt;
mod_rf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Fit your AdaBoostClassifier to the training data&lt;/span&gt;
mod_ada&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;, base_estimator=None, learning_rate=0.2,
                   n_estimators=600, random_state=None)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Now that I have fit each of the models, I will use each to &lt;code&gt;predict&lt;/code&gt; on the &lt;strong&gt;testing_data&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using RandomForestClassifier on the test set data&lt;/span&gt;
preds_rf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_rf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;span style=&#34;color:#75715e&#34;&gt;# Predicting using AdaBoostClassifier on the test set data&lt;/span&gt;
preds_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_ada&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Now that I have made the predictions, I&amp;rsquo;ll compare the predictions to the actual values using the function below for each of the models - this will give  the &lt;code&gt;score&lt;/code&gt; for how well each of the models is performing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print Random Forest scores&lt;/span&gt;
model_metrics(y_test, preds_rf, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;random forest&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Print AdaBoost scores&lt;/span&gt;
model_metrics(y_test, preds_rf, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adaboost&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Accuracy score for random forest : 0.832089552238806
Precision score for random forest : 0.8556701030927835
Recall score for random forest : 0.7280701754385965
F1 score for random forest : 0.7867298578199052



Accuracy score for adaboost : 0.832089552238806
Precision score for adaboost : 0.8556701030927835
Recall score for adaboost : 0.7280701754385965
F1 score for adaboost : 0.7867298578199052
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it&amp;rsquo;s evident that the random forest has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;83%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;86%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;73%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;79%&lt;/strong&gt; while  adaboost has an &lt;strong&gt;accuracy&lt;/strong&gt; of &lt;strong&gt;83%&lt;/strong&gt; with &lt;strong&gt;precision&lt;/strong&gt; rate being &lt;strong&gt;86%&lt;/strong&gt; while &lt;strong&gt;recall&lt;/strong&gt; score being &lt;strong&gt;73%&lt;/strong&gt; and &lt;strong&gt;f1&lt;/strong&gt; score as &lt;strong&gt;79%&lt;/strong&gt; which are the same.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use below code snippet in coming up with confusion matrix and make deduction on how the features have been predicted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; confusion_matrix

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m, model &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate([mod_rf,mod_ada]):
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_matrix(y_test, model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test))
    confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;astype(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; confusion_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] &lt;span style=&#34;color:#75715e&#34;&gt;# normalizing the data&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Plot the heatmap&lt;/span&gt;
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(m)
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heatmap(confusion_mat, annot&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, annot_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Reds&amp;#39;&lt;/span&gt;, square&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, fmt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.3f&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True label&amp;#39;&lt;/span&gt;, fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted label&amp;#39;&lt;/span&gt;,fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;})
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Confusion matrix for:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__class__&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__),fontdict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;});
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_167_0.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_167_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;Random Forest&lt;/strong&gt; &lt;strong&gt;91%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;73%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;27%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;9%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;AdaBoost&lt;/strong&gt; &lt;strong&gt;86%&lt;/strong&gt; of the labels was correctly predicted as &lt;strong&gt;death&lt;/strong&gt; and &lt;strong&gt;66%&lt;/strong&gt; as &lt;strong&gt;survived&lt;/strong&gt; while &lt;strong&gt;34%&lt;/strong&gt; of the &lt;strong&gt;survived&lt;/strong&gt; label was predicted incorrectly and &lt;strong&gt;14%&lt;/strong&gt; of death labels were misclassified as &lt;strong&gt;survived&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally I&amp;rsquo;ll compare all the algorithms that I have used in training the model and choose which is the best and use it tuning the model and also in making general overal deduction of the model perfomance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Algorithm Name&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Accuracy score&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Precision score&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Recall score&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;strong&gt;F1 score&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.76865671641791&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7826086956521740&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.631578947368421&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.699029126213592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;K-Nearest Neighbors&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.59328358208965&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.5242718446601942&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.473684210526316&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4976958525345622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Decision Tree&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.73880597014926&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7340425531914894&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.605263157894737&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6634615384615384&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Support Vector Machine&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.75373134328358&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7500000000000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.631578947368421&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6857142857142857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AdaBoost&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.83208955223880&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.8556701030927835&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.728070175438597&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7867298578199052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.83208955223880&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.8556701030927835&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.728070175438597&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7867298578199052&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From above table it&amp;rsquo;s evident that adaboost and random forest are the best algorithm for training the overal perfomance of the model and also it&amp;rsquo;s tuning process.&lt;/p&gt;
&lt;p&gt;Next I&amp;rsquo;ll use adaboost in tuning the model to see it&amp;rsquo;s improvement I&amp;rsquo;ll use gridsearchCv method of model tuning.&lt;/p&gt;
&lt;h1 id=&#34;4-optimizing-the-model&#34;&gt;4. Optimizing The Model.&lt;/h1&gt;
&lt;h1 id=&#34;41-gridsearchcv&#34;&gt;4.1 GridSearchCV.&lt;/h1&gt;
&lt;p&gt;GridsearchCV is a method of tuning wherein the model can be built by evaluating the combination of parameters mentioned in a grid.&lt;/p&gt;
&lt;p&gt;I can conduct a grid search much more easily in practice by leveraging &lt;strong&gt;model_selection.GridSearchCV&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing &amp;#39;GridSearchCV&amp;#39;, &amp;#39;make_scorer&amp;#39; &lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; GridSearchCV
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; make_scorer

&lt;span style=&#34;color:#75715e&#34;&gt;# Starting the timer&lt;/span&gt;
start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time()

&lt;span style=&#34;color:#75715e&#34;&gt;# Initializing the classifier&lt;/span&gt;
mod_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AdaBoostClassifier(base_estimator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DecisionTreeClassifier())

&lt;span style=&#34;color:#75715e&#34;&gt;# Creating the parameters list for tuning&lt;/span&gt;
parameters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;n_estimators&amp;#39;&lt;/span&gt;:[&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;], 
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;learning_rate&amp;#39;&lt;/span&gt;:[&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;],
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;base_estimator__min_samples_split&amp;#39;&lt;/span&gt; : np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;base_estimator__max_depth&amp;#39;&lt;/span&gt; : np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
             }

&lt;span style=&#34;color:#75715e&#34;&gt;# Making an fbeta_score scoring object&lt;/span&gt;
scorer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_scorer(fbeta_score,beta&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Perform grid search on the classifier using &amp;#39;scorer&amp;#39; as the scoring method&lt;/span&gt;
grid_search &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GridSearchCV(mod_ada, parameters, scorer)

&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting the grid search object to the training data and finding the optimal parameters&lt;/span&gt;
grid_fit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; grid_search&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Getting the estimator&lt;/span&gt;
best_mod &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; grid_fit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;best_estimator_

&lt;span style=&#34;color:#75715e&#34;&gt;# Making predictions using the unoptimized and model&lt;/span&gt;
predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
best_predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; best_mod&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;span style=&#34;color:#75715e&#34;&gt;# Report the before-and-afterscores&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Unoptimized model&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F-score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Optimized Model&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Final accuracy score on the testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, best_predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Final F-score on the testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, best_predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(best_mod)

&lt;span style=&#34;color:#75715e&#34;&gt;# Printing runtime&lt;/span&gt;
end &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time()
runtime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; end &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; start
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Runtime:&amp;#34;&lt;/span&gt;, runtime)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Unoptimized model
------
Accuracy score on testing data: 0.7425
F-score on testing data: 0.7072

Optimized Model
------
Final accuracy score on the testing data: 0.7873
Final F-score on the testing data: 0.7792
AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;,
                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,
                                                         class_weight=None,
                                                         criterion=&#39;gini&#39;,
                                                         max_depth=2,
                                                         max_features=None,
                                                         max_leaf_nodes=None,
                                                         min_impurity_decrease=0.0,
                                                         min_impurity_split=None,
                                                         min_samples_leaf=1,
                                                         min_samples_split=2,
                                                         min_weight_fraction_leaf=0.0,
                                                         presort=&#39;deprecated&#39;,
                                                         random_state=None,
                                                         splitter=&#39;best&#39;),
                   learning_rate=0.1, n_estimators=20, random_state=None)
Runtime: 64.8770227432251
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;5-feature-importance&#34;&gt;5. Feature Importance.&lt;/h1&gt;
&lt;p&gt;In any supervised machine learning model it&amp;rsquo;s best to identify the features that affect how the model perfom best.Like in the titanic case I identify which features most affect the survival rate of passengers in the titanic.&lt;/p&gt;
&lt;p&gt;From all the features in the titanic dataset I&amp;rsquo;ll use only the top 5 of the features that affect the how the model perfom as shown below&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;feature_plot&lt;/span&gt;(importances, X_train, y_train):
    
    &lt;span style=&#34;color:#75715e&#34;&gt;# Displaying the five most important features&lt;/span&gt;
    indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(importances)[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values[indices[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]]
    values &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; importances[indices][:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]

    &lt;span style=&#34;color:#75715e&#34;&gt;# Creating the plot&lt;/span&gt;
    fig &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;))
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Normalized Weights for First Five Most Predictive Features&amp;#34;&lt;/span&gt;, fontsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bar(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;), values, width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.6&lt;/span&gt;, align&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;center&amp;#34;&lt;/span&gt;, color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#00A000&amp;#39;&lt;/span&gt;, \
          label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Feature Weight&amp;#34;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bar(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cumsum(values), width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, align &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;center&amp;#34;&lt;/span&gt;, color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#00A0A0&amp;#39;&lt;/span&gt;, \
          label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Cumulative Feature Weight&amp;#34;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xticks(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;), columns)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlim((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.5&lt;/span&gt;))
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Weight&amp;#34;&lt;/span&gt;, fontsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Feature&amp;#34;&lt;/span&gt;, fontsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)
    
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(loc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;upper center&amp;#39;&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tight_layout()
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()  

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importing a supervised learning model that has &amp;#39;feature_importances_&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AdaBoostClassifier

&lt;span style=&#34;color:#75715e&#34;&gt;# Training the supervised model on the training set &lt;/span&gt;
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AdaBoostClassifier()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train,y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Extracting the feature importances&lt;/span&gt;
importances &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;feature_importances_

&lt;span style=&#34;color:#75715e&#34;&gt;# Plotting features.&lt;/span&gt;
feature_plot(importances, X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/post/2020-07-27-mine_files/Titanic_Project_Final_177_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;51-feature-selection&#34;&gt;5.1. Feature Selection.&lt;/h1&gt;
&lt;p&gt;An interesting thing to think about here is that how does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower, at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model we found earlier, and train it on the same training set with only the top five important features.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Importting functionality for cloning a model&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.base &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; clone

&lt;span style=&#34;color:#75715e&#34;&gt;# Reducing the feature space&lt;/span&gt;
X_train_reduced &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_train[X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values[(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(importances)[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]]]
X_test_reduced &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_test[X_test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values[(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort(importances)[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]]]

&lt;span style=&#34;color:#75715e&#34;&gt;# Training on the &amp;#34;best&amp;#34; model found from grid search earlier&lt;/span&gt;
mod_ada &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (clone(best_mod))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train_reduced, y_train)

&lt;span style=&#34;color:#75715e&#34;&gt;# Making new predictions&lt;/span&gt;
reduced_predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mod_ada&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test_reduced)

&lt;span style=&#34;color:#75715e&#34;&gt;# Reporting scores from the final model using both versions of data&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Final Model trained on full data&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, best_predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F-score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, best_predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Final Model trained on reduced data&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;------&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(accuracy_score(y_test, reduced_predictions)))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F-score on testing data: {:.4f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(fbeta_score(y_test, reduced_predictions, beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Final Model trained on full data
------
Accuracy on testing data: 0.7873
F-score on testing data: 0.7792

Final Model trained on reduced data
------
Accuracy on testing data: 0.6754
F-score on testing data: 0.6150
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From above it can be seen that training on full data gives the best score as compared to training on reduced data since there is reduction in score from 78% to 68%.Therefore it&amp;rsquo;s best to make inference on features that make great impact to the model but not training using the only those features but on whole features.&lt;/p&gt;
&lt;p&gt;Thank you for going through my first project in my blog stay in touch for more complex projects,&lt;/p&gt;
</description>
  </item>
  
</channel>
  </rss>